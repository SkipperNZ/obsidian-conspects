[[Big_data practicum made]]



# Лекция 2

Операции MAP и Reduce

##  W2L102. MapReduce (MR) и распределенные консольные утилиты

![[Pasted image 20220410172338.png]]

![[Pasted image 20220410172317.png]]

Для операции Reduce очень важен порядок.
![[Pasted image 20220410172512.png]]


Объединение последовательных вызовов Map и Reduce назвали - MapReduce
![[Pasted image 20220410172636.png]]

С помощью такого подхода, решается огромное число задач связанное с обработкой и анализом данных.

Несколько примеров:

### grep 
Консольная утилита, которая находит все вхождения указанных паттернов в интересующем датасете. 
У него есть много флагов. Например:
`-i`  ищет вхождения без учета регистра.
что бы узнать всё что он может, надо писать в консоли 
`man grep`

![[Pasted image 20220410172901.png]]

И так, в нашем мире биг даты есть здоровенный датасет который хранится в hdfs. хранится он по чанкам, на разных физических машинах. 
![[Pasted image 20220410173508.png]]

Выход map - сам grep. а  reduce -  не определён. (concate делать не нужно, так как выход программы возможно тоже является большими данными)
поэтому результат - распределённые данные, или данные в hdfs.

### head
выводит первые несколько строчек датасета. 
![[Pasted image 20220410173754.png]]

Тут всё же лучше пользоваться консольной утилитой hdfs dfs 
![[Pasted image 20220410174005.png]]

Но если всё же извернуться 
![[Pasted image 20220410174043.png]]

Когда мы читаем данные с каждого чанка на фазе map, через окружение от hadoop мы узнаем id блока и сколько байт мы отступили от начала файла. 
На основе этой информации мы можем выводить первые 10 строчек только из первого чанка . Если в чанке меньше 10 строк, то передаём дополнительные данные из map в фазу reduce.

### WC 

Позволяет посчитать число строк, слов символов и байт в нашем датасете.
По умолчанию выводит первые 3 числа, но с помощью флагов можно настроить  вывод как захочется.
![[Pasted image 20220410174414.png]]

подвоха то особо нет.
![[Pasted image 20220410174625.png]]


## W2L103. Word Count и формальная модель MapReduce

Разберём hello world  для мира MapReduce
Решение подсчета статистики встречаемости слов: **Word Count**

Первый обзац статьи википедии про hadoop
![[Pasted image 20220410175012.png]]

Результатом обработки этих данных будет являться статистика 
![[Pasted image 20220410175710.png]]

Что бы решить эту задачу в консоли на одном компьютере нужно: 

считать данные с помощью `cat` и передать их  на вход следующей команде: 
![[Pasted image 20220410175855.png]]

`tr` - делает так что все пробелы будут заменены на символ перевода строки. 
на выходе - каждое слово на новой строке
![[Pasted image 20220410175913.png]]

Затем сортируем данные, что бы одинаковые слова шли подряд
![[Pasted image 20220410180041.png]]

`uniq -c` - проходится по данным сверху вниз и счетчиком считает сколько раз какое слово встретилось
![[Pasted image 20220410180118.png]]


**Теперь перенесём эти вычисления на MapReduce**

`cat` - ничего сложного, данные мы читаем на фазе map 
`tr` - никакой агрегации нет, поэтому это можно применить на фазе map
`sort` -  как часть map фазы - задачу не решает, поскольку нужны данные отсортированые на вхов reducer'у, то есть без части reduce точно не обойтись. 
но, выполнить функцию полностью в рамках reduce тоже не выйдет. все данные не влезут ни в ram компа, ни даже на hdd одного физического узла.

`uniq -c` - агрегирующая функция, поэтому по идеологии производится на фазе reduce 


Поэтому одними фазами Map и Reduce не обойтись, для этого придумали фазу **shuffle and sort**

Как оно работает на примере: 

Наш большой датасет хранится в hdfs чанками по 128 мегабайт. 
На фазе map - мы применяем функцию `tr` что бы на выходе получить коллекцию слов и дальше shuffle and sort делает 2 вещи
1) **shuffle** - считает хеш для каждого слова. (хеш - детерминированная функция которая для каждого входного элемента выдаёт какое то число). Получаем это число, делим его на число машин, которое будет выполнять фазу reduce (R - на картинке) и отправляем это слово на машину, соответствующую остатку от деления нашего хеша на R. 

Если предположить что у нас слова только английского алфавита, 
В этом случае можем написать хеш функцию, которая для каждого слова выдаёт число от 1 до 26 в зависимости от того, на какую букву слово начинается. 
Тогда все слова начиная с **A** - улетят на первую машину, **B** - на 2ую, 
а слова на **Z** - на 26ую машину. 

следующая фаза:
2) **sort** - на каждой reduce машине всек данные будут отсортированы, таким образом можем правильно посчитать с помощью `uniq -c` сколько раз, какое слово встретилось в данном датасете. 

![[Pasted image 20220410182050.png]]

Для ясности работы **shuffle and sort** следует ответить на 3 вопроса:

1) Объясните, что будет происходить в случае если хеш функция будет не детерминированной.
В этом случае одно и то же слово в зависимости от подсчета хеша попадёт на разные физические reduce машины и результат вычисления будет неверный. Вместо hadoop-200 мы получим например 2 записи hadoop-137 на одной машине и hadoop-63 на другой 

2) Может ли так оказаться, что у нас будут коллизии при вычислении хеша

Да, такое наверняка скорее всего произойдёт, особенно после деления на число reduce машин. Но ничего страшного, потому что данные мы потом сортируем по слову целиком, а не по хешу, и результат будет верный

3) А может ли произойти так, что число данных которые приедут на одну машину будет на порядок больше чем на все остальные.

Да, например артикли a или the встречаются сильно чаще эта проблема называется data sque (перекошеные данные) обсудим эту проблему в самом конце специализации


### Формальная модель MapReduce

Есть 3 фазы:
![[Pasted image 20220410184030.png]]

На каждой фазе запускаются воркеры
которые отвечают за чтение данных, обработку их с помощью детерминированных функций map и reduce, за сохранение и пресылку результатов

Воркеры на фазе Map  называются мапперы 
на фазе reduce - редьюсеры
![[Pasted image 20220410184305.png]]

маппер - считывает данные, получает набор элементов, и для каждого элемента применяет детерминированную map функцию
на выходе может получатся разное число элементов, как например с `tr` где мы каждую строку разбиваем на слова. а число слов в строке - произвольное число. 

Коллеги из гугла немного формализовали вход и выход операции map и reduce. Они сказали, давайте оперировать не произвольными элементами на входе и выходе, а всегда с парами ключ-значение.


Обновим теперь Word count с учетом всего вышенаписанного. 

1) данные с жёсткого диска для маппера  мы должны представить в  формате ключ-значение. по факту в текстовых данных у нас только значения, поэтому  hadoop добавляет в качестве ключа - offset (информацию, сколько байт от начала файла мы отступили при чтении строки из датасета)

2) map принимает на вход пару: (отступ, содержимое строки) ключ игнорирует, и парсит строку на слова. 
	слова тоже не пара, и что бы удовлетворить формальным требованиям MapReduce мы можем каждое слово взять как ключ, а значением записать 1 (сколько раз этот ключ нужно сложить на фазе reduce) 

3) shuffle and sort - проходит всегда по ключам (ВСЕГДА И ТОЛЬКО ПО КЛЮЧАМ  у нас происходит вычисление хеша для shuffle и сортировка данных для редьюсера) 
	Значения в фазе shuffle and sort не анализируются, а только передаются.
	
4) После этого редьюсер смело идёт по элементам сверху-вниз и агрегирует значения для каждого ключа.

![[Pasted image 20220410190048.png]]

### Теперь то же самое, но с точки зрения сигнатур функций

На фазе map - у нас есть ридер, который читает данные и  пердставляет их в формате ключ-значение для нашей функции map
это `k_in`  и `v_in` на картинке. в случае с word count это пара int и str

Дальше функция map берёт на вход эту пару и возвращает на выход iterable пар `[(k_interm, v_interm), ...]` - промежуточный тип данных.  в случае с  word count это пара str и int

Дальше фреймворк раскидывает данные на основе значения хеша от КЛЮЧА и сортирует или группирует данные по ключам для редьюсера.

На вход функция Reduce получает пару `(k_interm, [v_interm, ...])`
на выходе можем получить новый тип данных `[(k_out, v_out), ...]` 
в случае с word count это  int и str

![[Pasted image 20220410191015.png]]




##  W2L104. MapReduce и Fault Tolerance

Есть пользовательское приложение (реализация 2х функций): 
map и reduce. всё остальное за нас делает фреймворк.

1) заводится специальный воркер Aplication master. он отвечает за выполнение нашей программы. отправляем программу на кластер и дальше спим спокойно. 
	Master может даже самостоятельно определить сколько маперов и редьюсеров запустить. Что бы обработать данные 
	split - определяет логические блоки данных для обработки с помощью map reduce. По умолчанию он приблизительно равен размеру чанка. но происходит выравнивание с учетом логики нашего приложения. 
	Например если работаем с текстом, то сплиты делят датасет не в произвольном месте. а в ближайшем месте где есть перевод строки. 

master - аркетстратор, который отвечает за распределение данных сплитов по воркерам.

Редьюсер все считает и сохраняет результат выполнения 1им файлом в hdfs.
![[Pasted image 20220410192639.png]]

Данные между мапером и редьюсером хранятся на локальных жестких дисках (LocalFS)

### Случаи когда какая либо из компонент кластера выходит из строя. 

1) вышел из строя какой либо mapper. 
В этом случае мастер просто определяет что какой либо воркер должен обработать все сплиты которые ранее были назначены поломаному воркеру а так же обновить конфигурацию, откуда забирать данные редбюсерам
![[Pasted image 20220410193608.png]]

2) допустим отвалился контейнер с редьюсером. 
В этом случае все тоже самое. поднимаем временные данные  с диска, считаем хеши по ключам и только данные с нужным хешом отправляем на новый редьюсер. 
![[Pasted image 20220410193624.png]]

3) вылетел не воркер, а жёсткий диск где хранятся временные данные. 
В этом случае мастер перезапустит все воркеры, данные которых не удалось достать из none hdfs storage. обновит матрицу соотсетствия для редьюсеров и всё заработает.

Если отвалится мастер то конечно беда - надо заново перезапускать приложение. но перезапуск происходит автоматически. 
Благодаря YARN - yet another resource negotiator
![[Pasted image 20220410194316.png]]



## W2L105. MapReduce Streaming, решение задачи Line Count


MapReduce Streaming

Вспомним еще раз как обрабатываются данные
![[Pasted image 20220410200310.png]]

В яве всё просто, там надо написать только 2 функции. 
В случае питона всё немного сложнее: 

![[Pasted image 20220410201010.png]]

Что бы сгладить просадки в производительности из за  дополнительных процессов и пересылок данных между ними надо обрабатывать данные батчами в потоковом режиме. 
![[Pasted image 20220410201346.png]]

Разберём простой пример подсчета строк в датасете:

На каждом маппере считаем число обработанных строк, на редьюсере складываем все полученные числа и сохраняем ответ

В качестве тестового датасета - случайная выборка статей из википедии.
датасет такой: в каждой строке - целая статья без знака переноса строки.

Для запуска yarn приложения, нужно в консоли набрать 
`yarn jar`  + путь до jar файла с решением
![[Pasted image 20220410201824.png]]

так как это стандартное стриминговое приложение, то такой jar файл уже существует и по дефолту входит в сборку hadoop

что бы его найти надо пользоваться locate find или спросить в чатике.

Для решения задачи надо определить:
`-input` - вход (это данные в hdfs которые мы обрабатываем)
`-output` - выход (папка в hdfs куда мы положим результат вычислений)

а так же стриминговые скрипты для обработки данных на mapper и reducer.

посчитать чило строчек в чанке можно не написав ни строчким кода на питоне. см. `-mapper`

Что бы запустить приложение без фаз shuffle and sort и reduce нужно выставить флаг `-numReduceTasks 0`

![[Pasted image 20220410202506.png]]

Запускаем и смотрим что вышло на выходе

В папке hdfs увидим 3 файла:
1) пустой саксесс файл (приложение успешно отработало)
2)  и 3 это результаты работы редьюсеров от 2х мапперов.  

![[Pasted image 20220410202539.png]]

если запустим run.sh без изменений повторно то выхватим ошибку что выходная папка уже существует
![[Pasted image 20220410202934.png]]

то добавим такую строчку в скрипт 
![[Pasted image 20220410203006.png]]

Что бы был 1 выходной файл, можно (так как выход у мапперов маленький) явно прописать число редьюсеров `-numReduceTasks 1`
и тогда все данные приедут на 1 редьюсер. (но только если это обдуманное решение)

ну и сам редьюсер который читает числа на входе, последовательно их складывает и выдаёт ответ
![[Pasted image 20220410203403.png]]

Тогда на выходе получим вот это: 
![[Pasted image 20220410203445.png]]

Можно спрятать в файл наши функции
![[Pasted image 20220410203541.png]]

и в run.sh говорим что мы используем доп. файлы для работы 
![[Pasted image 20220410203639.png]]

Скрип так же нужно сделать исполняемым. это делается с помощью утилиты chmode
![[Pasted image 20220410203757.png]]
`a+x` - означает добавить право x(execute) на указанные файлы для a(all) 

![[Pasted image 20220410204005.png]]



















































