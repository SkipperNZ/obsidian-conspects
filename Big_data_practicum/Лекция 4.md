
[[Big_data practicum made]]

Гайд и тест по SQL

https://www.w3schools.com/sql/sql_quiz.asp
https://www.w3schools.com/sql/


# Лекция 3

**SQL поверх больших данных (Hive)**

**Пример 1**
Допустим хотим персонализировать выдачу. 

Есть 2 папки в hdfs. 

В первой хранятся  все логи запросов к сайту. Это acсess логи и содержат информацию о том кто, когда, с какого ip и на какую страницу сайта заходил.

Во 2ой папке hdfs хранится геобаза (в первом приближении это соответствие каждому ip адресу какой либо географической локации)
![[Pasted image 20220425090436.png]]

У нас есть производство обуви, и надо узнать в каком объеме, куда какую партию везти. 

План вычислений MapReduce задач что бы посчитать число обращений к сайту из топ-100 городов по числу запросов. 

Сначала делаем join по ip обоих датасетов, потом решаем задачу wordCount и потом выбираем топ-100 регионов(sort+limit)
![[Pasted image 20220425090835.png]]

Это долгая задача на часы, но при этом она на изи решается с помощью Hive QL 
Делаем join двух таблиц по условию ON (сравнение ip адресов)
Группируем по регионам
Агрегируем число hit из логов  (COUNT(1) as hit_count)
Сортируем по частоте встречаемости ORDER BY hit_count
и выбираем топ-100 LIMIT 100
![[Pasted image 20220425100215.png]]

**Пример 2.**
Доля роботных запросов.

Хотим отделить запросы пользователей от запросов краулеров и спайдеров.
То есть мы хотим почистить аналитику, и посчитать, сколько запросов приходит от людей, а сколько от роботов. 
При этом у нас есть роботный датасет, где каждому боту, ставится в соответствие набор ip адресов, с которых он может приходить и его user_agent (так называемая подпись) в которой есть информация о нашей ОС и браузере. 

План MapReduсе вычислений в этом случае
тут почти то же самое, но сложный ключ для join

![[Pasted image 20220425104634.png]]

И аналогичный запрос на HIVE: 
![[Pasted image 20220425104816.png]]


**Пример 3**
Гендерное распределение
Работа с тремя датасетами в hdfs.

После ответов на предыдущие вопросы, интересуемся, а какое распределение аудитории. 
Добавляем датасет с персональной информацией, где у каждого пользователя будет свой `user id`  и информация о поле и возрасте.
![[Pasted image 20220425105753.png]]

В логах, в запросах, для каждого залогиненого пользователя отметим, что это именно этот пользователь в указанное время рассматривал ту или иную страницу сайта. 

Задача: посчитаем для каждого региона сколько запросов было от мужчин, а сколько от женщин. 

План решения в парадигмек MapReduce:
![[Pasted image 20220425110217.png]]

И с точки зрения Hive QL:
![[Pasted image 20220425110347.png]]

**Пример 4**
Средний возраст клиента.

Вместо расчета распределения по полу, посчитаем средний возраст аудитории в каждом регионе.  

Аналогичное решение, но вместо расчета word count мы будем считать среднее. и среднее считается с помощью комбайнера. 
отдельно считается числитель и знаменатель. 
![[Pasted image 20220425111145.png]]

![[Pasted image 20220425111259.png]]


### W4L102. Map-Side Join

Разберём первый тип join для big data:
![[Pasted image 20220425112027.png]]

Возьмем публично доступный телекоммуникационный датасет 
Он представляет из себя 2 набора. 
Большой датасет:
Агрегированные access логи которые содержат информацию о трафике в разных частях города. (например, сколько смс было получено и отправлено, сколько входящих и исходящих звонков было произведено, объем мобильного трафика)  
![[Pasted image 20220425112323.png]]

Маленький датасет:
Содержит мета информацию по этому датасету. 
Каждый грид - многоугольник определяемой телевышкой в локации.
![[Pasted image 20220425112404.png]]

Наша задача - имея эти 2 датасета сделать join максимально эффективным способом. 

![[Pasted image 20220425112658.png]]

Так как датасет с метаинформацией маленький, то загружаем его в distributed кэш. 
Огромный бонус - мы производим все вычисления на базе map без shuffle and sort и без reduce

Пример того, как запускать наше приложение, и данные в кеш можно загружать не локально, а сразу с hdfs
![[Pasted image 20220425112852.png]]
 
Возвращаясь в Hive QL отметим, что фреймворк самостоятельно пробует транслировать на запрос  в map-side join если один из датасетов маленький. (определение числа маленький зашито в конфиге хайв в количестве мегабайт, и мы можем его сами крутить и менять)

![[Pasted image 20220425113149.png]]

Hive закладывает данные в distributed кэш в 3 шага:
Сначала он скачивает маленький датасет локально, 
затем он строит хэш таблицу для использования join внутри jvm
затем этот слепок хэш таблицы он кладет в distributed кэш что бы не тратить время на построения этой таблицы внутри каждого контейнера, который будет обрабатывать сплиты большого датасета. 
Загвоздка в том, что мы не всегда можем построить хэш таблицу в ram на локальной машине, откуда стартуем hive приложение. если машина загружена и не может построить эту хэш таблицу то будет запущена неоптимизированная версия join.
(а в спарк так вообще приложение отвалится по out of memory exeption)


### W4L103. Reduce-Side Join
Расширим пример с анализом телекоммуникационных логов 

Допустим город не один, или вышек в нем слишком много и датасет с метаинформацией слишком большой и не влезает в ram.


![[Pasted image 20220425135315.png]]

Классический join по этой схеме выглядит так: 
Помечаем каждый датасет каким либо тегом что бы на редьюсере отличать какие данныепришли из какого датасета. 
Ключ у нас сложный. он включает ключ для джоина и тег

При работе каждый воркер получает метаинформацию по сплиту по которому он работает, в рамках этой метаинформации можно вытащить информацию по пути в hdfs из которого этот сплит получен. На основе этого пути можно легко принять решение, что нужно пометить эту запист тегом "grid" или "logs"
В маппере так же проведём все необходимые вычисления, для grid записей, посчитаем, находимся мы на сереве или на юге, что бы потом можно было сравнить аналитику по более крупным регионам. 
А для logs записей оставим только одну колонку с интересующей нас статистикой, что бы не тратить лишние ресурсы на таскание ненужных данных. 
![[Pasted image 20220425135626.png]]

Когда мы запустим наше приложениебез фазы shuffle and sort
то увидим в выводе интересующую статистику для log записи и отметку грида для записи из другого датасета. 
Классический sql тут ломается так как мы не выдерживаем даже базового  требования того, что колонки одного типа 
![[Pasted image 20220425140512.png]]
Накинем редьюсеров и посмотрим на данные которые приедут на редьюс  до агрегации данных(до операции join):
![[Pasted image 20220425140827.png]]
Для того, что бы произвести join нужно все записи с одним ключем загрузить в оперативную память. 
При  этом оперативную память необходимо экономить и обрабатывать данные максимально в стриминговом режиме.
Зная что в одной из таблиц всего одна запись по каждому ключу, мы можем оптимизировать эти вычисления: 
Надо решить задачу вторичной сортировки(secondary sort) 
То есть мы определяем сложный ключ из 2х колонок, сортируем по обеим колонкам, но хэшируем данные только по первой колонке, что бы гарантировать распределение данных по редьюсерам правильно 
![[Pasted image 20220425141244.png]]
ну и сортировка в обратном порядке: 
![[Pasted image 20220425141511.png]]


### W4L104. Bucket Map- Side Join + оптимизации
Оказывается что между большими и малыми есть средние датасеты, и работу с ними можно тоже оптимизировать. 

Данные в Hive могут быть разложены по папкам, в соответствии с какой либо логикой, например если это логи, то их удобно складывать по дням, а если их слишком много, то можно раскладывать по дням, а в каждом дне иметь еще вложеные папки с часами. (это называется секционирование (partitioning))
![[Pasted image 20220425142432.png]]

В дополнении к этой технике есть еще и бакетирование(bucket)
В каждой папке мы задаем количество бакетов (по факту -файлов в которой все данные, соответствующие этой партиции  будут складываться )
Определять какие данные в какой бакет направлять, будем на основе хеша  который мы используем в mapreduce в рамках фазы shuffle and sort
С точки зрения задания схемы данных в hive, пишем, что данный clustered by ключ into число buckets
![[Pasted image 20220425144251.png]]
Оказывается что если у нас 2 датасета похешированы по одному ключу, на одинаковое число бакетов, и если один из датасетов такого размера, что каждый их его бакетов в отдельности помещается в оперативную память , то можно применить технику называемую bucket map-side join 
В рамках этой версии join мы произведём все вычисления в рамках только фазы map.
Каждый воркер, будет загружать только 1 бакет в память, а второй - стримить 
Бонус: если число бакетов не одинаковое, но кратное, то всё равно можно применить оптимизированную версию join 
![[Pasted image 20220425144232.png]]
Распределение данных по бакетам производится с помощью вычисления хеша и взятия операции деления по модулю числа редьюсеров (то есть по числу бакетов)
Таким образом, если у нас в датасете 64 бакета а в другом 128 то каждому бакету из первого датасета можно легко поставить соответствие ровно 2 бакета из 2ого.

Разберём 3 примера:

В первом датасете 2 бакета, во 2ом 4
![[Pasted image 20220425145049.png]]

Если в одном датасете 2 бакета а в другом 10 то тоже всё ок 
![[Pasted image 20220425145134.png]]


А если 3 бакета слева и 5 справа то их наибольший общий делитель - еденицаи никакой оптимизации не будет 
![[Pasted image 20220425145254.png]]

Вывод  
Всегда бакетировать на число бакетов какой либо степени 2ки 
![[Pasted image 20220425145324.png]]

Оптимизированая версия bucket map -side join называемая smb:

Допустим у нас данные не только бакетированые на одинаковое число бакетов в обоих датасетах, но и отсортированы по ключам 
Мы можем не грузить один из бакетов в ram, а воспользоваться техникой external merge sort
В этих случаях мы будем делать join обоих бакетов на стриминге, читая в ram только те записи из обоих бакетов которые соответствуют ровно одному ключу
![[Pasted image 20220425145818.png]]

![[Pasted image 20220425150040.png]]

В зависимости от версии hive на кластере, она может быть включена или выключена по умолчанию.
проверка включения
![[Pasted image 20220425150141.png]]


Теперь разберём как понимать и дебажить Hive запросы:

### W4L105. Немного внутренностей Hive DDL и HiveQL

Hive состоит из хранилища мета информации по данным в hdfs и инструмента для трансляции и запуска Hive QL запросов в парадигме mapReduce вычислений. по умолчанию используется хранилище hdfs и движок yarn для запуска mapReduce вычислений.
Но легко настроить hive на работу поверх данных в облаках (например s3) или запуск вычислений с помощью  движка tez или spark
![[Pasted image 20220425150615.png]]

Язык запросов для задания мета информации называется  Hive DDL
(от data difinition language)
Существует еще Hive dml (data manipulation language) это инструмент по загрузке и  выгрузке данных между клиентской машиной и hdfs с помощью hive.

Разберем пару полезных примеров для лучшего понимания hive QL:
![[Pasted image 20220425151251.png]]
ORDER BY опасная штука, которая говорит что все данные поедут на 1 редьюсер и для выполнения глобальной сортировки и его следует применять если мы уверены, что у нас немного данных 
SORT BY делает сортировку данных внутри каждого редьюсера которую аздает компаратор 

Прежде чес запускать наш Hive QL запрос и тратить время добавляем ключевое слово **EXPLAIN** перед запросом и посмотрим на план решения.
В выводе увидим 3 плана 
1 - каким образом наш запрос распарсился hive (он нам не нужен)
2 - Сколько mapReduce задач будет запущена и какие у них зависимости каждая задача называется stage
3 - детализированный план каждого stage. откуда читаем данные, какие фазы map reduce  запускаем, и запускаем ли вообще.

![[Pasted image 20220425152019.png]]

















































































