[[Big_data practicum made]]



# Лекция 5
“Spark: from zero to hero”


  

### W5L102. Архитектура Spark-приложения и Spark RDD

Какие есть ограничения MapReduce:
Между любой фазой MapReduce задач все данные (и временные) которые передаются между фазами  они сериализуются и десериализуются (запаковываются в какой то бинарный формат, сохраняются на жёсткий диск, вероятно передаются по сети, снова сохраняются на жесткий диск и только после этого попадают в оперативку) 

Основная оптимизация при работе с большими данными почти всегда упирается в так называемый IOps это объемы данных которые мы сохраняем или считываем с жестких дисков или по сети. 
Есть ряд задач которые под это не оптимизированы, например различные задачи где мы решаем задачи оптимизации (большая часть задач по мл, работа с различными графами где мы храним что то в оперативке и делаем какие то итерации по обновлению наших знаний) и если мы на каждой итерации будем  скидывать данные на хард и потом поднимать то это будет медленно. 

Предложение:
Давайте мы научимся хранить данные в оперативке и к этим данным применять преобразования. 
Мы так же остаемся в функциональной парадигме, где у нас есть неизменяемые объекты и мы к этим объектам будем применять какие то трансформации. 

Как мы можем раскидать данные по кластеру и как это может жить. 


Архитектура спарк приложения:
![[Pasted image 20220506023232.png]]

Для запуска Spark приложения есть компонент, называемый Driver.
Он содержит SparkContrxt - это точка входа для работы с нашими Executor'ами которые применяют вычисления непосредственно к хранящимся данным. 
Есть рабочие лошадки - Worker Node которые могут запускать executor'ы (по факту контейнеры) но тут task'и, будут работать каждый со своим сплитом  данных. 
Task - преобразование которое мы делаем над нашими данными к какому то сплиту данных, который тут будет называться партицией.

В отличае от MapReduce эти Executor'ы будут долгоживущие. мы будем запускать не каждый отдельный на обработку какого то сплита и потом умирать. Мы данные Task можем хранить в executor'e и накатывать к ним необходимые преобразования. 

Соответственно у нас есть драйвер, который управляет всем процессом обучения (это будет интерактивная консоль) которая говорит о том что мы хотим совершить необходимые преобразования и мы хотим запустить сколько то executor'ов  выдаём ограничения сколько ядер нам нужно на каждый executor, сколько параллельно сплитов данных мы там можем обрабатывать и начинаем  распределять данные. хранить их в оперативке и применять к ним какие то функции, так же есть возможность хранить данные между какими то стадиями обработки. 

Какие есть **Cluster Manager** 
По умолчанию есть 3:
- stand alone - то что входит в базовую поставку спарка, то что можем  развернуть на локаоке (не нужен хадуп кластер)
- yarn который мы видели. Можем запускать наши вычисления поверх hadoop кластера и запрашивать у resourse менеджера необходимые ресурсы что бы запустить необходимые контейнеры для обработки данных. 


Дальше в видео гайд как запустить спарк приложение через jupyter.


Базовый концепт для работы со спарк приложением - Rdd
Rdd - Resilient Distributed Dataset

Resilient - устойчивый
Distributed - распределённый
Dataset - датасет

Есть несколько форматов получения данных в рамках нашего спарк приложения или выгрузки (загрузки/выгрузки данных)

1. 
```
fruit_rdd = sc.parallelize(["Apple", "Banana", "Orange"])
fruit_rdd
```
в паралелайз передаём любой iterable где мы попросим питоновские объекты передать в спарк.

Что бы получить эти данные и увидеть их на экране:
```
fruit_rdd.collect()
```
и тогда мы увидем содержимое этого Rdd
 
Это ключевой инструмент как мы можем убить наше приложение-драйвер
если у нас есть большой Rdd который содержит несколько петабайт данных и мы просим его сделать коллект, то все эти данные со всех экзекьюторов поедут на драйвер.
поэтому коллект надо делать если вывод небольшой.

А как эта коллекция у нас распределена по экзекьюторам?

Мы знаем что запустили приложение с 2мя экзекьюторами и хотим понять как по ним распределены данные. 

по умочанию спрашиваем у rdd сколько у нас патриций(конкретный сплит данных)
```
fruit_rdd.getNumPartitions()
```
видим что у нас 2 сплита
Если мы хотим понять распределение по сплитам, то 
```
fruit_rdd.glom().collect()
```
и увидим как эти данные распределены по экзекьюторам.
![[Pasted image 20220506044342.png]]

Обратим тут внимание вот на что:
Что то про паралелизм. 
Неизменяемые коллекции. мы живём в той же функциональной парадигме, данные мы не изменяем, но можем применить к ним преобразования, что бы получить новые данные.
И только в конечный момент, когда хотим получить финальный ответ, все вычисления происходят.


Как считывать данные с внешних источников? Например из HDFS.
```
stop_words_rdd = sc.textFile("hdfs:///data/stop_words")
print(stop_words_rdd.getNumPartitions())
print(stop_words_rdd.count()) # посчитает сколько там данных
print(stop_words_rdd.take(10)) # выведет первые 10 объектов
```

take пойдёт на какую то случайную партицию и возьмет от туда 10 первых элементов
![[Pasted image 20220506045711.png]]
У спарк контекста sc есть метод textFile. а так же есть много библиотек, позволяющих читать не только из hdfs но и из других хранилищ, таких как s3 и aws 


Допустим мы понимаем что для паралелизма нам необходимо изменить распределение данных, для этого применяем преобразование repartition(10): 
```
stop_words_rdd.repartition(10)
print(stop_words_rdd.getNumPartitions())
print(stop_words_rdd.count()) # посчитает сколько там данных
print(stop_words_rdd.take(10)) # выведет первые 10 объектов
```
И тут будет снова 1 партиция (функциональная парадигма - мы не изменяем существующие объекты)

```
stop_words_rdd = stop_words_rdd.repartition(10)
print(stop_words_rdd.getNumPartitions())
print(stop_words_rdd.count()) # посчитает сколько там данных
print(stop_words_rdd.take(10)) # выведет первые 10 объектов
```
А так сработает, мы создали новый rdd.
![[Pasted image 20220506050148.png]]

Так же данные можно выводить не только на экран/драйвер, но и по аналогии  с textfile сохранять их  в hdfs и другие внешние источники. И тут есть для этого куча библиотек.

Запустив приложение, мы отжираем ресурсы кластера, поэтому надо не занимать долго ресурсы
```
sc.stop()
```

и что бы работать снова надо будет создать новый sc загрузить в него данные и начать работать.


Теперь разберёмся как оно работает под капотом, как этим пользоваться.


### W5L103.Spark RDD, преобразования (transformations) и действия (actions)

Разберём что такое трансформации и преобразования.

**Hello, world! in Spark** (тот же word count)
```
sc
```

В чем разница между преобразованиями и действиями. 
У нас существует Rdd - неизменяемая коллекция объектов. и мы к ним можем клеить разные преобразования. (строить план запроса)

Ни одно из этих  преобразований выполнено не будет ровно до тех пор,  пока мы не попросим получить какие то данные.
Данные можно получить либо в формате выгрузки в какое то внешнее хранилище(hdfs или локальный файл или база данных),  либо показать на экране нашего терминала(передать их на драйвер) 
actions которые мы видели - collect который собирает всю коллекцию данных 

в случае когда датасет большой и мы находимся в середине нашего эксперимента мы иногла хотим сделать что то типа дебага что бы понять какие предбразования можно сделать 

Возьмем на пример датасет википедии и посмотрим на один из его элементов. 
.first() - actions - попросить первый элемент и сразу ограничим вывод до 100 символов
```
wiki_rdd = sc.textFile("hdfs:///data/wiki/en_articles_part")
wiki_rdd.first()[:100]
```
![[Pasted image 20220506145634.png]]


Как мы это можем попарсить. 
берём rdd применяем к нему трансформацию map
берём rdd сполитим по знаку табуляции, разделяя на пары ключ значения 
ключ - id статьи а значение - всё содержимое
нас интересует только содерждимое, приводим все к нижнему регистру
content - одна строка приведённая к нижнему регистру, мы её просто сплитим по всем пробельным символам

```
words_rdd = ( 
	wiki_rdd 
	.map(lambda x: x.split('\t', 1)) 
	.map(lambda pair: pair[1].lower()) 
	.map(lambda content: content.split()) 
	)
```
если мы вызовем этот код, то никаких вычислений на кластере производится не будет. и времени тоже никакого не уйдёт - это просто план вычислений. 

```
type(words_rdd.first())
```

попытаемся вывести первый элеменет еще раз
```
words_rdd.first()[:10]
```
и тут уже происходит активные вычисления. 
в результате уже будет список объектов (потому что content.split() возвращает список)
![[Pasted image 20220507165925.png]]

Мы хотим по каждой статье просуммировать количество слов которое встречалось. для этого нужно сделать эту коллекцию плоской(flatMap) что бы itarable по объектам был в общем iterable.

```
words_rdd = (
    wiki_rdd
    .map(lambda x: x.split('\t', 1))
    .map(lambda pair: pair[1].lower())
    .flatMap(lambda content: content.split())
)
```

это уже будет str
```
type(words_rdd.first())
```

```
words_rdd.first()
```

```
words_rdd.take(10)
```
![[Pasted image 20220507171427.png]]

итак мы нашли слова, посплитили, и надо сделать какую то агрегацию, и за агрегацию тоже отвечает транеформация
Эта трансформация - reduceByKey
делаем доп. преобразование .map(lambda x: (x, 1)) разбивая коллекцию на пары ключ-значение (слово, 1 )
теперь все эти единички надо просуммировать  
reduceByKey
```
word_count_rdd = (
    words_rdd
    .map(lambda x: (x, 1))
    .reduceByKey(lambda x, y: x + y)
)
```

```
word_count_rdd.take(10)
```
![[Pasted image 20220507182211.png]]

Теперь хотим посчитать какую то полезную аналитику.
что бы посчитать на драйвере и не убить всю машину есть специальный экшон takeOrdered
```
word_count_rdd.takeOrdered(10, key=lambda x: -x[1])
```
![[Pasted image 20220507182849.png]]

Слова как минимум имеющие 5 символов 
```
word_count_rdd.filter(lambda x: len(x[0]) > 5).takeOrdered(10, key=lambda x: -x[1])
```
![[Pasted image 20220507183114.png]]

Сколько вообще слов было в датасете 
```
word_count_rdd.map(lambda x: x[1]).sum()
```
![[Pasted image 20220507183241.png]]

После преобразований сохраняем результаты вычислений в hdfs
каждый элемент коллекции будет переводится в стринг
```
word_count_rdd.saveAsTextFile("hdfs:///user/aadral/word_count_dataset")
```

```
!hdfs dfs -ls /user/aadral/word_count_dataset
```
![[Pasted image 20220507183500.png]]

```
!hdfs dfs -text /user/aadral/word_count_dataset/part-00000 | head
```
![[Pasted image 20220507183530.png]]

Удаляем данные что бы не мешались.
```
!hdfs dfs -rm -r -skipTrash /user/aadral/word_count_dataset
```


Теперь разберём как происходят эти вычисления. 

Когда мы указываем какие либо трансформации, то мы показываем путь преобразования одного rdd(неизменяемой коллекции) в другой 

Rdd по факту разделены на какие то партиции.
Потом применили какой то фильтр, получили новый Rdd который тоже разделён на какие то партиции
с map всё так же
сделали агрегацию по ключам и получили новый rdd  
![[Pasted image 20220507183920.png]]

Преобразования могут быть узкими, а могут быть широкими.

узкие - мы взяли какой то сплит данных и его мы можем независимо обработать и получить новую партицию по факту не пересылая данные по сети. (типо разделение строк на слова)
тоесть мы не сериалезуем/десекриалезуем данные. и не трогаем их пока мы не вызвали экшон.

Широкие - что бы посчитать результат надо объединить результаты или данные находящиеся в разных партициях с предыдущего Rdd.

Соответственно на основе этих узких и широких преобразований выделяются stage - набор преобразований который можем сделать не пересылая данные по сети. (шафлы)
И именно  на минимизации шафлов идёт вся оптимизация спарк вычислений. 


![[Pasted image 20220507184223.png]]

Есть ряд комманд 
![[Pasted image 20220507184921.png]]


на их основе задача разделилась на 3 стейджа
![[Pasted image 20220507184955.png]]

Какое преобразование в рамках какого стейджа будет посчитано?

тут документации к трансформациям.
-   [https://spark.apache.org/docs/latest/rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html)




### W5L104. Spark PairRDD, Join'ы и Cache
**Spark Cache**

Заводим спарк сессию
``` python
sc
```

Простой текстовый файл из 3х колонок
буквы/числа/слова
```python
%%writefile simple.txt
a 1 first
b 2 second
c 3 third
```

сохраним в hdfs
```python
%%bash
hdfs dfs -rm -r -skipTrash simple.txt
hdfs dfs -put simple.txt .
```

читаем в rdd
```python
simple_rdd = sc.textFile("hdfs:///user/aadral/simple.txt")
```

первое преобразование(не вызывает вычисления)
```python
favourite_letters_rdd = simple_rdd.map(lambda x: x.split())
```

вычисление происходит тут
любые преобразования в момент вычислений производятся заново
```python
favourite_letters_rdd.first()
```
![[Pasted image 20220508024658.png]]

По умолчанию ничего не закешировано
![[Pasted image 20220508024608.png]]
обновляем текстовый файл, обновляем его в hdfs  считаем так же и вызываем first и получаем обновлённый ответ. simple_rdd не закеширован. 
![[Pasted image 20220508024750.png]]

добавим кеширование какого либо rdd.
кеширование позволяет сохранить промежуточные результаты вычислений, что бы все последующие вычисления производились быстрее
(мы готовим большой агрегат, а потом считаем быстрые статистики и что бы для них пайплайн не производить заново кешируем его и пользуемся)
```
simple_rdd.cache()
```
после этого simple_rdd.is_cached будет True
но его в памяти пока не будет, оно закешируется только когда первый раз дойдёт до этой трансформации.
![[Pasted image 20220508025329.png]]
теперь меняя датасет, сохраняя его в hdfs он все равно будет выдаваться закешированым.

если хотим вернуть обратно незакешированую версию, то нужно будет вызвать `simple_rdd.unpersist()`  и снова всё обновится из датасета.
```python
# simple_rdd.is_cached
# simple_rdd.cache()
# simple_rdd.unpersist()
```

**Spark Implicit Cache**

Датасет википедии
```python
wiki_rdd = sc.textFile("hdfs:///data/wiki/en_articles_part")
```

сплитим на идентификаторы и содержимое 
приводим к нижнему регистру 
оставляя содержимое 
сплитим по пробелам 
```python
words_rdd = (
    wiki_rdd
    .map(lambda x: x.split('\t', 1))
    .map(lambda pair: pair[1].lower())
    .flatMap(lambda content: content.split())
)
```

разделяем на ключ значения и ссумируем
```python
word_count_rdd = (
    words_rdd
    .map(lambda x: (x, 1))
    .reduceByKey(lambda x, y: x + y)
)
```

кешируем 
```python
word_count_rdd.cache()
```

это занимает первый раз много времени  ~20 секунд
вызвав его повторно ~158мс так как закешировано
```python
%%time
print(word_count_rdd.is_cached)
print(word_count_rdd.first())
print()
```
Сделав анперсист результат вычисления всё равно будет как не странно быстрым
в гайде всё описано 
В приписке написано что спарк тоже может кешировать временные операции после шафл операций (прим. reduceByKey) и спарк это сделал сам, но лучше делать это явно.
-   [https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)

```python
word_count_rdd.unpersist()
```


**Spark Broadcast**

Как удобно распределять данные что бы они были доступны на каждом екзекьюторе.

рассмотрим на основе файла из стоп слов. и профильтруем какие ни будь данные(статистику по встречаемости слов)

читаем данные из hdfs 
загружаем его в оперативную память на драйвере с помощью .collect()
затем пакуем обратно в sc.broadcast() и это будет специализированная переменная, которую можно использовать во всех mapReduce вычислениях 
```python
stop_words_rdd = sc.textFile("hdfs:///data/stop_words")
stop_words_broadcast = sc.broadcast(stop_words_rdd.collect())
```

В этот момент мы раскидываем этот датасет на все екзекьюторы. 
бонусом для больших кластеров, и если этот датасет большой он работает по формату p2p протокола, и очень быстро раскидывает его внутри по екзекьюторам.

Что можем делать с этим датасетом.
можем к нему обращаться через stop_words_broadcast.value 
Это обращение к локальной оперативке на екзекьюторах(не надо дополнительно складывать никуда)

Фильтруемся по всем словам, которых нет в этом списке стоп слов. И смотрим самые популярные слова по этой статистике.
```python
word_count_rdd.filter(lambda x: (x[0] not in stop_words_broadcast.value)).takeOrdered(10, key=lambda x: -x[1])
```

хоба
![[Pasted image 20220508031617.png]]

 \+ с помощью broadcast можно реализовать map Side Join



**Spark Joins and PairRDD**

Как работают джоины в спарк, и какие виды бывают.

Что бы что то джойнить нам нужны пары ключ-значение
Нужно задать сущность в спарке называемую pairRdd
Это есть обозначение произвольное sparkRdd которое которое гарантирует что в ней есть хотя бы 2 элемента.
Первый считаем ключем, второй - значением.
-  [https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)


Приведём верхний датасет к виду 
первая буква слова / слово / сколько раз слово встретилось

Это для того что бы можно было поджойнить с датасетом из начала урока 
![[Pasted image 20220508040805.png]]

```python
tripple_dataset = word_count_rdd.map(lambda x: (x[0][0], x[0], x[1]))
tripple_dataset.first()
```


```python
favourite_letters_rdd.first()
```


Вот тут джойним.

```python
favourite_letters_rdd.join(tripple_dataset).take(5)
```
![[Pasted image 20220508040934.png]]

Неочевидно. 
Джойним датасеты по первой колонке на выходе - только 2ые элементы их каждых датасетов и всё что после - спарк потерял


Что бы с этим бороться делается преобразование.
Запакуем все оставшиеся элементы в этой паре во внутренний тупл

```python
tripple_dataset = word_count_rdd.map(lambda x: (x[0][0], (x[0], x[1])))
tripple_dataset.first()
```


```python
favourite_letters_rdd.join(tripple_dataset).take(5)
```

![[Pasted image 20220508043129.png]]




И закрываем сессию
```python
sc.stop()
```
