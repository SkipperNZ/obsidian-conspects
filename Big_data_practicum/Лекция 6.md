
[[Big_data practicum made]]



![[spark_dataframe_sql.ipynb]]
# Лекция 6
**Spark DataFrames, Spark SQL**


###  Spark DataFrames: общие сведения

Датафреймы в спарке - это некоторая структурированная колоночная структура данных, которую можно назвать таблицей. 

Создаётся эта таблица на базе файла или файлов расположенных в директории на распределённой файловой системе или на локальной файловой системе. на базе таблицы из базы данных или же обычную локальную коллекцию

датафреймы под капотом используют RDD api и все его приметивы.

датафреймы в отличие от rdd в питоне работает так же быстро как в скале и ждаве. 

В отличие от rdd датафреймы работают по другому принципу:
Мы описываем некий sql. этот код будет превращён в план выполнения, на основе этого плана будет скомпилирован некий jawa код и будет применён к данным.  а питон просто как обёртка. 

По аналогии с rdd, датафреймы это ленивая и неизменяемая коллекция. 


### W6L103. Базовые функции Spark DataFrames

Датафрейм - таблица состоящая из колонок, к каждой есть имя и тип. И строки с данными.

Создадим список словарей.
В нем будут примерно одинаковые структуры у данных, но некоторые ключи в некоторых строках будут пропущены.
и иногда будут дубликаты

с помощью `rdd = sc.parallelize(test data)` создадим rdd из этой локальной коллекции и затем превратим в датафрейм с помощью 
`df = spark.read.json(rdd)
На выходе получаем датафрейм, он будет распределённым, партиционированным и все основы для rdd будут справедливы и для датафреймов. 
![[Pasted image 20220513164735.png]]

Выполним базовые операции с нашим датафреймом. 

выведем первые 10 строк на экран.
`df.show(10, False)`
![[Pasted image 20220513165721.png]]

Как работает show. 
Под капотом спарк сначала вычисляет одну из патриций, которую он предпочтет. Смотрит сколько в ней получилось данных, если в ней данных больше чем мы попросили, то обработка останавливается и результат передаётся на драйвер. Если же данных не достаточно, то тогда спарк обрабатывает одну или несколько партиций и повторяет последний шаг. 
Таким образом минимизируются вычисления. 
`False и True`  вторым аргументом отвечают за обрезание длинных строк (если например есть колонка с типом стринг и есть в ней очень длинная строка, то если мы передаем `True` то она будет обрезана)

Можем посмотреть схему датафрейма с помощью метода `df.printSchema()`
![[Pasted image 20220513170415.png]]

Он возвращает схему в удобно читаемом  виде. 
Отметим что спарк легко поддерживает сложные схемы, то есть колонки состоящие из словарей, массивов, структур. 

Для того что бы выбрать одну или несколько колонок, как и в sql используем метод `SELECT`
Он принимает на вход одну или несколько колонок (большинство функций в спарке работает с типом колонка, мы передаём внутрь колонку - континент)
```python
from pyspark.sql.functions import *

df.select(col("comtinent"), col("country")).show(10, False)
```
Получаем новый датафрейм состоящий только их этих 2х колонок. 
![[Pasted image 20220513171155.png]]

Метод `filter` позволяет фильтровать датасет.
Получим только те строки, в которых континент - европа.

```python 
df.filter(col("continent") == "Europe").show(10, False)
```
![[Pasted image 20220513171440.png]]

А с каким параллелизмом был применен фильтр к данному датафрейму.
```python
df.rdd.getNumPartitions()
```
![[Pasted image 20220513171546.png]]
Параллелизм ограничен сверху количеством партиций, но не более чем суммарное количество ядер всех воркеров в нашем приложении. У нас  коллекция была создана ла основе локальной коллекции, то количество партиций в датафрейме будет равно сумме ядер на всех воркерах. 
У нас партиций 2 - это значит что любые операции не использующие shuffle выполняются в 2 потока 

### W6L104. Работа с пропущенными значениями (NA) в Spark DataFrames

**Очистка данных**

Тут возникает эффект shuffle, что бы удалить дубликаты их нужно переместить на один воркер на одну партицию. происходит неявный шафл. ключ все колонки, по этому ключу берётся хеш и по этому хэшу перемешиваются данные, одинаковые хеши попадают в одинаковые партиции, и уже в них удаляются данные. 
Так же можно передать в этот метод параметр - список колонок по которому нужно делать дедубликацию и это удобно когда в датафрейме есть колонка которая является уникальным ключем.
Удалим дубликаты. 
По умолчанию метод  `dropDuplicates` удаляет дубликаты строк, у которых ВСЕ колонки совпадают.
```python
df.dropDuplicates().show(10, False)
```
![[Pasted image 20220514002935.png]]

Метод `.na.drop` удаляет СТРОКИ, в которых отсутствует часть данных. Параметр `how="all"` (есть вариант с any - хотя бы один аргумент null) означает, что будут удалены строки, у которых ВСЕ колонки `null` вторым аргументом можно передать список колонок по которым происходит проверка.
```python
df.dropDuplicates().na.drop(how="all").show(10, False)
```
тут кстати всё работает быстро
![[Pasted image 20220514003238.png]]

Метод `.na.fill` заполняет `null`. Для работы этого метода требуется словарь с изменениями
```python
fill_dict = {'continent': 'n/a', 'population': 0}

df.dropDuplicates().na.drop(how="all").na.fill(fill_dict).show(10, False)
```
![[Pasted image 20220514003549.png]]


Метод `.na.replace` заменяет данные в колонках. Для его работы требуется словарь с заменами
```python
replace_dict = {'Rossiya': 'Russia'}

df.dropDuplicates().na.drop(how="all").na.fill(fill_dict).na.replace(replace_dict).show(10, False)
```
![[Pasted image 20220514003842.png]]



Подготовим датафрейм с очищенными данными.
Дополнительно наложим фильтр в котором укажем что популяция должна быть >0
И потом применим селект и выберем несколько колонок в нужном нам порядке
![[Pasted image 20220514004033.png]]
ячейка выполняется мгновенно, так как это просто трансформация
![[Pasted image 20220514004134.png]]

![[Pasted image 20220514004202.png]]


### W6L105. Группировки в Spark DataFrames

Подготовим базовый агрегат. По умолчанию имена колонок принимают неудобные названия

работает также как groupBy во всех базах данных

на входе название колонки по которой нужно сгруппировать данные("continent")
затем используя метод `.agg` передаем колонки по которым мы хотим посчитать агрегат
`count("*")` и сумма по колонке популэйшн `sum(col("population"))`
 все функции нестандартные а импортированы из пакета.
 так же неявный шафл.
 но для ускорения группировка разбита на 2 этапа. 
 на первом этапе - частичная группировка, на которых наши функции `count("*"), sum(col("population"))` вычисляются внутри каждых партиций, сокращая их размер 
 после наступает 2ой этап. уже как обычно
```python
from pyspark.sql.functions import count, sum

agg = clean_data.groupBy("continent").agg(count("*"), sum(col("population")))

agg.show(10, False)
```
![[Pasted image 20220514011828.png]]

Метод `alias` позволяет переименовывать колонки

`.withColumn("continent", lower(col("continent")))` приводит все значения к нижнему регистру.
```python
from pyspark.sql.functions import count, sum, lower

pop_count = count("*").alias("city_count")
pop_sum = sum(col("population")).alias("population_sum")

agg = clean_data \
            .groupBy("continent") \
            .agg(pop_count, pop_sum) \
            .withColumn("continent", lower(col("continent")))
            
agg.show(10, False)
```
![[Pasted image 20220514012358.png]]


### W6L106. Чтение и запись данных в Spark DataFrames

Основной метод чтения любых источников

Чтение происходит на уровне воркеров. Драйвер в работе с данными не участвует.

`df = spark.read.format(datasource_type).option(datasource_options).load(object_name)`

В большинстве случаев чтение источников в датафрейм - это ленивая операция, она не запускает физического чтения с источника. 
Поддержка источников всегда на уровне jvm, питон тут не учавствует.

-   `datasource_type` - тип источника ("parquet", "json", "cassandra") и т. д.
-   `datasource_options` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)
-   `object_name` - имя таблицы/файла/топика/индекса

[DataframeReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader):
-   по умолчанию выводит схему данных
-   является трансформацией (ленивый)
-   возвращает [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)


Список (неполный) поддерживаемых источников данных
-   Файлы:
    -   json
    -   text
    -   csv
    -   orc
    -   parquet
    -   delta

-   Базы данных
    -   elasticsearch
    -   cassandra
    -   jdbc
    -   hive
    -   redis
    -   mongo

-   Брокеры сообщений
    -   kafka

**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**

В качаестве примера прочитаем csv файл расположеный на hdfs
`df = spark.read.format("csv").options(header=True, inferSchema=True).load("/tmp/datasets/airport-codes.csv")`
`header=True` - в файле есть хедер, и на основании него будут названы колонки

`inferSchema=True` - спарк автоматически прочитает и сделает проход по всему файлу и сформирует типы колонок изходя из фактических данных. (случай с нелинивой операцией)

`df.printSchema()`
![[Pasted image 20220514021936.png]]

`df.show(n=1, truncate=False, vertical=True)`
![[Pasted image 20220514022127.png]]



**Запись данных**


Основной метод записи в любые системы

`df.write.format(datasource_type).options(datasource_options).mode(savemode).save(object_name)`

-   `datasource_type` - тип источника ("parquet", "json", "cassandra") и т. д.
-   `datasource_options` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)
-   `savemode` - режим записи данных (добавление, перезапись и т. д.)(error if exists - дефолтное, запись не будет сделана, если объект уже существует.
   append - дозапись
   overwrite - перезапись объекта, опасно. 
   noope режим- записываем в null полезно во всяких тестах)
-   `object_name` - имя таблицы/файла/топика/индекса

так же распределённый процесс на уровне воркеров
далеко не всгда верефицируется формат данных.
пример - если мы имеем каталог на hdfs который содержит orc файлы а пишем датафрейм в формате паркет в этот каталог, то никакой ошибки тут не произойдёт.
Ошибка будет когда мы попытаемся прочитать датафрейм из этого каталога. 


[DataFrameWriter](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter):
-   метод `save` является действием
-   позволяет работать с партиционированными данными (parquet, orc)
-   не всегда валидирует схему и формат данных


Список (неполный) поддерживаемых источников данных
-   Файлы:
    -   json
    -   text
    -   csv
    -   orc
    -   parquet
    -   delta
-   Базы данных
    -   elasticsearch
    -   cassandra
    -   jdbc
    -   hive
    -   redis
    -   mongo
-   Брокеры сообщений
    -   kafka

**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**

попробуем записать наш агрегат в паркет, в режиме overwrite

```python
condition = col("continent") != "n/a"

  
agg \
    .filter(condition) \
    .write \
    .format("parquet") \
    .mode("overwrite") \
    .save("/tmp/agg0.parquet")

print("Ok! Data is written to {}".format("/tmp/agg0.parquet"))
```

```python
# P.S.
# Когда мы делаем .filter в DataFrame API, мы передаем условие типа pyspark.sql.column.Column.

print(type(condition))

# когда раньше мы использовали лямбда функции в RDD, мы передавали лямбда функцию:
condition_old = lambda x: x != "Earth"

print(type(condition_old))
```


### W6L107. Соединения в Spark DataFrames

Join'ы позволяют соединять два DF в один по заданным условиям.
По типу условия join'ы делятся на:

-   equ-join - соединение по равенству одного или более ключей
-   non-equ join - соединение по условию, отличному от равенства одного или более ключей (неравенство, пользовательская функция итд)

![[Pasted image 20220514034647.png]]

По методу соединения join'ы бывают:

При выполнении join Spark автоматически выбирает один [из доступных алгоритмов](https://youtu.be/fp53QhSfQcI) соединения и не всегда делает это оптимально, часто применяя cross join. Поэтому, в последних версиях Spark метод `join()` приведет к ошибке, если под капотом он будет использовать cross join. Отключить эту проверку можно с помощью опции `--conf spark.sql.crossJoin.enabled=true`

В качестве примеров наши датафреймы:
1: наш очищенный clean_data с примененным lower к континентам
2: То что записали в прошлом видео в паркет
```python 
# Для демонстрации работы join используем подгтовленные данные

left = clean_data.withColumn("continent", lower(col("continent")))
left.printSchema()

right = spark.read.parquet("/tmp/agg0.parquet")
right.printSchema()
```
![[Pasted image 20220514035011.png]]

Самый простой join - inner join по равенству одной колонки

```python
joined = left.join(right, 'continent', 'inner')

joined.printSchema()

joined.show(10, False)

```
![[Pasted image 20220514035221.png]]
![[Pasted image 20220514035249.png]]

Как делать join по двум колонкам: 

Inner join по равенству двух колонок. Поскольку двух одинаковых колонок у нас нет, мы создадим их, используя константу
`lit("x")` - константа
```python
from pyspark.sql.functions import lit

new_col = lit("x").alias("x")

left = left.select(col("*"), new_col)
right = right.select(col("*"), new_col)

joined = left.join(right, ['continent', 'x'], 'inner')

joined.printSchema()
joined.show()
```

![[Pasted image 20220514035611.png]]


Теперь перейдём к:
**non-equ left join**

условие - join_condition

у левого датафрейма добавим новую колонку которую назовем `city_count_max` и она будет иметь значение 2. и переименуем континент на континент_лефт

в правом просто переименуем колонку и делаем джоин

```python
from pyspark.sql.functions import lit

  
left = left \
            .withColumn("city_count_max", lit(2)) \
            .withColumnRenamed("continent", "continent_left")
  
right = agg.withColumnRenamed("continent", "continent_right")

join_condition = \
            (col("continent_left") == col("continent_right")) & (col("city_count") < col("city_count_max"))

joined = left.join(right, join_condition, 'left')

joined.show()
```
![[Pasted image 20220514040135.png]]

то же самое но в качестве условия - классическая sql строкаю. 
помечаем датафреймы лефт и райт.

строку помещаем в функцию expr()
```python 
# non-equ right join
from pyspark.sql.functions import expr


left = left.withColumnRenamed("continent_left", "continent").alias("left")
right = right.withColumnRenamed("continent_right", "continent").alias("right")

  
join_condition = """ left.continent = right.continent AND right.city_count < left.city_count_max """

joined = left.join(right, expr(join_condition), 'right')

  
joined.show()
```
![[Pasted image 20220514040453.png]]

хотим руками сделать очень дорогой crossJoin
```python
left.crossJoin(right).show(30, False)
```
ооочень долго даже на малых датафреймах
![[Pasted image 20220514040729.png]]


### W6L108. Оконные функции в Spark DataFrames

Оконные функции позволяют делать функции над "окнами" (кто бы мог подумать) данных

Позволяют применять некоторые агрегатные методы из пакета `pyspark.sql.functions` к окнам данных в датафрейме.
это значит что 
Мы формируем некоторые партиции в датафрейме по колонкам и внутри этих партиций применяем агрегатные функции.


Окно создается из класса [pyspark.sql.Window](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:

Формируем окно 
`"a", "b"` колонки по которым хотим сделать окно и при необходимости можем сортировать `.orderBy("a")` по одной из колонок и дальше к этим окнам применяем некоторые функции.
`window = Window.partitionBy("a", "b").orderBy("a")`

И дальше к этим окнам можем применять некоторые функции 
функции делятся на несколько классов
1. агрегатные sum count mean max
2. функции которые позволяют работать с колонками из разных строк. (взять значение из текущей строки и сравнить его со значением из предыдущей строки)
3. функции работы с time series данными. нужно формировать окнами с помощью rowsBetween или rangeBetween помимо partitionBy
4. функции которые позволяют пронумеровать порядок строк в окне rowNumber

Применяя окна, можно использовать такие полезные функции из [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), как `lag()` и `lead()`, а также эффективно работать с данными time-series, вычисляя такие параметры, как, например, среднее значение заданного поля за 3-х часовой интервал

тот же count и sum что и в пунктах выше только без группировки 
.over(window) - оконная функция
```python
# В нашем случае, используя оконные функции, мы можем построить DF из предыдущих примеров c join, 
# но без использования соединения

from pyspark.sql import Window
import pyspark.sql.functions as F

window = Window.partitionBy("continent")

agg = clean_data \
    .withColumn("city_count", F.count("*").over(window)) \
    .withColumn("population_sum", F.sum("population").over(window)) \

agg.show()
```
![[Pasted image 20220514043415.png]]


### W6L109. Функции pyspark.sql.functions

Spark обладает достаточно большим набором встроенных функций, доступных в [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), поэтому перед тем, как писать свою UDF, стоит обязательно поискать нужную функцию в данном пакете.

К тому же, все функции Spark принимают на вход и возвращают [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column), а это значит, что вы можете совмещать функции вместе

**Также важно помнить, что функции и колонки в Spark могут быть созданы без привязки к конкретным данным и DF**

В качестве примера - функция которая считает некоторое отношение (col("population_sum") / col("city_count")) превращает это в структуру и дальше в жсончик.
```python
from pyspark.sql.functions import to_json, col, struct


avg_pop = \
    to_json(
        struct(
            (col("population_sum") / col("city_count")).alias("value")
        )
    ).alias("avg_pop")

agg.select(col("*"), avg_pop).show(truncate=False)
```
![[Pasted image 20220514044831.png]]

возьмем все колонки из датафрейма и превратим их в одну сложную колонку  типа struct 
это называется структурная колонка / вложенная колонка
```python
# Большим преимуществом Spark по сравнению с большинством SQL ориентированных БД является наличие
# встроенных функций работы со списками, словарями и структурами данных

from pyspark.sql.functions import *

all_in_one = agg.select(struct(*agg.columns).alias("allinone"))

all_in_one.printSchema()
all_in_one.show(20, False)
```
![[Pasted image 20220514045055.png]]
![[Pasted image 20220514045211.png]]

Можем применять некоторые функции к массивам данных
 в ячейке фрмируем массив констант, называем его `a`
 точно так же с  `b`
 и объединяем их в новый `c`
```python
# Например, можно создавать массивы и объединять их
from pyspark.sql.functions import *

arrays = \
    spark.range(0,1) \
    .withColumn("a", array(lit(1), lit(2), lit(3))) \
    .withColumn("b", array(lit(4),lit(5),lit(6))) \
    .select(array_union(col("a"), col("b")).alias("c"))

arrays.show(1, False)
```
![[Pasted image 20220514045412.png]]


Также, в разделе [SQL, Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html) присутствует еще более широкий список функций, доступных в Spark. Некоторые из них отсутствуют в [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)!

Эти функции нельзя использовать как обычные методы над [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column), однако вы можете использовать метод `expr()` для этого.

просто достаем фунецию помещаем в кавычки и в expr()
```python
from pyspark.sql.functions import *

spark.range(10).select(expr(""" pmod(id, 2) """).alias("foo")).show()
```
![[Pasted image 20220514045658.png]]


```python
# В данном примере мы используем Java функцию с помощью функции java_method
# Запомните этот пример и используйте всегда, когда вам не хватает какой-либо функции в pyspark, 
# доступной в Java, ведь, используя такой подход, вы не снижаете производительность вашей программы за счет
# передачи данных между Python и JVM приложением Spark, и при этом вам не нужно уметь писать код на Java/Scala :)

from pyspark.sql.functions import *

spark.range(0,1).withColumn("a", expr("java_method('java.util.UUID', 'randomUUID')")).show(1, False)
```
![[Pasted image 20220514045759.png]]

**Выводы**
-   мощный инструмент для работы с данными
-   в отличие от RDD, Dataframe API устроен так, что все вычисления происходят в JVM
-   обладает единым API для работы с различными источниками данных
-   имеет большой набор встроенных функций работы с данными
-   имеет возможность использовать в pyspark функции, доступные в Java


































































































































