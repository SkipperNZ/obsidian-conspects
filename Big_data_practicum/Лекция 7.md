[[Big_data practicum made]]



# Лекция 7
**Оптимизация Spark вычислений**

### W7L102  Репартиционирование данных
RDD и DF являются представляют собой классы, описывающие распределенные коллекции данных. Они (коллекции) разбиты на крупные блоки, которые называются партициями
В спарк при построении графа вычислений который называется в Spark DAG (Direct Acyclic Graph) есть 3 важных термина

* Job
* Stage
* Task

**Task** - самая гранулярный процесс(самый маленький) и по сути это просто обработка какой то партиции данных. Обычно обрабатывается в один поток, одним ядром воркера.
Из этого следует что если партиция отличается по размеру от остальных партиций в датафрейме, то она может быть ботлнеком в нашем графе вычислений.
Несколько тасков объединяются в стейдж
**Stage** - запустили select, filter или любую другую операцию и в этом случае появится стейдж который объединит все наши таски. Как только спарк будет выполнять репартиционирование данных(запустит шафл) у нас возникнет новый stage. то есть это группы тасков которые разбиты шафлом.
**Job** -полный граф вычислений с момента создания датафрейма до вывода трансформированых данных куда либо

`job` представляет собой весь граф целиком, от момента создания DF, до применения `action` к нему. Состоит из одной или более `stage`. Когда возникает необходимость сделать `shuffle` данных, Spark создает новый `stage`. Каждый `stage` состоит из большого количества `task`. `task` это базовая операция над данными. Одновременно Spark выполняет N `task`, которые обрабатывают N партиций, где N - это суммарное число доступных потоков на всех воркерах.

Исходя из этого, важно обеспечивать:

-   достаточное количество партиций для распределения нагрузки по всем воркерам(как минимум столько же партиций, сколько ядер доступно в нашем спарк приложении, но по факту лучше больше ~3x)
-   равномерное распределение данных между партициями (каждое ядро будет обрабатывать свою партицию, и та в которой будет больше всего данных будет обрабатываться дольше всего и будет ботлнеком)

Для примера создадим перекошеный датасет.

![[Pasted image 20220520033930.png]]

функция для печати числа партиций в датасете
![[Pasted image 20220521024254.png]]

Результат
![[Pasted image 20220521024336.png]]

Любые операции с таким датасетом будут работать медленно, т.к.
-   если суммарное количество потоков на всех воркерах больше 10, то в один момент времени работать будут максимум 10, остальные будут простаивать
-   из 10 партицийи только в 2 есть данные и это означает, что только 2 потока будут обрабатывать данные, при этом из-за перекоса данных между ними (900 vs 100) первый станет bottleneck'ом

Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников.

Для устранения проблемы перекоса данных, следует использовать метод `repartition`:
У него есть 2 режима:
1. RoundRobinPartitioning - равномерно наполняет партиции данными случайным образом
![[Pasted image 20220521024936.png]]

2. HashPartitioning - если добавить колонку, по которой хотим сделать репартиционирование, то данные будут алоцированы по новым партициям на основе хеша этой колонки.
	Можно указывать и несколько колонок таким образом.
![[Pasted image 20220521025015.png]]


### W7L103. Использование соли для устранения перекосов данных
Часто при вычислении агрегатов приходится работать с перекошенными данными:
в спарке, когда мы делаем groupBy мы неявно запускаем hashPartitioning по ключу который мы передаем в метод groupBy
Если этот ключ будет перекошен, то и датафрейм будет перекошен.

Сгруппируем датасет аэропортов по колонке "type" и посмотрим что получится.
![[Pasted image 20220521032459.png]]

![[Pasted image 20220521032631.png]]

маленьких аэропортов значительно больше чем остальных, и если мы будем делать какие то агрегаты по этому ключу "type"  то получим перекошеные данные.
Например, если делаем groupBy по "type" и потом делаем collect_list по колонке ident. мы конечно посчитаем результат так как наш датасет маленький, а если бы у датасет был бы большой а у воркеров было бы мало оперативы то могло бы произойти следующее. Все "ident" были бы собраны в одну партицию неудачливого воркера и он бы упал. 
![[Pasted image 20220521101236.png]]

![[Pasted image 20220521101303.png]]

Как лечить эту ситуацию.
Поскольку groupBy всегда будет запускать неявный hashPartitioning кроме тех ситуаций когда датафрейм уже правильно партиционирован то тут поменять у нас ничего не получится. 
Вместо этого можно разбить агрегат на 2 части (вместо одного groupBy запустить несколько и это нам позволит избежать перекоса по ключу)
Это называется соление(добавление колонки, которую в простонародие наывают "соль")
Эта добавляемая колонка содержит равномерные случайные значения 
salt - создает равномерный вектор значений от нуля до 9
![[Pasted image 20220521102732.png]]

Получаем такой датафрейм
![[Pasted image 20220521102907.png]]

Что это нам даст?
Когда мы будем делать агрегат поколонке "type" и по колонке "salt" то уйдет сильный перекос данных а тут распределение уже будет сильно лучше, уже не будет одной партиции из 34 тысяч элементов, а вместо неё будет 10 более маленьких партиций.
![[Pasted image 20220521103332.png]]

![[Pasted image 20220521103420.png]]

Это позволяет выполнить нам задачу в 2 этапа.

Сначала делаем groupBy по колонкам "type" и "salt" 
потом еще один groupBy где просто объединяем списки в список списков. Уже почти правильный ответ, осталось только "распрямить" списки списков в просто списки
![[Pasted image 20220521104455.png]]

![[Pasted image 20220521104522.png]]

### W7L104. Кеширование
По умолчанию спарк будет пересчитывать все наши датафреймы, даже если мы применяем разные экшоны к одному датафрейму.

(при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения)
Например возьмем датасет с аэропортами
![[Pasted image 20220521185528.png]]

Посчитаем несколько действий. Несмотря на то, что `only_ru` является общим для всех действий, он пересчитывается при вызове каждого действия.

![[Pasted image 20220522023641.png]]

Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти

![[Pasted image 20220522022405.png]]

после `only_ru.cache()` все применяемые экшоны будут кешировать партиции датасета, которые они использовали.
в нашем случае сначала закешируется одна партиция с помощью show, потом остаьные на count. 
Так делать нелюзя, нужно кешировать весь датасет, что бы соблюдать консестентность данных.

-   Использование `cache` и `persist` позволяет существенно сократить время обработки данных, однако следует помнить и об увеличении потребляемой памяти на воркерах


### W7L105. План выполнения задач
Как спарк обрабатывает наши данные.

Любой `job` в Spark SQL имеет под собой план выполнения, кототорый генерируется на основе написанно запроса. План запроса содержит операторы, которые затем превращаются в Java код. Поскольку одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы, например:

-   убрать лишние shuffle
-   убедиться, чтот тот или иной оператор будет выполнен на уровне источника, а не внутри Spark
-   понять, как будет выполнен `join`


Планы выполнения доступны в двух видах:

-   метод `explain()` у DF
-   на вкладке SQL в Spark UI

На примере того же датасета с кодами аэропортов:
![[Pasted image 20220522053628.png]]

Используем метод `explain`, чтобы посмотреть план запроса. Наиболее интересным является физический план, т.к. он отражает фактически алгоритм обработки данных. В данном случае в плане присутствует единственный оператор `FileScan csv`:
![[Pasted image 20220522054015.png]]

![[Pasted image 20220522054043.png]]

Выполним `filter` и проверим план выполнения. Читать план нужно снизу вверх. В плане появился новый оператор `filter`

![[Pasted image 20220522055410.png]]

Выполним агрегацию и проверим план выполнения. В нем появляется три оператора: 2 `HashAggregate` и `Exchange hashpartitioning`.

Первый `HashAggregate` содержит функцию `partial_count(1)`. Это означает, что внутри каждого воркера произойдет подсчет строк по каждому ключу. Затем происходит `shuffle` по ключу агрегата, после которого выполняется еще один `HashAggregate` с функцией `count(1)`. Использование двух `HashAggregate` позволяет сократить количество передаваемых данных по сети.
![[Pasted image 20220522060508.png]]

![[Pasted image 20220522060544.png]]

Выводы:
-   Spark составляет физический план выполнения запроса на основании написанного вами кода
-   Изучив план запроса, можно понять, какие операторы будут применены в ходе обработки ваших данных
-   План выполнения запроса - один из основных инструментов оптимизации запроса


### W7L106. Оптимизация объединений и группировок
Как под капотом происходят джойны  группировки и как их оптимизировать.

При выполнении `join` двух DF важно следовать рекомендациям:

-   фильтровать данные до join'а
-   использовать equ join
-   если можно путем увеличения количества данных применить equ join вместо non-equ join'а, то делать именно так
-   всеми силами избегать cross-join'ов
-   если правый DF помещается в памяти worker'а, использовать broadcast()

Виды соединений
-   **BroadcastHashJoin**
    -   equ join
    -   broadcast
-   **SortMergeJoin**
    -   equ join
    -   sortable keys
-   **BroadcastNestedLoopJoin**
    -   non-equ join
    -   using broadcast
-   **CartesianProduct**
    -   non-equ join

[Optimizing Apache Spark SQL Joins: Spark Summit East talk by Vida Ha](https://youtu.be/fp53QhSfQcI)

Возьмем для примера 2 датасета:

![[Pasted image 20220522061559.png]]


 **BroadcastHashJoin**

-   работает, когда условие - равенство одного или нескольких ключей
-   работает, когда один из датасетов небольшой и полностью вмещается в память воркера
-   оставляет левый датасет как есть
-   копирует правый датасет на каждый воркер
-   составляет hash map из правого датасета, где ключ - кортеж из колонок в условии соединения
-   итерируется по левому датасета внутри каждой партиции и проверяет наличие ключей в HashMap
-   может быть автоматически использован, либо явно через `broadcast(df)`

![[Pasted image 20220522062026.png]]

![[Pasted image 20220522063006.png]]


**SortMergeJoin**
-   работает, когда ключи соединения в обоих датасета являются сортируемыми
-   репартиционирует оба датасета в 200 партиций по ключу (ключам) соединения
-   сортирует партиции каждого из датасетов по ключу (ключам) соединения
-   Используя сравнение левого и правого ключей, обходит каждую пару партиций и соединяет строки с одинаковыми ключами


![[Pasted image 20220522063346.png]]

![[Pasted image 20220522063430.png]]


**BroadcastNestedLoopJoin**
один из датафреймов маленький, но сложное условие.
-   работает, когда один из датасетов небольшой и полностью вмещается в память воркера
-   оставляет левый датасет как есть
-   копирует правый датасет на каждый воркер
-   проходится вложенным циклом по каждой партиции левого датасета и копией правого датасета и проверяет условие
-   может быть автоматически использован, либо явно через `broadcast(df)`

![[Pasted image 20220522064129.png]]

![[Pasted image 20220522064214.png]]


**CartesianProduct**
-   Создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один воркер и проверяет условие соединения
-   на выходе создает N*M партиций
-   работает медленнее остальных и часто приводит к ООМ воркеров

Самый плохой из всех джоинов
![[Pasted image 20220522064759.png]]

![[Pasted image 20220522064858.png]]


### W7L107. Снижение количества shuffle
Один из способов оптимизации - это снижение количества шафлов.

В ряде случаев можно уйти от лишних `shuffle` операций при выполнении соединения. Для этого оба DF должны иметь одинаковое партиционирование - одинаковое количество партиций и ключ партиционирования, совпадающий с ключом соединения.

Разница между планами выполнения будет хорошо видна в Spark UI на графе выполнения в Jobs и плане выполнения в SQL

![[Pasted image 20220522065842.png]]

**Выводы:**
-   В Spark используются 4 вида соединений: `BroadcastHashJoin`, `SortMergeJoin`, `BroadcastNestedLoopJoin`, `CartesianProduct`
-   Выбор алгоритма основывается на условии соединения и размере датасетов
-   `CartesianProduct` обладает самой низкой вычислительной эффективностью и его по возможности стоит избегать



### W7L108. Управление схемой данных

В DF API каждая колонка имеет свой тип. Он может быть:

-   скаляром - `StringType`, `IntegerType` и т. д.
-   массивом - `ArrayType(T)`
-   словарем `MapType(K, V)`
-   структурой - `StructType()`

DF целиком также имеет схему, описанную с помощью класса `StructType`

Посмотреть список колонок можно с помощью атрибута `columns`:

![[Pasted image 20220522081641.png]]

Схема DF доступна через атрибут `schema`

![[Pasted image 20220522081743.png]]

![[Pasted image 20220522081825.png]]

Если указать схему при чтении источника, то spark не будет пытаться определить ее автоматически, что, в случае работы с такими типами файлов, как `csv` и `json`, сократит время создания `df`
![[Pasted image 20220522093600.png]]

Схема может быть создана вручную:
![[Pasted image 20220522093745.png]]


Выводы:
-   Spark использует схемы для описания типов колонок, схемы всего DF, чтения источников и для работы с JSON
-   Схема представляет собой инстанс класса `StructType`
-   Колонки в Spark могут иметь любой тип. При этом вложенность словарей, массивов и структур не ограничена

### W7L109. Оптимизация запросов Catalyst

Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие методы:
-   Column projection
-   Partition pruning    
-   Predicate pushdown
-   Constant folding

Подготовим датасет для демонстрации работы Catalyst:
Прочтаем датасет, сделаем партиционирование по "iso_country" и перезапишем его в формате паркет.
После чего снова прочитаем.
![[Pasted image 20220522095911.png]]


**Column projection**
Данный механизм позволяет избегать вычитывания ненужных колонок при работе с источниками когда это возможно.
Такая возможность есть когда мы работаем с паркетом.
(Внутри паркета данные хранятся не построчно, а поколоночно)

Тут мы вычитываем только одну колонку
![[Pasted image 20220522100343.png]]
![[Pasted image 20220522102358.png]]

![[Pasted image 20220522100403.png]]
![[Pasted image 20220522102627.png]]

**Partition pruning**
Данный механизм позволяет избежать чтения ненужных партиций
Именно для этого мы записывали данные с partitionBy.
С помощью этого датасет сохраняется не в 1 файл, а в дерикторию и разбиением по партициям
![[Pasted image 20220522103345.png]]
И в случае когда мы  работаем с такими данными и делаем срез по колонке, которая является колонкой партиционирования. Спарк оптимизирует эти вычисления и на уровне источника вычитываются только нужные партиции. и скорость сильно растёт.
![[Pasted image 20220522100457.png]]


**Predicate pushdown**
Данный механизм позволяет "протолкнуть" условия фильтрации данных на уровень datasource(источника
Спарк вместо того что бы вычитывать весь датасет в себя делает срез в базе.(каждый воркер будет доставать только определённый срез данных а не всю таблицу)
(Тут фильтруем по исо регион а не кантри)
![[Pasted image 20220522100606.png]]
![[Pasted image 20220522104514.png]]
 в паркете у каждого блока данных внутри файла есть заголовок, где указано миниальное и максимальное значение по каждой колонке
 Лучше всего это работает с базами, такими как эластик, монго, касандра


**Simplify casts**
Данный механизм убирает ненужные `cast`
она убирает ненужные касты из нашего плана. 
Допустим мы создаем датафрейм из счетчика, состоящий из 10 элементов и дальше имея этот датасет с колонками типа лонг, пытаемся еще раз кастануть их к лонгу
![[Pasted image 20220522111428.png]]
хотя этот каст есть в логиеском плане, в физическом его уже нет.
![[Pasted image 20220522111825.png]]

![[Pasted image 20220522111457.png]]
Тут уже каст сработает, так как мы кастуем к другому типу
![[Pasted image 20220522112036.png]]


**Constant folding**
Данный механизм сокращает количество констант, используемых в физическом плане
Данная оптимизация сдвигает константы в 1, когда это возможно.
например: lit(3)> lit(0) это просто true
![[Pasted image 20220522112829.png]]
Тут уже считаем по полной
![[Pasted image 20220522113112.png]]

**Combine filters**
Данный механизм объединяет фильтры
объединяет несколько фильтров в 1 что бы избежать нескольких разных физических операторов фильтров
![[Pasted image 20220522113405.png]]


