[[Big_data practicum made]]




# Лекция 8
**Потоковая обработка данных (Kafka, Spark Streaming)**



Что такое обработка больших данных в realTime? Зачем нужна и чем отличается от классической батчевой обработки данных?



**Батчевый подход**
![[Pasted image 20220528002831.png]]

На вход подается поток событий, который накапливается в распределённой файловой системе(DFS). Поверх неё настроен etl(от англ. _Extract, Transform, Load_ — дословно «извлечение, преобразование, загрузка») (hive или spark) который обрабатывает данные и результат складывает в ту же распределённую файловую систему.  
 DFS обычно слишком медленная для построения приложений поверх неё. Поэтому результат обработки переливается в более быстрое хранилище, на базе которого и строится конечный api или интерфейсы для пользователя. 


**Главный минус батчевого подхода**
В реальном проде батч - данные накопленные за какой то большой интервал времени(час или день). 
* Лаг(задержка) - это время между возникновением события и моментом, когда оно повлияло на результат решения задачи. 
	* Размер батча определяет минимальное значение лага.
* Чем меньше лаг, тем больлее ценные данные мы получили.


**Real-time big data**
* Real-time big data - набор технологий обработки Big Data с минимально возможным лагом.
* Ключевые особенности
	* Без DFS (приложение читает данные напрямую из хранилища потока событий, обрабатывает их и пишет результат в целевую базу данных)
	* Работа не с батчем, а с потоком событий 

![[Pasted image 20220528004024.png]]


Какие задачи требует использование real-time?
* Лаг в минуты
	* Ранжирование ленты новостей под пользователя (соц.сети)
* Лаг в секунды
	* Современная RTB Реклама (google yandex rambler)
* Лаг в миллисекунды
	* High-frequency trading (HFT) роботы играющие на бирже.


  
### W8L104. Подходы к потоковой обработке данных
Какие подходы к real-time обработке данных существуют?


Самый базовый подход: **пособытийный (event based)** 
Есть поток событий, который упорядочен во времени. 
Пособытийный подход заключается в обработке одного события за раз. События могут обрабатываться и параллельно, и даже иметь общий state(хранилище информации) но они принимаются в работу независимо. 
С таким подходом можно обеспечить ЛАГ в десятки милисекунд.
![[Pasted image 20220528101914.png]]

В противоположность пособытийному - **микробатчевый подход** 
Поток событий разрезается на небольшого размера батчи (пример батч раз в 10 секунд или по батчу на 100.000 сообщений). Затем запускается обработка полученного микробатча. Обычно все батчи обрабатываются последовательно.
Такой подход обеспечивает задержки чуть больше, обычно превышающие секунду.
![[Pasted image 20220528102230.png]]

Так же существует **оконный подход**
Это по сути разновидность микробатчевого. Но батчи могут перекрывать друг друга, или наоборот, оставлять дырки. Батч задается скользящим окном, для него указываем 2 параметра - размер батча и размер сдвига.
![[Pasted image 20220528102529.png]]

* Пособытийный подход позволяет достичь наименьшего лага
* Микробатч позволяет сэкономить ресурсы

Пример: 
Приложение работает с базой данных, нам надо для множества пользователей получить доп. информацию из этой базы
есть 2 подхода: 
* по запросу на каждого пользователя
* 1 запрос на всех пользователей

1вый вариант - типичный антипатерн. очень много накладных расходов для каждого пользователя 
![[Pasted image 20220528103107.png]]

В плоскости real-time обработки:
Есть событие, его надо прочитать, обработать и куда то записать результат. С помощью микробатч подхода можно сэкономить ресурсы. 

Если в системе требуется увеличить пропускную способность, то придется увеличивать задержку, и если нужна минимальная задержка, то надо снижать пропускную способность.

### W8L105. Знакомство с Kafka
Что такое kafka и зачем она нужна.

Хранилище потока больших данных.
* Обладает большой пропускной способностью
* Обеспечивает небольшую задержку между записью данных в  него и доступом для чтения следующими элементами пайплайна 
* умеет в себе агрегировать достаточно большой объем информации.

Для работы с потоком событий существует такая концепция как **очередь сообщений(MQ)**
Это абстракция где с одной стороны множество акторов(сервисов/потоков/процессов) могут писать данные а с другой стороны множество акторов может их читать.
В добавок обеспечивается логика first in first out
![[Pasted image 20220528105847.png]]

Это не просто алгоритм - это асинхронный протокол для коммуникации между приложениями или процессами и потоками в рамках одного приложения.
Самый популярный пример реализации этой концепции - **Rabbit MQ** 

Зачем еще что то изобретать? 
В классических реализациях очередей сообщений есть важные фичи которые в случае big data превращаются в минусы.

* Поддержка продвинутых схем доставки сообщений
	* Снижает пропускную способность 
* Хранение статуса для каждого сообщения 
	* Снижает пропускную способность
* Хранение данных в оперативной памяти 
	* Не персистентное хранилище
	* Сильно ограничивает объем хранимой информации


Для big data пришлось переизобрести очередь сообщений.
![[Pasted image 20220528111140.png]]

Что такое Kafka?

* Универсальная платформа для обработки потоков данных заточенная под высокую пропускную способность и низкую задержку. 
* Хранилище потока событий для real-time обработки
* Шина данных для мира Big Data
* Движок для пособытийной обработки данных в real-time(Kafka Streams)

Архитектура Kafka 

Это распределённое хранилище данных - основа которого составляют **брокеры** - узлы которые ответственны за хранение данных. 
Основная мета-информация необходимая для работы кластера kafka храниться в **Zookeeper'е** 
Очень часто инстансы zookeepera раскатываются на те же самые серверы, где находятся брокеры, просто они слушают другой порт.

**Zookeeper**  - распределённое  отказоустойчивое  иерархическое хранилище данных типа "Ключ-значение". Он разработан и используется под потребности конфигурации и синхронизации распределённых приложений. 
Приложения которые работают с Kafka тоже имеют специализированные названия:
Приложения производящие запись в kafka - называются **Producer**
Приложения читающие запись - **Consumer**
![[Pasted image 20220528112302.png]]


### W8L106. Погружение в Kafka

Базовой абстракцией kafka является поток сообщений `Topic`
Топик в свою очередь состоит из патриций 
Партиция - упорядоченный и пронумерованный набор сообщений.
Номер сообщений в kafka называется отступ(offset)
Партиции в рамках топика полностью равноправны. 

Как происходит запись данных в kafka?
Поступающие сообщения на уровне продюсера шардируются(распределяются по разным партициям) и всегда записываются в конец. 
![[Pasted image 20220528124130.png]]

Отступы(номера сообщений в партициях) очень важное понятие
`latest offset` - номер самого последнего доступного на чтение сообщения.
`earliest offset` - номер самого раннего доступного на чтение сообщения. 
![[Pasted image 20220528125146.png]]

Очевидно что при процессе записи данных в kafka `latest offset` будет постоянно увеличиваться вместе с поступлением новых событий в партицию. 
А почему `earliest offset` не всегда равен 0? Дело в том, что невозможно все данные хранить вечно. И в kafka для этого предоставлен механизм, который удаляет  слишком старые сообщения (а глубина хранения настраивается отдельно на уровне config самой kafka) 
Получается в любой момент времени мы имеем для каждой партиции свой интервал от `earliest offset` до `latest offset` с которым мы можем работать.

Итак мы приблизились к механизму **чтения** данных из kafka.

Предположим что до определённого отступа мы уже все данные вычитали на предыдущих итерациях, тогда нам нужно хранить или обработать кусок данных от  `offset` до `latest offset` а так же обновить значение отступа
![[Pasted image 20220528130003.png]]

**Репликация**
Репликация производится на разных, а часто и на нескольких уровнях. 
(между дисками одной машины, между серверами в одном кластере и даже между датацентрами)
Существует 2 принципиально разных подхода к реплекации - синхронный и асинхронный.

На рисунке изображена синхронная репликация, в ней все реплики равноправны. И запись считается завершенной при успешной записи всех реплик. (пример hdfs)
![[Pasted image 20220528130748.png]]

В асинхронной репликации определённые узлы важнее прочих. 
Такие узлы называются master а остальные - slave.
При асинхронной репликации запись считается успешной если запись произошла в master, а до всех остальных информация доезжает асинхронно в фоне.
пример: postgreSQL 
![[Pasted image 20220528130846.png]]

В кафка используется асинхронная репликация, причем происходит она на уровне партиций 
мастер в кафка называется лидер
В отличие от прочих систем не только запись, но и чтение производится с мастер реплики.
В кафка репликация решает только одну задачу - обеспечение отказоустойчивости 
![[Pasted image 20220528131352.png]]

Так как репликация асинхронная, то слейвы на какую то дельту отстают от мастера. Если по какой то причине мастер партиция умрет, то с помощью механизма переизбрания лидеров - slave партиция станет новым мастером. и мы получим потерю данных. 
Избежать это можно только принудительно включить синхронную репликацию. 
![[Pasted image 20220528131600.png]]

Как в кафка решается вопрос **распределения нагрузки**. 
Он решается за счет того, что в одном топике есть множество партиций. 
Все реплики всех партиций топика, равномерно размазываются по кластеру kafka. Причем кластер старается максимально хорошо распределить именно мастер реплики. 
А так как чтение происходит только с мастер реплик партиций и одного топика партиций может быть много. За счет этого мы и получаем возможность масштабировать нагрузку на топик. Как на чтение так и на запись.
![[Pasted image 20220528132101.png]]




### W8L107. Live Demo: работа с Kafka CLI

Проинициализируем несколько базовых переменных, нужных для дальнейшей удобной работы с консольным клиентом кафка. 

1. TOPIC - название нашего топика с которым мы будем работать.
2. ZOOKEEPERS - список зукиперов (хранилища метаинформации)
3. BROKERS - список брокеров(узлов кластера кафка где хранятся данные)
```python
TOPIC=test_topic 

ZOOKEEPERS=brain-node1.bigdatateam.org:2181,brain-node2.bigdatateam.org:2181,brain-node3.bigdatateam.org:2181

BROKERS=brain-node1.bigdatateam.org:9092,brain-node2.bigdatateam.org:9092,brain-node3.bigdatateam.org:9092
```

если мы не знаем как устроена комманда, можно просто вбить её название и нажать enter. и вылетит описание всех параметров

Начнем с первой команды для получения листа топиков
```python
kafka-topics.sh --bootstrap-server $BROKERS --list
```
![[Pasted image 20220528133537.png]]

Так же можно с помощью этой утилиты создать новый топик 
```python
kafka-topics.sh --create --bootstrap-server $BROKERS --replication-factor 2 --partitions 3 --topic $TOPIC
```
![[Pasted image 20220528133738.png]]

С помощью другой комманды можно посмотреть а какой топик был создан (get topic meta information)
Создан топик из 3х партиций, лидеры их лежат на разных нодах и у каждой партиции есть по 2 реплики (Isr - это реплики без лага)
```python
kafka-topics.sh --describe --bootstrap-server $BROKERS --topic $TOPIC
```
![[Pasted image 20220528133902.png]]

Теперь научимся писать данные в топик `kafka-console-producer.sh` позволяет данные из консоли записать напрямую в kafka.
Вызвали утилиту 
```python
kafka-console-producer.sh --broker-list $BROKERS --topic $TOPIC
```
потом что то набрали в консоли и при нажатии enter сообщение улетает в kafka
![[Pasted image 20220528135526.png]]

Теперь научимся это от туда читать. 
В соседней вкладке запустим (сначала проинициализируем переменные как в начале)
```python
kafka-console-consumer.sh --bootstrap-server $BROKERS --topic $TOPIC
```
В консоль ничего не напечатолось. `console-consumer.sh` работает только с latest offset и если мы в этот момент что то запишем то это появится в соседней вкладке
![[Pasted image 20220528135910.png]]

А если мы хотим посмотреть последние написаные сообщения в этот топик? Надо использовать ключевое слово `--from-beginning`
Оно позволяет вычитать все данные начиная с earliest отступа.
```python
kafka-console-consumer.sh --bootstrap-server $BROKERS --topic $TOPIC --from-beginning
```
![[Pasted image 20220528140211.png]]


Есть такая утилита, называемая `seq` (утилита баша которая позволяет напечатать какой то набор чисел по порядку)
Отправим в нашу кафку числа от 1 до 100:
(c помощью `--max-partition-memory-bytes 1` мы заставляем шардировать с точностью до 1 байта каждое новое сообщение шардировано на отдельную партицию)
```python
seq 1 100 | kafka-console-producer.sh --broker-list $BROKERS --topic $TOPIC --max-partition-memory-bytes 1
```
Сообщения уехали в кафку. 
А в соседней вкладке терминала видим, что сообщения напечатались не по порядку. Это говорит нам, что в кафка сообщения упорядочены только с точностью до партиций. 
В топике кафка не гарантирует порядок сообщений.
![[Pasted image 20220528140840.png]]


Попробуем вывести на экран только однну партицию добавив `--partition 1`
Видим что числа переданые в партицию 1 - все упорядочены. 
```python
kafka-console-consumer.sh --bootstrap-server $BROKERS --topic $TOPIC --partition 1
```
![[Pasted image 20220528141122.png]]


Для работы в терминале нам может не хватить готовых комманд.
Для этого есть утилита `kafka-run-class.sh` эта утилита позволяет запустить произвольный  java код. в runtime кафка.
разберем её на утилите `kafka.tools.GetOffsetShell` которая позволяет получить отступы для топика.
(в неё так же передаём список брокеров, название топика и параметр ` --time -1`)
Глянем в документации что обозначает time. 
-1 это latest отступы
-2 earliest
![[Pasted image 20220528141645.png]]

```python
kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list $BROKERS --topic $TOPIC --time -1
```
(Тут на видео съезала картинка, поэтому ничего особо не видно)

### W8L201. Знакомство со Spark Structured Streaming
Рассмотрим центральный элемент, который и отвечает за обработку данных.
![[Pasted image 20220528142213.png]]

Spark Structured Streaming - высокоуровневый фреймворк для обработки потоков данных в real-time, позволяющий максимально сосредоточится на решаемой задаче, а не на деталях real-time обработки данных.
* DataFrame -> DataFrame (надо описать преобразование одного датафрейма в другой, синтаксис максимально близок к обычному спарк)
* Основывается на микробатчевом подходе
* С версии Spark 2.3.0 добавлен пособытийный подход.

Рассмотрим 2 программы: 
1. Одну на dataframe API она обработает просто батч данных
2. Другую на  Structured Streaming  она будет выполнять те же самые вычисления, но уже для потока. 


Первая 
Считываем какой то батч данных из кафка, получаем датафрейм `input_df`, разбиваем каждую строчку на отдельные слова и операцией `explode` разворачиваем всё в 1 столбец. На выходе получаем датафрейм `result` в котором находится единственная колонка со словами. Результат в формате паркет сохраняем в hdfs.
![[Pasted image 20220528143545.png]]

Вторая
Аналогичная программа на Structured Streaming
Она почти не отличается. Мы изменили ключевые слова на чтение и запись. добавился метод `.start()` который запускает realtime обработку данных.
![[Pasted image 20220528143821.png]]

**Бесконечная таблица**
Ключевой концепцией Structured Streaming является бесконечная таблица(streaming dataframe)
Механика следующая:
Все приходящие события добавляются в конец бесконечной таблицы в виде новой строки. 
По сути мы получаем представление входного потока данных в виде таблицы в которую постоянно дописываются новые строчки. 
![[Pasted image 20220528144215.png]]

Как бесконечная таблица используется в Structured Streaming

Тут структура любой программы на этом фреймворке
![[Pasted image 20220528144308.png]]

Входной поток данных наполняет собой бесконечный датафрейм input table.
Логика на обычном Spark происходит преобразование бесконечной входной таблицы в бесконечную таблицу с результатом.
На этом этапе и описывается основная логика работы с результатом. 
Не совсем понятно как работать с бесконечной таблицей на выходе. Поэтому существует механизм sink'ов и специальных режимов вывода. Обработки части данных из бесконечной таблицы результатов. Обычно с целью записи в какое то целевое хранилище.


### W8L202. Как выглядит Spark Structured Streaming pipeline

Начнем с источника ввода данных для построения бесконечной таблицы
По умолчанию есть следующие источники ввода: 
* **Kafka source** - чтение данных из Kafka
* **Rate source** - порождается заданное число строк в секунду, каждая строка содержит timestamp и value (порядковый номер строки) (порождение тестовых данных на лету)
* **File source** - чтение файлов записываемых в папку как поток данных
* **Socket source** - чтение потока сообщений из сокета

Rate source - простейший источник удобный для отладки приложений 
Kafka source - ключевой источник используемый в проде.

Рассмотрим примеры их использования 
![[Pasted image 20220528145535.png]]


Как же бесконечная таблица представлена в коде?
В коде бесконечный датафрейм выглядит как обычный. Для их различения добавлено свойство isStreaming
![[Pasted image 20220528145715.png]]


Важная фича - работа с бесконечным датафреймом с помощью sql
методом `createOrReplaceTempView` мы можем зарегистрировать его в виде таблицы и в виде sql описать требуемую логику.

Самое главное что после такой обработки на выходе будет так же бесконечный датафрейм.
![[Pasted image 20220528145911.png]]


**Триггеры**
Функционал, который задает момент времени запуска фактической обработки очередной порции данных 
Схема работы триггера, который срабатывает раз в секунду.
![[Pasted image 20220528150220.png]]

Виды триггеров
![[Pasted image 20220528150338.png]]

Примеры задания необходимых триггеров в коде приложения: 
Для этого используется метод `trigger` в который можно передать необходимые настройки.
В верхнем примере триггер срабатывает каждые 2 секунды, а в нижнем лишь раз. Если убрать этот метод, то будет использоваться дефолтный триггер.
![[Pasted image 20220528150527.png]]


**Как работать с бесконечной таблицей содержащий результат обработки данных** 

Для этого используются специальные режимы вывода, они определяют какие данные из этой бесконечной таблицы будут доступны для обработки  по итогу работы каждого батча.

Всего существуют 3 режима вывода.
Синий - данные которые не изменились за время обработки последнего батча. 
Красный - данные которые были изменены  
Зеленый  - новые данные которые пришли 
![[Pasted image 20220528151001.png]]


По итогу отработки триггера на основе режима вывода, мы получаем конечный батч данных. 
Что мы можем с ним сделать?
В этом нам помогут различные  способы вывода данных которые называют `output sink`

![[Pasted image 20220528151523.png]]

В коде выбор синка выглядит так 
`truncate` - если тру, обрежет слишком длинные выводы колонок

![[Pasted image 20220528151645.png]]

### W8L203. Пример Spark Structured Streaming приложения
В качестве примера классический word count

* Создание Spark Session
* Чтение данных
* Преобразование dataFrame
* Вывод результата
* Запуск вычислений
* Одидание завершения (только для spark-submit)
* В REPL используем query.stop()
![[Pasted image 20220528152757.png]]

Как эта программа будет производить обработку данных: 

С верху приложение nc которое отправляет данные в сокет из которого программа и  вычитывает данные. 
Каждую секунду срабатывает триггер, который запускает вычисления с учетом пришедшей к этому моменту информацией. 
![[Pasted image 20220528153234.png]]

Такая программа будет бесконечно расти по памяти, что бы это исправить, существует набор специальных практик. 


### W8L204. Как правильно использовать Spark Structured Streaming в проде

Функция **Window**
![[Pasted image 20220528154715.png]]

Как на примере работает window
![[Pasted image 20220528154845.png]]

Если прилетело событие которое произошло давно, то спарк отнесет его честно к окну в котором оно произошло.
![[Pasted image 20220528155109.png]]

Но так как хранить все данные за всё время невозможно была придумана еще одна концепция **watermarking**
![[Pasted image 20220528155215.png]]

Вертикальная ось - когда событие произошло
Горизонтальное - когда событие поступило на обработку. 
желтые точки - события которые пришли вовремя.
оранжевые - не сильно опоздавшие события
белая с красной окантовкой - событие с опозданием превышающим watermark
![[Pasted image 20220528155310.png]]


Зависимость типа запроса и допустимого режима вывода:

Без агрегаций - операции аналогичные groupBy (не требующие шафла данных) 
![[Pasted image 20220528155649.png]]

Итого разобрали вот такую схему.
![[Pasted image 20220528160017.png]]





