[[CV]]




# Лекция 6

**Задача сегментации**
**UNet, FPN. Проблемы памяти**

На этой лекции
-   Задача сегментации и ее виды
-   Semantic Segmentation
-   Instance Segmentation: Mask R-CNN
-   BatchNorm и проблемы памяти
-   Итоги


Cобственно задача детекции из прошлых лекций это частный случай задачи локализации.

В сегментации положение объекта задается попиксельной маской

* Сегментация позволяет более точно определять форму интересующих объектов


Есть несколько видов сегментации с разной полнотой извлекаемой информации:
![[Pasted image 20220503002327.png]]
* Semantic Segmentation
Семантическая (semantic) - каждому пикселю (кроме фона) ставится в соответствие номер класса
![[Pasted image 20220503002454.png]]

* Instance Segmentation
Объектная (instance) - разделяются маски разных экземпляров одного класса
![[Pasted image 20220503004145.png]]
какие то пиксели можне не размечать вообще

* Panoptic Segmentation
для классов типа “небо”, “дорожное полотно” и т.д. (stuff) только класс
для остальных (things) - отдельная маска
![[Pasted image 20220503004326.png]]



## Семантическая сегментация
Семантическая сегментация ~= попиксельная классификация

* На входе в модель - изображение (RGB, RGBD, …), облако точек, …
* На выходе - набор масок разных классов


Рассмотрим бинарный случай - классы “фон” и “объект”
![[Pasted image 20220503004858.png]]

**Что использовать в качестве лосса?**

* Попиксельная классификация -> Cross-Entropy?
$$\large
BCE(y, \hat{y}) = -y\log(\hat{y}) - (1-y)\log(1-\hat{y})
$$
Считаем в каждом пикселе, усредняем по площади

Аналогично задаче детектирования, может быть сильный дисбаланс классов

Возможные решения:
* Веса для балансировки вклада разных пикселей
* **Focal Loss**
* Другие функции (см. дальше)


Какой может быть архитектура модели?

Интерфейс модели:
* На входе - изображение, HxWxC
* На выходе - K масок, HxWxK

![[Pasted image 20220503010650.png]]


### Fully-Convolutional Network (FCN) (2014)

[Fully convolutional networks for Semantic Segmentation (2014)](https://paperswithcode.com/paper/fully-convolutional-networks-for-semantic)
* Backbone (VGG)
* Upsampling для приведения к исходному размеру

Берется какая то сеть, типа VGG, обрезается голова
На выходе у неё получается какой то эмбединг, после которого идёт слой апсемплинга  который увеличивает размер нашей карты активаций до исходного размера и получаем тензор нужного нам размера. 
(на картинке модель должна была решать задачку классификации на 21 класс, по этому у нас 21 канал)

Какие тут проблемы:
* Узкое бутылочное горлышко (как раз переход от маленького эмбединга до апсемплинга)
* Резкое увеличение H/W
![[Pasted image 20220503011048.png]]


Как увеличить размер (H, W) карты активаций?
Есть два основных подхода
* Интерполяция значений (Upsampling)
* Транспонированная свертка (Transposed Conv)

### Upsampling
Есть 2-мерный сигнал,  и мы хотим получить сигнал в 2 раза больше по ширине и высоте
1) раздвигаем существующие значения равномерно
![[Pasted image 20220503012503.png]]

2) первый вариант - взять и заполнить пустые пространства нулями - это выглядит не очень. 
![[Pasted image 20220503012659.png]]
3) скопировать каждое значение нужное количество раз (не меняется разрешение, просто количество значений станет больше)
![[Pasted image 20220503012758.png]]

4) (би-) Линейная интерполяция 
Когда мы значение яркости в промежуточных пикселях вычисляем как линейную интерполяцию соседних 4х пикселей с известными значениями.
![[Pasted image 20220503012923.png]]

Примеров очень много разных 
![[Pasted image 20220503013506.png]]


### Transposed Conv
Транспонированная свёртка 

![[Pasted image 20220503013726.png]]

* Вместо "сворачивания" с ядром (как в обычном ConvLayer) происходит "разворачивание" входного сигнала
* Значения входного сигнала выступают весами перед ядром транспонированной свертки

Тут ядро обучаемое, и ядро может быть вычислено более подходящим для нашей задачки образом. 


Есть свои нюансы:
При определённых комбинациях размера ядра свёртки и стрейда, может получатся шахматный паттерн который довольно сильно ухудшает качество модели:
[Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)

![[Pasted image 20220503014926.png]]



### SegNet (2015)

[SegNet: A Deep Convolutional Encoder-Decoder Architecture](https://paperswithcode.com/paper/segnet-a-deep-convolutional-encoder-decoder)

* Симметричная архитектура вида Encoder-Decoder
* Постепенный Upsampling


![[Pasted image 20220503015120.png]]

### UNet (2015)
Очень хорошо зарекомендовавший себя подход, по типу ResNet в классификации. 

[U-Net: CNNs for Biomedical Image Segmentation](https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical)

* Добавили горизонтальные связи к Encoder-Decoder

![[Pasted image 20220503015531.png]]

* Сильное улучшение сегментации на границах объектов
* Всего лишь 30 изображений 512х512 для обучения!


![[Pasted image 20220503015908.png]]

### UNet-like
Из конкретной архитектуры для сегментации UNet давно превратился в "подход" для задач image-to-image:
* Сегментация (subj)
* Колоризация (предсказание цветных каналов для grayscale-входа)
* InPainting ("закрашивание" пустот)
* ...

UNet-like сеть =
* encoder (resnet, efficientnet, …) +
* decoder

 image-to-image это когда на входе изображение и на выходе что то вроде изображения.


### Feature Pyramid Networks (FPN) (2017)
[Feature Pyramid Networks for Object Detection (2017)](https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection)

* Та самая пирамида признаков, что использовали в RetinaNet
* Идея подойдет и для улучшения сегментации

Собственно верхняя картинка это перевёрнутый U-net
Но если использовать не только последнюю карту но и другие то можно еще немного улучшить качество.
![[Pasted image 20220503021312.png]]

Из основных архитектур для сегментации вроде всё. 


Еще немного про альтернативные функции потерь для сегментации.
### Beyond BCE
* Применение кросс-энтропии может сломаться о дисбаланс классов (и не только в сегментации)
* Вспомним, что в детекторах объектов говорили про понятие Intersection-over-Union (IoU) (синоним - Jaccard Index):
![[Pasted image 20220503021828.png]]
Вообще IoU это некая функция, которая на вход получает 2 множества а на выходе возвращает некую оценку близости этих множеств. 


**Jaccard Index для сегментации**

Если продолжить цепочку рассуждений и обобщить эту функцию на случай сегментации и  работать с сегментационными масками.

* По аналогии определим Jaccard Index для пары сегментационных масок:
![[Pasted image 20220503025731.png]]
Хотелось бы эту штуку использовать для оптимизации наших моделей почему: если наша модель еще не начала хорошо работать на пикселях переднего плана, и сильно ошибается, то если мы будем брать BCE по каждому пикселю а потом усреднять, то в случае если наши объекты занимают мало места мы будем получать мало значение Loss и соответственно наша сеть будет долго учится на таких примерах и все градиенты будут заполнены сигналом от фона. 
Но в случае если наша предсказанная маска очень плохо пересекается с маской ground true в этом смысле индекс жокара будет очень низким. И такой лосс будет более эффективно обучать нейросеть.

* Напрямую оптимизировать Jaccard Index нельзя 

Можно взять аппроксимацию, которую легко посчитать числено. 
(оригинальную нельзя посчитать числено, потому что мы не очень понимаем как посчитать мощность множества и их пересечения)
но если мы в качестве пересечения возьмем произведение этих двух масок, а в знаменателе возьмем не сумму пересечений, а просто сложем 2 маски и вычтем их них результат перемножения этих 2х масок.
А еще сложность в том, что у нас одно множество ground true это бинарные метки, а предсказанная маска - небинарная (от нуля до единици) и поэтому тоже нельзя напрямую индекс посчитать.
* Но можно аппроксимировать его, например:
![[Pasted image 20220503025835.png]]

Давайте сделаем из этого Loss
Индекс жокара чем больше, тем лучше, а в лосах наоборот, чем он больше, тем дело хуже.
Получается Jaccard loss который очень часто используется в задачках сегментации.
* Получить из этого лосс можно, например, так:
$$\large
Loss_{J} = 1 -\log(J_{seg})
$$
* Часто комбинируют с BCE:
$$\large
Loss = \alpha \cdot Loss_{BCE}+(1-\alpha)\cdot Loss_{J}
$$


Есть еще **Dice Loss** почти тоже самое, но немного отличаются

Jaccard Loss vs Dice Loss - постоянная [путаница](https://stats.stackexchange.com/questions/381789/what-is-the-difference-between-dice-loss-vs-jaccard-loss-in-semantic-segmentatio)

[A survey of loss functions for semantic segmentation (2020)](https://arxiv.org/pdf/2006.14822.pdf)

**итоги Semantic Segmentation**

-   Используется, когда достаточно разбить пиксели по классам
-   Модели получают на вход изображения и отдают маски с вероятностями для каждого класса (U-Net)
-   Для обучения используют попиксельный BCE/Focal + Jaccard/Dice


## Instance Segmentation
Если объекты не “касаются” и не перекрывают друг друга, то разделить их маски не составит труда и после semantic segmentation
Но так  бывает не всегда 
![[Pasted image 20220503032625.png]]


  **Идея:**
  встроить в детектор объектов семантическую сегментацию для бокса вокруг каждого объекта


### Mask R-CNN

* [Mask R-CNN (2017)](https://paperswithcode.com/paper/mask-r-cnn)
* В основе - Faster R-CNN
* Дополнительная ветвь для предсказания бинарной маски во всех proposals
* Маски не зависят от классов (т.е. сегментация на 1 класс)
* Вместо RoIPool — RoIAlign

![[Pasted image 20220503205526.png]]


**RoIAlign**

В Mask R-CNN используется операция RoIAlign: вместо грубого округления границ и пулинга значений используется интерполяция значений по сетке
![[Pasted image 20220503205628.png]]


Примеры:
![[Pasted image 20220503205701.png]]


Есть в [PyTorch](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)

```python
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
```


### Проблемы с памятью

Модели для сегментации (и не только) могут быть прожорливы по памяти по сравнению с классификацией

* Больше карт активаций (encoder + decoder)
* Больше HW

Увеличивается потребление памяти на 1 пример -> уменьшается размер батча

**Чем это грозит?**

Малый размер батча ->
* Шумные градиенты
* Проблемы с BatchNorm



**Шумные градиенты**

Мы не можем сделать больший батч, но нам это по чему то нужно.
Есть подход аккамулирования градиентов:
[Gradient](https://stackoverflow.com/questions/62067400/understanding-accumulated-gradients-in-pytorch) [Accumulation](https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html)

Накапливание градиентов по нескольким батчам перед обновлением весов

Прогоняем батчи через модель, считаем градиенты, но не обнуляем градиенты каждую итерацию, а в течении скольких то n батчей эти градиенты копим (усредняем или суммируем (в основном усредняем)) и потом уже обнуляем.


**Проблемы с BatchNorm**

* BN вычисляет статистики по батчу
* Попался выброс в малом батче - беда

Как быть?

* Не использовать BN (-> [InstanceNorm](https://paperswithcode.com/method/instance-normalization), …)
* Использовать много GPU + [SynchronizedBN](https://paperswithcode.com/method/syncbn)
* Если делаете fine-tuning: [заморозить](https://stackoverflow.com/a/63020188/6440490) слои BN
* Оптимизировать вычисления для BN

Вспомним что такое BatchNorm
[BatchNorm](https://paperswithcode.com/paper/batch-normalization-accelerating-deep-network)

* *Для обновления параметров слоя требуется хранить тензор x после forward pass в буфере
* *При backward pass значение x извлекается из буфера*
* Есть подходы, избавляющие от этой необходимости*

Как можно сэкономить в этом случае.


### Inplace-ABN (2018)
* [In-Place Activated BN for Memory-Optimized Training of DNNs](https://paperswithcode.com/paper/in-place-activated-batchnorm-for-memory)
* Рассмотрим связку BN + Activation + Conv


**Vanilla BN**
![[Pasted image 20220504162256.png]]
слой активаци  тут обозначен как $\large \phi$ 

Для backward pass необходимо хранить только промежуточные тензоры x, z
* Для обратимых активаций вроде Sigmoid/Tanh y восстанавливается по z
* Для ReLU по z нельзя восстановить y, но легко восстановить градиент dz/dy

![[Pasted image 20220504162309.png]]


 **Vanilla BN + checkpoints**
Предлагается сделать подход с чекпоинтами.
Да, надо для того что бы сделать backprop знать Z, но если мы храним  x (y и z сохранять не стали) то вычислим их еще раз
![[Pasted image 20220504162641.png]]

* Если после forward pass буферизовать только x, то по сохраненным статистикам BN можно вычислить заново y и затем z
* Меньше памяти, больше вычислений
* Нелокальность вычислений
* [Pytorch Checkpointing Doc](https://pytorch.org/docs/stable/checkpoint.html)


**Inplace-Activated BN**
![[Pasted image 20220504163029.png]]
Берём, делаем forward, сохраняем не x а z, и теперь когда мы делаем backward, и теперь зная z можем вычислить y.(а в бачнорме все значения обратимы) по этому значению мы можем вычислить x и всё здорово. + все вычисления являются локальными. 
![[Pasted image 20220504163044.png]]

* Будем использовать только обратимые активации
* Тогда можно при forward pass хранить только z
* Вычисления теперь локальны



Заявленная авторами экономия памяти GPU: до 50%
Можно [комбинировать с SyncBN](https://github.com/mapillary/inplace_abn/blob/master/inplace_abn/abn.py)



### Итоги
* Сегментация нужна для чёткого выделения границ объектов
	*  Разница между Semantic и Instance - в разделении объектов одного класса
	* U-Net как хороший старт в Semantic Segmentation
	* Для Instance Segmentation — Mask R-CNN
* Для моделей локализации могут требоваться хитрые оптимизации по памяти
	* Чекпоинты
	* Inplace-ABN






















































































