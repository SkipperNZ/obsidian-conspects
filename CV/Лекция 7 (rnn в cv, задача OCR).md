[[CV]]




# Лекция 7
**Рекуррентные сети в компьютерном зрении**
Распознавание текста

![[Pasted image 20220525161407.png]]

OCR - optical character recognition Оптическое распознавание символов


### Детектирование
Просто детекторы из прошлых лекций наверное не очень подходят слова могут быть повернуты 
![[Pasted image 20220526094130.png]]


Можно применить сети сегментации.  Для каждого пикселя изображения будем решать задачу бинарной классификации - есть текст/нет текста.
![[Pasted image 20220526094241.png]]

Тут применяется семантическая сегментация 
![[Pasted image 20220526094434.png]]

Какие тут могут быть трудности: 
* Соседние слова склеиваются, между ними очень маленький зазор
* Как объединять слова в строки

Можно делать более многоклассовую классификацию , выделять не только слова, но так же выделять разделители и выделять целиком строки. хорошо делается на синтетических данных. 

Можно использовать сеть U-net:
![[Pasted image 20220526095526.png]]

После нахождения слов их нужно выровнять по ориентации, для каждого найденого слова надо определить его ориентацию. 
Тут можно использовать легковесную нейронку с 4мя классами которая определяет угол поворота исхолного изображения и его выравнивает.
![[Pasted image 20220526095818.png]]

Как перейти от маски слов к баундинг боксам?

В библиотеке OpenCV есть методы, которые по бинарной маске могут построить минимальный описывающий прямоугольник(даже повернутый). 


### Распознование

Теперь у нас на входе есть отдельно вырезанные слова (в некоторых случаях можно вырезать каждую букву в отдельности если буквы далеко друг от друга как в автомобильных номерах )

Обчыно мы работаем с батчами. Для этого нужно преобразовать картинки к единому размеру.
![[Pasted image 20220526100830.png]]

И тут возникает проблемма, что если слова слишком длинные, то при масштабировании они могут сжаться по ширине так, что буквы будут неразборчивы.

После нормализации(приведении) к единому размеру используем модель распознавания. 
![[Pasted image 20220526103032.png]]

Мы хотим взять входное изображение, и для каждого окошка горизонтали предсказать какой символ там изображен
Если у нас один символ попадает в несколько окошек, то мы его дублируем.
Тут для каждого скользящего окна выполняется классификация по числу символов. 

**Зачем нужно красное T(разделитель между буквами)?** 
Пример слово apple. в нем есть 2 повторяющиеся p

Так же предположим что этот разделитель встречается в начале и конце слова и он обязательно встречается между буквами 

И для распознования можно использовать полносвёрточную сеть которая будет подавать на выходе цепочку, длина которой будет зависеть от длины входа. 
Проблема полносверточных сетей в том, что контекст который воспринимает эта сеть ограничен. 
И если мы хотим что бы сеть выучивала язык и могла по префиксу исправлять ошибки в будущем то нам нужны другие подходы.

Тут нам могут помочь RNN и трансформеры.
![[Pasted image 20220526105446.png]]
![[Pasted image 20220526110231.png]]
RNN
![[Pasted image 20220526110314.png]]
![[Pasted image 20220526110337.png]]


Есть комбинация сверточной и рекуррентной сети которая состоит из 2х частей: 
![[Pasted image 20220526110428.png]]
Это полносверточная сеть внизу у которой каждый выход имеет поле внимания 

Что бы захватить контекст(левый и правый) поверх признаков полученных свёрточный сетью применяется двунаправленная рекуррентная сеть которая затем выдает вероятности букв, которые можно использовать для распознавания. 

Эта сеть может применятся на ровне с другими, что бы конвертировать входное изображение в цепочку вероятностей символов: 
![[Pasted image 20220526110626.png]]
То что слева внизу `ToTuuTrT` - выравнивание,
а ментка справа внизу - our

### CTC loss
**Вероятность выравнивания** 
Есть выравнивание. Есть на каждом шаге векроятности символов.
Если мы перемножим эти вероятности получим апостериорную вероятность выравнивания. (вероятность что такое распределение букв случилось, при условии выхода модели)
![[Pasted image 20220526182214.png]]

**Вероятность слова**
Складывается из вероятностей всех возможных выравниваний этого слова. 

![[Pasted image 20220526182551.png]]

Объяснение почему всего сущесьвует 3 выравнивания в нашем случае:
У нас длинна цепочки получилась 8.
Мы хотим оценить какое слово из "словаря" наиболее вероятно. 
Перебираем все слова. 
В примере момент когда мы перебираем как раз слово "our"
Задаемся вопросом какими способами слово our может поместится в цепочку длины 8.
Возможны всего 3 способа. 
Мы перебираем все выравнивания, и для каждого оцениваем вероятность. Суммируем все эти вероятности и получаем вероятность суммы
Для каждого слова так считаем и берем слово с максимальной вероятностью 


**Теперь мы можем ввести аналог кросс энтропийного лосса для обучения.**

Представляет из себя отрицательный логарифм вероятности правильного класса. 
![[Pasted image 20220526184645.png]]

Представляем что количество классов совпадает с количеством слов в словаре и мы минимизируем отрицательный логарифм вероятности правильного слова при условии входа. 

**Как его вычислить эффективно?**
Утверждается что число всевозможных выравниваний которое можно построить для каждого слова, растет экспоненциально с ростом длины входной цепочки.
![[Pasted image 20220526185201.png]]
И уже практически перебрать их очень трудно. 


Нужен более эффективный способ как их вычислить. 

CTC loss работает с помощью алгоритмов динамического программирования. 

Введём несколько обозначений 
Порядковый номер ячеек - время 
Выход модели - матрица. 
по горизонтали - время
по вертикали буквы + символ разделитель.
![[Pasted image 20220526234458.png]]

В то же время введём сущность **"состояние"**
Оценим вероятность слова our
Можем его конвертировать в цепочку расставив разделитель между буквами. 
называем эту цепочку - **последовательность состояний**
Состояния не допускают дублирования букв

![[Pasted image 20220526234607.png]]


Суть динамического программирования: 

1. Разбиваем задачу на подзадачи.
2. Решаем сложные задачи используя ответы простых задач.


Будем решать задачу так: 
Построим двумерную матрицу у которой по x будет время а по оси y будут состояния(в какой позиции слова мы сейчас находимся)
Постараемся вычислить такую функцию - вероятность префикса слова длины s в момент времени t
![[Pasted image 20220526235434.png]]

В правом нижнем углу - вероятность нахождения всего слова. 
В ячейке 3/4 вероятность нахождения префикса ou итд.

Самая простая задача тут - это понять вероятность пустой цепочки, при условии что мы посмотрели только на первое значение это - 0.9
какова вероятность "o" в момент времени 3 - это всего одно выравнивание  `ToT` - 0.9/* 0.9/* 0.8
символ "o" в 3тий момент времени можно получить 2мя способами - цепочка `TTo` или цепочка `Too` поэтому мы суммируем 2 вероятности (`TT` и `To`) в клеточках слева и умножаем на 0.1(вероятность o в 3тий момент времени)
![[Pasted image 20220527001014.png]]
Заполняя матрицу получаем такую картинку: 
(Крестики - просто не посчитаные значения)
в конце будет какое то число - суммарная вероятность всех выравниваний. 
![[Pasted image 20220527001455.png]]

в других ситуациях(когда длинные цепочки и короткое слово) ветвление может быть намного больше.

Что у нас пока получилось: 
У нас есть входное изображение есть для него метка и мы смогли вычислить вероятность этого слова
forward pass нашего лосса мы уже сделали(оценили лосс - отрицательный логарифм вероятности слова)

Как пробросить градиент через это дело. 
Сам по себе этот алгоритм не диференциируемый но есть одна хитрость 

**Обратный проход**
Помимо вероятности префикса - есть вероятность суфикса
Это вероятность того, что при отрезании конца выхода модели будет закодирован конец слова.
Вычисляется так же только в обратном направлении. 
![[Pasted image 20220527003229.png]]
![[Pasted image 20220527003413.png]]

Итак имеем две вероятности - префикса и суфикса (альфа и бета)

Есть математический трюк, который говорит что у нас можно вычислить вероятность всех путей проходящих через какую то ячейку по фромуле гамма
$\large p_{t}(c(s))$ - нормировочная константа.
![[Pasted image 20220527003618.png]]

И используя эту функцию гамма можем переписать вероятность слова при условии выхода модели 
![[Pasted image 20220527003953.png]]

![[Pasted image 20220527004040.png]]


### Оценка OCR

Редакционное расстояние: замена, удаление, вставка

Метрики: 
* Доля неверно распознанных символов (CER) 
*  Доля неверно распознанных слов (WER)

![[Pasted image 20220527004400.png]]















