[[CV]]




# Лекция 8
Metric learning

Первая задача из которой возник подход metric learning это
**Face recognition task**

Её можно разделить на две подзадачи
* Верификация. Есть один человек и нужно понять это он или не он.
* Идентификация. Среди многих найти одного конкретного человека.

Есть много готовых датасетов.

Составлялась с помощью поискового движка, поэтому очень шумная:
![[Pasted image 20220624122157.png]]


Более маленький датасет, но он маленький 
![[Pasted image 20220624124124.png]]


Дистракторы - люди которых не нужно найти, эмуляция того что обычно бывает на проде
![[Pasted image 20220624124207.png]]


### Что такое metric learning
Что такое метрика?
Это механизм с помощью которого можно сравнить 2 числовых вектора и понять на сколько они близки друг к другу.
![[Pasted image 20220624124355.png]]

В качестве метрики можно задать функцию обладающую следующими тремя свойствами: 
![[Pasted image 20220624124612.png]]


1. Метрика Минковского

Одна из самых распространённых метрик 
В зависимости от значения p она называется либо манхетенским расстоянием либо евклидовой дистанцией
![[Pasted image 20220624124715.png]]

2. Расстояние Махаланобиса.

![[Pasted image 20220624124912.png]]

3. Косинусное сходство 
Хороша для многоразмерных векторов. 
Чаще всего используется 
![[Pasted image 20220624125037.png]]

### Перейдём к обучению
Нужна модель которая будет сравнивать лица
Мы этой сеткой будем получать эмбединг.

![[Pasted image 20220624145059.png]]

Самый простой подход - учим модель для классификации с классификационным лоссом и брать от туда эмбединг и надеятся что всё будет хорошо. 
Это плохой подход, вектора будут перемешиваться, и классы будут находится слишком близко. 
![[Pasted image 20220624145306.png]]

Хотим перейти к вот такому поведению:
![[Pasted image 20220624145349.png]]

Для этого нужны специальные подходы (а именно специальные функции потерь) которые можно разделить на 2 класса:

Прямые - непосредственно что то делаем с эмбедингами. 
косвенные - используем классификационный лосс, просто его модифицируем. 
![[Pasted image 20220624145959.png]]

**Прямые подходы:**
Similarity based

Как будем рассматривать наши данные?
x - некоторый элемент данных
к нему можем подобрать два других элемента (позитивный - элемент того же класса и негативный - элемент другого класса)
![[Pasted image 20220624161000.png]]
Если подаем на вход алгоритму позитивную пару, то хотим получать единичку, если негативную то 0 

Сиамская сеть:
![[Pasted image 20220624161906.png]]

Должно выглядеть как то так:
![[Pasted image 20220624162646.png]]


Что бы этого добиться в сиамских сетях используется вариант функции потерь основанный на softmax + cross entropy

В качестве метрики схожести (sim) используется косинусное расстояние. 
И тогда мы просто берём и считаем софтмакс по этим косинусным расстояниям
![[Pasted image 20220624163814.png]]

На практике этот подход устарел

Лучше работает **Triplet loss**
![[Pasted image 20220624163955.png]]

Один элемент выбирается случайно, он называется якорем и к нему выбираются позитивные и негативные примеры. 
Все это дело образует тройку. 
Идея обучения Triplet loss заключается в том, что бы после обучения позитивный пример лежал ближе к якорю чем расстояние якорь-негативный

![[Pasted image 20220624164844.png]]
m - margin, параметр обычно близкий или равный единице (что бы был какой то запас между классами)

d - чаще всего косинусное расстояние 

Как формировать триплеты?
![[Pasted image 20220624165212.png]]
С прозитивными классами всё ясно, а вот с негативными не очень.
Простые примеры сеть бысьро напучится 


Если мы хотим разделить похожие классы мы бы хотели получить такое:
![[Pasted image 20220624165830.png]]

А выходит такое(сеть коллапсирует)
![[Pasted image 20220624165858.png]]

Триплеты нужно выбирать аккуратно
![[Pasted image 20220624165945.png]]

В центре - якорь
рассматриваем зоны негативных примеров:
Hard-negative - Примеры в которых  сеть уверена что они находятся в том же классе что и якорь тут сеть коллапсирует.
Easy negative - те негативы с которыми сеть уже разобралась и отлично их выделяет.
а в желтой зоне - та что нам нужна - semy-hard примеры. 

![[Pasted image 20220624170024.png]]


Обычно у нас при обучении есть большие датасеты.
И каждый раз что бы найти semi-hard примеры нам нужно сканировать весь датасет. 

Решение - использовать мини-батчи большого размера.
И мы надеемся что в этом батче есть примеры, которые попадают в зону semi-hard
![[Pasted image 20220624171105.png]]



### Softmax-based
Рассмрим несколько более простые Softmax-based методы: 

Первый из методов это 
### Center loss
Идея: (почти такая же как у k-means) давайте будем выбирать в каждом классе центроид и будем приближать к нему все остальные типы данных.

![[Pasted image 20220625115753.png]]

Если испльзовать центр лосс без использования софтмакса, то это тоже приведёт к коллапсу. 
![[Pasted image 20220625121253.png]]
$\large C_{y_{i}}$ - центроид к которому относится элемент данных.

Параметр лямбда очень важен, если он маленький, то будет та же картина что и при обучении классификации

![[Pasted image 20220625122551.png]]

**Center loss градиенты**

Мы не можем как в k-means на каждой итерации пересчитывать центроиды на каждой итерации из за очень большого датасета. Это очень накладно
Решение: 
Обучаем центроиды как параметры алгоритма и обновляем их по минибатчам с помощью градиентного спуска. 
![[Pasted image 20220625125558.png]]

![[Pasted image 20220625125814.png]]

Не показывает на бенчмарках лучших результатов.

### Angular Softmax
На картинке слева представлено 2 эмбединга полученых с помощью классификации обычным софтмаксом а серая линия - разделяющая плоскость. Эта плоскость описывается уравнением под картинкой
W1 - вектор весов относящихся к первому класу
W2 - то же ко 2ому классу
b1 и b2 - смешение баесы

Возьмем и отнормируем входной вектор x и получим нечто на рисунке справа.
но если отнормируем вектора и баесы сделаем равными нулю то уравнение станет как справа. 

![[Pasted image 20220625130343.png]]

Это дает нам вот что (далее следует очень сбивчивое и невнятное объяснение)
![[Pasted image 20220625133117.png]]

![[Pasted image 20220625133327.png]]

![[Pasted image 20220625133400.png]]

Добавление m дает поменьше скор слишком уверенным объектам и дает модели разобраться в этом чуть лучше.
![[Pasted image 20220625133547.png]]

![[Pasted image 20220625134243.png]]


### AKNN (апроксимейт knn)
Поговорили про обучение, а теперь надо делать инференс.

Что если мы имеем 100 миллионов  персон в базе данных, как эффективно искать их.

1. Первый подход 
строим бинарное дерево, берем 2 точки данных и между ними проводим разделяющую плоскость и так далее пока в листьях окажется какое то разумное число примеров

![[Pasted image 20220625134543.png]]

визуализируя получается как то так 
![[Pasted image 20220625142211.png]]

Его сейчас особо не используют

### Метод тесного мира
Идея такая: рассматриваем датасет как граф, где каждый элемент данных - вершина, а длина ребра - расстояние между эмбедингами представляющими 2 элемента данных
Итак:
Приходит фотка, берем случайную точку в графе, находим следующий элемент с наименьшим ребром и переходим на него и так далее
![[Pasted image 20220625142336.png]]

Всё равно очень долго. Но есть его модификация:


### HNSW
Использование иерархического поиска. 
Вершины в нашем графе разделяются на несколько слоев.
на верхнем слое мало данных, а на нижнем все данные 
сначала ищем на верхнем слое как в оригинале, и спукаемся на нижние слои.

![[Pasted image 20220625142846.png]]

![[Pasted image 20220625143128.png]]

### FAISS: inverted file
Берем базу данных и разбиваем её с помощью k-means и разбиваем датасет на k кластеров
делаем что то типа хеша, где ключ - центроид а значения - элементы кластера.

![[Pasted image 20220625143236.png]]

Что делается дальше?
Для картинки запроса взять, найти ближайший центроид перейти в список и там уже найти ближайших соседей.
Что бы экономить память будем так же делать квантизацию.
сжимаем эмбединги  по схеме и храним сжатые

![[Pasted image 20220625150323.png]]
К картинке справа:
Каждый вектор разбиваем на элементы по 128 флатов, и уже их кластеризуем


![[Pasted image 20220625150825.png]]

![[Pasted image 20220625151018.png]]

![[Pasted image 20220625151117.png]]


Сравнение методов (1 миллион векторов)
![[Pasted image 20220625151232.png]]



## Ladmarks recognition
Распознование достопримечательностей.
По фотографии находит название достопримечательностей.

Использовался center loss 
![[Pasted image 20220625151731.png]]

![[Pasted image 20220625151802.png]]
![[Pasted image 20220625151838.png]]






















