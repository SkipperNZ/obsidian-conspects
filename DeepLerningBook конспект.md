#math #linearAlgebra
# DeepLerningBook 

```toc
```

## Линейная алгебра

* Скаляры - просто число, обозначается курсивом, как правило строчными буквами.
Пример:  $\large s \in \mathbb {R}$ - скаляр s - вещественное число или
$\large n \in \mathbb {N}$ - скаляр n - целое натуральное число.

* Вектор - массив чисел, строчные буквы, курсивный полужирный шрифт.
Обозначается как 

$$\large x = 
\begin{bmatrix}

x_1\\
x_2 \\
...\\
x_n \\

\end{bmatrix}
$$

Для обозначения подмножества вектора $S = \{1, 3, 6\}$ , $x_s$ - вектор соcтоящий из $[ x_1, x_3, x_6]$.
$x_{-1}$ - весь вектор кроме $x_1$

* Матрица - полужирный курсивный шрифт, заглавная буква.
Матрица 

* Тензоры - В общем случае массив чисел в узлах равномерной сетки с произвольным числом осей называется тензором. супер жирные вертикальные буквы.


Транспонирование матриц
![[Pasted image 20211230075753.png]]

### Умножение матриц
Операция определена если число столбцов $\textbf{A}$  равна числу столбцов $\textbf{B}$. 
Если  $\textbf{A} = m\times n$, а  $\textbf{B} = n\times p$  то произведение $\textbf{С} = m\times p$

$С_{i,j} = \sum\limits_{k} A_{i,k}B_{k,j}$

Поэлементное же перемножение называется произведением Адамара и обозначается как $\textbf{A} \otimes \textbf{B}$

**Скалярное произведение** - двух векторов ***x*** и ***y*** одинаковой размерности называется произведение матриц $x^Ty$. Таким образом, элемент $C_{i,j}$ матрицы   $\textbf{C = AB}$ является скалярным произведением *i* -й строки **А** и *j*-го столбца **B**. 

Полезные свойства матричных операций:
1. $\large\textbf{\textit{A(B+C) = AB + AC}}$ 
2. $\large\textbf{\textit{A(BC) = (AB)C}}$.

Умножение матриц не коммутативно:
$\large\textbf{\textit{AB = BA}}$. не всегда справедливо. 

Скалярное произведение векторов коммутативно:
$\large x^Ty = y^Tx$.

Так же 
$\large\textbf{\textit{(AB)}}^T = B^TA^T$.

$\large x^Ty = (x^Ty)^T = y^Tx$.

Система линейных уравнений: $\large Ax = b$.
Где  $\large A \in \mathbb {R^{m \times n}}$ - известная матрица. $\large b \in \mathbb {R^{m}}$ - известный вектор, а  $\large x \in \mathbb {R^{n}}$ - неизвестный вектор, элементы которого мы хотим найти.


### Единичная и обратная матрица
Единичная матрица - при умножении на любой вектор оставляет этот вектор без изменения. Обозначается как $\large  I_n$. 
Пример единичной матрицы:
$$\large
\begin{bmatrix}
1 \ 0 \ 0\\
0\ 1\ 0 \\
0\ 0\ 1\\
\end{bmatrix}
$$

Обратная матрица к  $\large A$ , обозначается $\large A^{-1}$ и по определению удовлетворяет отношению:

$\large A^{-1}A = I_n$.

Отсюда линейные уравнения решаются так


$$Ax = b;$$
$$A^{-1}Ax = A^{-1}b;$$
$$I_nx = A^{-1}b;$$
$$x = A^{-1}b.$$

В следующем разделе описано условие существования обратной матрицы.
 
### Линейная зависимость и линейная оболочка

Для существования $A^{-1}$ уравнение $Ax = b$ должно иметь единственное решение для любого значения  $b$. 

Что бы проанализировать, сколько решений имеет уравнение, представим себе, что столбцы $A$ определяют различные направления от начала координат (точки, соответствующей вектору, все элементы которого равны нулю), а затем подумаем, сколько есть способов достичь точки $b$. Тогда элемент xi определяет, как далеко следует пройти в направлении столбца $x_i$:

$\large Ax = \sum\limits_{i} x_{i}A_{:,i}$

Это выражение называется **линейной комбинацией**

**Линейной оболочкой** множества векторов называется множество всех векторов, представимых в виде их линейных комбинаций.

$\large L(a_1;a_2;...;a_n) =\{ \alpha_1a_1+...+\alpha_na_n \; | \; \alpha_i \in {R}\}$

Что бы определить имеет ли уравнение $Ax = b$ решение, нужно проверить входит ли $\large b$ в  линейную оболочку столбцов матрицы $\large A$. 
Эта линейная оболочка называется пространством столбцов, или областью значений матрицы $\large A$.

Что бы $Ax = b$ имела решение для любого $\large b \in \mathbb {R}^m$  необходимо, что бы пространство столбцов $\large A$ совпадало с $\large \mathbb{R}^m$. Из этого требования следует что в $\large A$ должно быть по меньшей мере $m$ столбцов. Т.е  $n \geq m$  

Это условие необходимое, но не достаточное.
Дальше идет какое то кривоватое понятие линейной зависимости, но оно кривовато поэтому вот с матпрофи:

Два вектора плоскости линейно зависимы, тогда и только тогда, когда они коллинеарны.
Два вектора плоскости линейно независимы в том и только том случае, если они не коллинеарны.

Множество векторов называется линейно независимым, если ни один вектор из этого множества не является линейной комбинацией других.

Для существования обратной матрицы необходимо еще что бы уравнение $Ax = b$ имело не более одного решения для каждого значения $b$. Это означает что в матрице может быть не больше $m$ столбцов.

Собирая все условия получаем, что матрица должна быть квадратной, т.е $m = n$, и что её столбцы должны быть линейно независимы.

Квадратная матрица с линейно зависимыми столбцами называется **вырожденной**. 

Если $A$ не квадратная, или квадратная но вырожденная, то решить уравнение всё таки можно, но метод обращения матрицы не подойдет.

Умножение справа тоже подходит: $\large AA^{-1} = I$.
Для квадратных матриц обе обратные матрицы совпадают.

### Нормы 

Иногда требуется получить длину вектора. В машинном обучении для измерения длины применяются функции, называемые нормами.

Норма $L^p$ определяется следующим образом:

$$
\large\|x\|_p = \left( \sum\limits_{i}|x_i|^p \right)^\frac{1}{p}
$$
Для $p \in \mathbb{R}, p \geq 1$.

Нормы, в том числе и норма $L^p$ это функции, отображающие векторы на неотрицательные числа. Интуитивно норма вектора $\large x$  измеряет расстояние от точки $\large x$ до начала координат. 

Более строго, норма - любая функция $f$, обладающая следующими свойствами:

* $f(x) = 0 \Rightarrow x = 0;$ 
* $f(x+y) \leq  f(x) + f(y)$ (неравенство треугольника);
* $\large\forall \alpha \in \mathbb{R} \quad f(\alpha x) =|\alpha|f(x)$.

Норма $L^2$  - евклидова норма - евклидово расстояние от начала координат до точки, определяемой вектором $x$. Так как она применяется очень часто, её обычно обозначают просто $\large\|x\|$, опуская индекс 2. 

Длину вектора принято измерять как квадрат нормы $L^2$, который можно записать в виде $\large x^Tx$.

Квадрат нормы $L^2$ удобнее самой нормы с точки зрения математических выкладок и вычислений. Например, частная производная квадрата нормы $L^2$ по каждому элементу $x$ зависит только от этого элемента, тогда как производные самой нормы $L^2$ зависит от вектора в целом.

Во многих контекстах квадрат нормы $L^2$  нежелателен, так как он слишком медленно растёт вблизи начала координат. В некоторых приложениях машинного обучения важно различать элементы, в точности равные нулю и мало отличающиеся от нуля.

В таких случаях мы прибегаем к функции, которая всюду растет с одинаковой скоростью, но сохраняет математическую простоту: 

норма $L^1$ : $\large\|x\|_1 = \sum\limits_{i}|x_i|$.

Эта норма часто используется, когда важно различать нулевые и ненулевые элементы.
Всякий раз как элемент вектора $\large x$ отдаляется от 0 на $\large\varepsilon$  норма $L^1$ увеличивается на $\large\varepsilon$. 

Иногда длина вектора измеряется количеством его ненулевых элементов. Некоторые называют это $L^0$ нормой, но это некорректно. Зачастую вместо подсчета ненулевых элементов используют норму $L^1$.

В машинном обучении также часто возникает норма $L^∞$, которую еще называют максимальной нормой. Она определяется как максимальное абсолютное значение элементов вектора.
$$
\large\|x\|_∞ = \max\limits_{i}|x_i|
$$
Иногда возникает необходимость оценить размер матрицы. В контексте глубокого для этого обычно применяют **норму Фробениуса**:
$$
\large\|A\|_F = \sqrt{\sum\limits_{i,j}A^2_{i,j}}
$$
аналогичную норме $L^2$ вектора.

Скалярное произведение двух векторов можно выразить через нормы:

$$
\large x^Ty = \|x\|_2\|y\|_2 \cos\theta
$$
Где $\large\theta$ - угол между $x$ и $y$.

### Специальные виды матриц и векторов

Некоторые частные случаи матриц и векторов особенно полезны.

В **диагональной матрице** все элементы, кроме находящихся на главной диагонали, равны нулю.

Квадратная диагональная матрица, диагональ
которой описывается вектором $v$, обозначается $diag(v)$.

Диагональные матрицы представляют особый интерес, потому что произведение с такой матрицей вычисляется очень эффективно.

Чтобы вычислить $diag(v)x$, нужно умножить каждый элемент $x_i$
на $v_i$. Иначе говоря, $diag(v)x = v ⊙ x$.

Обратная матрица для диагональной существует только если все диагональные элементы отличны от нуля, и тогда:
$$
diag(v)^{-1} = diag([1/v_1,...,1/v_n]^T)
$$
Диагональная матрица может быть не только квадратной, но и прямоугольной.
У неквадратной диагональной матрицы нет обратной.
Произведение диагональной матрицы $D$ на вектор $x$ сводится
к умножению каждого элемента $x$ на некоторое число и дописыванию нулей в конец получившегося вектора, если высота $D$ больше ее ширины, или отбрасыванию последних элементов вектора в противном случае.

Матрица называется симметричной, если совпадает с транспонированной:
$$
A = A^T.
$$

**Единичным вектором** называется вектор с нормой, равной 1:
$$
\|x\|_2=1.
$$

Говорят, что векторы $x$ и $y$ ортогональны (перпендикулярны), если $x^⏉y = 0.$
Если нормы обоих векторов не равны 0, то это значит, что угол между векторами составляет 90 градусов.

Ортогональные векторы, норма которых равна 1, называются **ортонормированными**.

**Ортогональной** называется квадратная матрица, строки и столбцы которой образуют ортонормированные системы векторов:
$$
A^TA = AA^T = I
$$
отсюда следует:
$$
A^{-1}=A^T
$$

Ортогональные матрицы интересны из-за простоты вычисления обратной матрицы. Обратите внимание на определение ортогональной матрицы: ее строки должны образовывать не просто ортогональную, а ортонормированную систему векторов. Не существует специального термина для матриц, строки и столбцы которых образуют ортогональную, но не ортонормированную систему векторов.

### Спектральное разложение матрицы

Спектральное разложение - это разложение на множество собственных векторов и собственных значений.

**Собственным вектором** квадратной матрицы $A$ называется ненулевой вектор $v$ – такой, что умножение $A$ на $v$ изменяет лишь масштаб $v$: 
$$
Av = λv.
$$

Скаляр $λ$ называется **собственным значением**, соответствующим этому собственному вектору.

Если $v$ – собственный вектор $A$, то собственным будет и вектор $sv$ для любого $\large s ∈ ℝ,\: s ≠ 0.$, Более того, вектору $sv$ соответствует то же собственное значение, что и $v$. Поэтому мы обычно ищем только единичные собственные векторы.

Пусть матрица $A$ имеет $n$ линейно независимых собственных векторов $\{v^{(1)}, …, v^{(n)}\}$ с собственными значениями $\{λ_1, …, λ_n\}$.
Образуем из них матрицу $V$, в которой каждый столбец – это собственный вектор: $V = [v^{(1)}, …, v^{(n)}]$. А из собственных значений образуем вектор $λ = [λ_1, …, λ_n]^⏉$. Тогда **спектральное разложение** матрицы $A$ описывается формулой:
$$\large
A = V diag(\lambda)V^{-1}. 
$$
Не у каждой матрицы есть спектральное разложение. Иногда спектральное разложение существует, но состоит из комплексных, а не вещественных чисел. К счастью, в этой книге нам обычно придется иметь дело только с матрицами специального вида, у которых имеется простое разложение. Точнее, у любой симметричной вещественной матрицы все собственные векторы и собственные значения вещественные.
$$
A = QΛQ^⏉,
$$
$Q$ – ортогональная матрица, образованная собственными векторами $A$
$Λ$ – диагональная матрица.
Собственное значение $Λ_{i, i}$ ассоциировано с собственным вектором в i-м столбце $Q$, обозначаемым $Q_{:, i}$

Поскольку $Q$ – ортогональная матрица, можно считать, что $A$ масштабирует пространство с коэффициентом $λ_i$ в направлении $v^{(i)}$.

Матрица, все собственные значения которой положительны, называется **положительно определенной**. Если же все собственные значения положительны или равны нулю, то матрица называется **положительно полуопределенной**. Аналогично, если все собственные значения отрицательны, то матрица называется **отрицательно определенной**, а если отрицательны или равны нулю – то **отрицательно полуопределенной**. 
Положительно полуопределенные матрицы интересны тем, что для них выполняется неравенство $x^⏉Ax ≥ 0$ при любом $x$.

Положительная определенность дополнительно гарантирует, что $x^⏉Ax = 0 ⟹ x = 0.$

### Сингулярное разложение

**Сингулярное разложение** (singular value decomposition —SVD) – это другой способ разложения матрицы: по **сингулярным векторам** и **сингулярным значениям.**

SVD несет ту же информацию, что спектральное разложение, но применимо в более общем случае.

Сингулярное разложение есть у любой вещественной матрицы,
чего не скажешь о спектральном разложении. Например, если матрица не квадратная, то спектральное разложение не определено, поэтому мы вынуждены использовать сингулярное разложение.

Сингулярное разложение аналогично, спектральному, но теперь 
$A$ записывается в виде произведения трех матриц:
$$
A = UDV^⏉.
$$
Пусть $A$ – матрица $m×n$.
Тогда $U$ должна быть матрицей $m×m$,
$D$ – матрицей $m×n$,
$V$ – матрицей $n×n$.

Эти матрицы обладают определенными свойствами.

Матрицы $U$ и $V$ ортогональные, а матрица $D$ диагональная (при этом необязательно квадратная).

Элементы на диагонали $D$ называются **сингулярными значениями** матрицы $A$. Столбцы $U$ называются **левыми сингулярными векторами**, а столбцы $V$ – **правыми
сингулярными векторами**.

Но, пожалуй, самое полезное свойство сингулярного разложения – использование его для обобщения операции обращения матриц на неквадратные матрицы.



### Псевдообратная матрица Мура–Пенроуза

Операция обращения определена только для квадратных матриц. Допустим, что требуется найти левую обратную матрицу $B$ для матрицы $A$, что позволило бы решить
линейное уравнение:
$$
Ax = y
$$
путем умножения обеих частей слева на $B$:
$$
x = By
$$
Единственное отображение $A$ на $B$ возможно не всегда; это зависит от характера задачи.

Если высота $A$ больше ширины, то у такого уравнения может не оказаться решений. Если же ширина $A$ больше высоты, то решений может быть много.

Операция **псевдообращения Мура–Пенроуза** позволяет кое-что сделать и в этих случаях. Псевдообратная к $A$ матрица определяется следующим образом:
$$
A^+ = \lim\limits_{a->0}(A^⏉A + αI)^{–1}A^⏉.
$$
Применяемые на практике алгоритмы вычисления псевдообратной матрицы основаны не на этом определении, а на формуле:
$$
A^+=VD^+U^⏉
$$
где $U$, $D$ и $V$ составляют сингулярное разложение A, а псевдообратная матрица $D^+$ диагональной матрицы $D$ получается путем обращения всех ненулевых диагональных элементов с последующим транспонированием.

Если число столбцов $A$ больше числа строк, то решение линейного уравнения, найденное псевдообращением, – лишь одно из многих возможных. Точнее, будет найдено
решение $x = A^+y$ с минимальной евклидовой нормой $||x||_2$.

Если число строк $A$ больше числа столбцов, то может не оказаться ни одного решения. В таком случае псевдообращение находит такой вектор $x$, для которого $Ax$ максимально близко к $y$ в терминах евклидовой нормы $||Ax – y||_2$.

### Оператор следа

Оператор следа вычисляет сумму всех диагональных элементов матрицы:

$$
Tr(A) = \sum\limits_{i} A_{i,i}.
$$

Этот оператор полезен по многим причинам. Некоторые операции, которые трудно выразить, не прибегая к нотации суммирования, можно записать с помощью произведения матриц и оператора следа. Вот, например, как можно переписать определение нормы Фробениуса матрицы:
$$
\|A\|_F= \sqrt{Tr(AA^T)}
$$
Если в выражение входит оператор следа, то открываются возможности преобразовывать это выражение, пользуясь различными тождествами. Например, след инвариантен относительно транспонирования:
$$
Tr(A) = Tr(A^T)
$$
След квадратной матрицы, представленной в виде произведения сомножителей, инвариантен также относительно перемещения последнего сомножителя в начало, если формы матриц допускают такую операцию:
$$
Tr(ABC) = Tr(CAB) = Tr(BCA),
$$
Эта инвариантность относительно циклической перестановки имеет место даже тогда, когда меняется форма произведения. Например, если $A ∈ ℝ^{m×n}, B ∈ ℝ^{n×m},$ то
$$
Tr(AB)= Tr(BA)
$$
несмотря на то что $AB ∈ ℝ^{m×m}$, а $BA ∈ ℝ^{n×n}$.

Полезно также помнить, что след скаляра равен ему самому:
$a = Tr(a)$.

### Определитель

Определитель квадратной матрицы, обозначаемый det($A$), 
– это функция, сопоставляющая матрице вещественное число. 

Определитель равен произведению всех собственных значений матрицы.
Абсолютную величину определителя можно рассматривать как меру сжатия или расширения пространства матрицей. 

Если определитель равен 0, то пространство полностью сворачивается хотя бы по одному измерению, т. е. теряется весь объем. 
Если определитель равен 1, то преобразование сохраняет объем.


### Пример: метод главных компонент.

Один из простых алгоритмов машинного обучения, метод главных компонент (principal components analysis – PCA), можно вывести из основ линейной алгебры.

Пусть имеется набор $m$ точек $\{x^{(1)}, …, x^{(m)}\}$ в пространстве $ℝ^n$,
и мы хотим подвергнуть их сжатию с потерей информации, т. е. сохранить точки в меньшем объеме памяти, возможно, ценой некоторой потери точности. Но хотелось бы свести эти потери
к минимуму.

Один из способов кодирования точек – представить их в пространстве меньшей размерности.
Для каждой точки $x^{(i)} ∈ ℝ^n$ мы ищем соответствующий ей кодовый вектор $c^{(i)} ∈ ℝ^l.$ Если $l$ меньше $n$, то для хранения кодированных точек потребуется меньше памяти, чем для исходных.
Мы хотим найти функцию кодирования $f(x)=c$ и функцию декодирования, реконструирующую исходную точку по кодированной $x ≈ g(f(x))$. 



