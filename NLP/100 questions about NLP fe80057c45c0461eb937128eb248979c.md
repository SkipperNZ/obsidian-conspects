# 100 questions about NLP

*Один из кайфовых отработанных навыков - это **задавать вопросы**. Не знать ответ - это не плохо, плохо даже не загуглить.* 

- п.c. предупреждения
    
    П.c. 1. Я во многом душнила, учитывайте это. Поэтому многие вопросы - это не конкретные вопросы, которые мне задавали, а необходимые вопросы, ответы на которые мне были нужны. 
    
    П.c. 2. На какие-то вакансии вас могут спросить про старые технологии, на какие-то нет
    
    П.c. 3. Я не веду собеседования (и слава богу), это лишь список вопросов, которые я получала или которые я слышала или развивала темы. 
    

В создании данного списка помогали: 
мой канал - [grokaem seby](https://t.me/grokaem_seby) 

[ds girl](https://t.me/girlinds)

[Alexander Babiy](https://www.linkedin.com/in/asbabii/)

[канал что-то на DL-ском](https://t.me/nadlskom)

[канал Dealer.AI](https://t.me/dealerAI)

[канал алиса олеговна](https://t.me/alisaolega)

[канал Плюшевый Питон](https://t.me/plush_python)

[канал То шо нейросети](https://t.me/toshoseti)

Перед собеседованием - копируй этот документ к себе. **Ответив на вопрос - зачеркивай его.** В будущем здесь будут мои ответы.

[100 questions NLP (english)](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/100%20questions%20NLP%20(english)%20d4eefd43fcaa44d48a5b3c44abff49f0.md)

**Answered 47/100** 

# **CLASSIC NLP**

- **TF-IDF  & ML (8/8)**
    - [ ]  1. Напишите **TF-IDF** с нуля?
        - ***ответ***
            
            ```python
            # legit - https://www.askpython.com/python/examples/tf-idf-model-from-scratch
            
            import numpy as np
            from nltk.tokenize import  word_tokenize 
            from typing import List, Dict
            
            def create_counts(texts: List):
                sentences = []
                word_set = []
                
                for sent in texts:
                    # split into words / разбиваем пословно только слова
                    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]
                    
                    sentences.append(x)
                    for word in x:
                        if word not in word_set:
                            word_set.append(word)
                
                # set a vocab of unique words / создаем словарь уникальных слов 
                word_set = set(word_set)
                total_documents = len(sentences)
             
                # Creating an index for each word in our vocab / каждому слову уникальный индекс 
                index_dict = {} 
                i = 0
                for word in word_set:
                    index_dict[word] = i
                    i += 1
                    
                return sentences, word_set, total_documents, index_dict
            
            def count_dict(sentences: List, word_set: set) -> Dict:
                """
                Counts of words.
                @sentences: the list of sentences 
                @word_set: all words without their unique ids
            
                return: frequencies of each word
                """
                word_count = {}
                for word in word_set:
                    word_count[word] = 0
                    for sent in sentences:
                        if word in sent:
                            word_count[word] += 1
                return word_count
            
            def termfreq(document: List, word: str) -> float:
                """
                Count the term frequency according to the formula 
                (num of term occurencies in the doc d) / (num of non-unique terms in doc d)
            
                @document: a list of words in the doc
                @word: a unique word
            
                return: TF value
                """
                # occurance = len([token for token in document if token == word])
                occurance = document.count(word)
                return occurance / len(document)
            
            def inverse_doc_freq(word: str, total_documents: int, word_count: Dict):
                """
                Count the IDF according to the formula
                log (num of docs / num of docs with term t)
            
                @word: a unique word
                @total_documents: num of docs in corpus
                @word_count: word frequencies {word: number of docs with word}
            
                return: IDF value
                """
                word_occurance = word_count.get(word, 0) + 1
                return np.log(total_documents / word_occurance)
            
            def tf_idf(sentence: List[str], vector_shape: int, index_dict: Dict, total_documents: int, word_count: Dict) -> np.array:
                """
                Get the sentence tf-idf vector
            
                @sentence: list of words in a sentence
                @vector_shape: number of unique words in corpus
                @index_dict: ids of words
                @total_documents: num of docs in corpus
                @word_count: word frequencies {word: number of docs with word}
            
                return: tf-idf vector as np.array
                """
                tf_idf_vec = np.zeros((vector_shape, ))
                for word in sentence:
                    tf = termfreq(sentence, word)
                    idf = inverse_doc_freq(word, total_documents, word_count)
                     
                    tf_idf_vec[index_dict[word]] = tf * idf 
                return tf_idf_vec
            
            def create_vectors(texts: List):
                vectors = []
                sentences, word_set, total_documents, index_dict = create_counts(texts)
                vector_shape = len(word_set)
                word_count = count_dict(sentences, word_set)
            
                for sent in sentences:
                    vec = tf_idf(sent, vector_shape, index_dict, total_documents, word_count)
                    vectors.append(vec)
            		vectors = np.array(vectors)
                return vectors, index_dict
            
            sentences = ['This is the first document.',
                    'This document is the second document.',
                    'And this is the third one.',
                    'Is this the first document?',]
            
            vectors, word2id = create_vectors(sentences)
            vectors.shape
            ```
            
            $$
            TfIdf(t, d, D) = Tf(t, d) * IDF(t, d)
            $$
            
            $$
            Tf = \frac{\text{Number of occurrences of term } t \text{ in document } d}{\text{Total number of terms in document } d}
            $$
            
            $$
            \text{IDF} = \log_e \left( \frac{\text{Total number of documents in the corpus}}{\text{Number of documents with term } t \text{ in them}} \right)
            
            $$
            
            $$
            TfIdf(t, d) = tf(t, d) * log(\frac{N}{df + 1})
            $$
            
    - [ ]  2. Что такое нормализация в **TF-IDF**?
        - ***ответ***
            
            TF-IDF нормализацию можно понимать в рамках самой функции. TF - это частота терма в документе. В более длинных документах TF значения могут быть больше, так как слов в документе просто больше. 
            
            *Нормализация на длину документа позволяет считать слово “кот”, которое встречается в тексте из 1000 слов 10 раз, таким же важным как слово “кот”, которое встречается 5 раз в тексте длиной 500 слов.*
            
            [ресурс](https://www.quora.com/What-is-the-benefit-of-normalization-in-the-tf-idf-algorithm)
            
            В TF-IDF vectorizer в sklearn мы также можем использовать norm после подсчета всех скоров. Если ставить l2 нормализацию, то мы можем использовать cosine similarity между двумя векторами как dot product. 
            
            [документация](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
            
            [ссылка на пост про tf-idf](https://t.me/grokaem_seby/118)
            
    - [ ]  3. Зачем вы вообще знаете про **TF-IDF** в наше время и как можете использовать в сложных моделях?
        - ***ответ***
            
            TF-IDF может использоваться для проверки первых гипотез о классификации. Это быстрый и простой способ векторизации текстов, так что проверка гипотез будет очень быстрой. 
            
            Также TF-IDF используется для выделения фичей, так как каждое значение в векторах можно рассматривать как важность слова, тем самым мы можем использовать tf-idf в:
            
            1. [Topic modelling](https://maartengr.github.io/BERTopic/getting_started/ctfidf/ctfidf.html)  для получения из большого кол-а слов репрезентаций самые важные 
            2. Использовать для классификации вместе с BERT-like моделями в разных видах. Например, добавлять к вектору ([видео](https://www.youtube.com/watch?v=NbbsVcs42jE&list=PLam9sigHPGwNx3cBUNwKSOGhda9u44Z03&index=4))
    - [ ]  4. Объясните, как работает Наивный Баес? Для чего вы можете его использовать?
        - ***ответ***
            
            Naive Bayes - это группа алгоритмов, основанный не теореме Байеса, которая гласит: 
            
            $$
            P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
            $$
            
            где: 
            
            - P(A|B) - вероятность гипотезы A при наличии доказательств B (апостериорная вероятность)
            - *P*(*B*|*A*) - вероятность доказательства B при условии гипотезы A (правдоподобие).
            - P(A) - априорная вероятность гипотезы A.
            - P(B) - априорная вероятность доказательства B.
            
            - **Пример подсчета:**
                
                *На склад поступило 2 партии изделий: первая – 4000 штук, вторая – 6000 штук. Средний процент нестандартных изделий в первой партии составляет 20%, а во второй – 10%. Наудачу взятое со склада изделие оказалось стандартным. Найти вероятность того, что оно из первой партии*
                
                $$
                P(B) = \frac{4000}{6000 + 4000} = 0.4 
                $$
                
                ***P(B) - вероятность, что изделие из первой партии*** 
                
                ---
                
                $$
                P(A) = (1 - 0.2) * 0.4 + (1 - 0.1) * 0.6 = 0.32 + 0.54 = 0.86
                $$
                
                ***P(A) - полная вероятность, что изделие стандартное*** 
                
                ---
                
                $$
                P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} = \frac{P(стандартно - первой-категории) \cdot P(стандартное)}{P(первой- категории)} = \frac{ (1 - 0.2) \cdot 0.86}{0.4} = \frac {16}{43}
                $$
                
                [link](http://mathprofi.ru/formula_polnoj_verojatnosti_formuly_bajesa.html)
                
            
            Чаще всего Naive Bayes используют для классификации текстов, при этом учитывая: 
            
            - Bag of Words assumption: порядок слов не имеет значений
            - Conditional Independence assumption: слова независимы друг от друга
            
            **Пример для двух классов:** 
            
            ![Снимок экрана 2024-03-05 в 12.49.36.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-03-05_%25D0%25B2_12.49.36.png)
            
            [Lena Voita course](https://lena-voita.github.io/nlp_course/text_classification.html)
            
            [explanation link](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html) #naivebayes #naivebayestypes 
            
            Дополнительный вопрос на подумать: что если мы не видели слово x в документе k? Тогда вероятность будет равна 0 и вероятность всего документа тоже 0? 
            
    - [ ]  5. Как может переобучиться SVM?
        - ***ответ***
            
            Для SVM мы можем использовать различные kernel функции, если использовать слишком сложную kernel функцию - можно переобучиться. 
            
            Для классического случая также возможно: неустойчивость к шуму как один из минусов может повлиять на разделяющую гиперплоскость: выбросы в обучающих данных становятся опорными объектами-нарушителями и напрямую влияют на построение разделяющей гиперплоскости. 
            
            **Регуляризация**: Параметр регуляризации контролирует баланс между достижением высокой точности на тренировочных данных и поддержанием простоты модели. Слишком низкое значение может привести к переобучению.
            
            Помощь в ответе: [То шо нейросети](https://t.me/toshoseti)
            
            [Overview of svm](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/) (vpn) 
            
            [Подробное объяснение](https://habr.com/ru/companies/ods/articles/484148/) 
            
    - [ ]  6. Объясните возможные методы предобработки текста (**лемматизацию** и **стемминг**). Какие алгоритмы для этого знаете и в каких случаях будете использовать?
        - ***ответ***
            
            Лемматизация - это процесс подготовки данных, при котором слово приводится к его начальной словарной форме (root). Например, doing, went, gone → go. 
            
            Стемминг же - это процесс также нормалиции текста, но при этом у слова удаляется окончание до образования определенной формы. Для этого используют различные методы: 
            - SnowBallStemmer, PorterStemmer и другие 
            Важным минусом является то, что слово после стемминга может быть нереальным и нигде больше не встречаться. 
            
            [HyperSkill overview](https://hyperskill.org/learn/step/31378#lemmatization-in-nltk)
            [HyperSkill overview stemming](https://hyperskill.org/learn/step/31367) 
            
            ```python
            from nltk.stem import WordNetLemmatizer
            
            lemmatizer = WordNetLemmatizer()
            lemmatizer.lemmatize('playing')  # playing
            ```
            
    - [ ]  7. Какие метрики для близости текстов вы знаете?
        - ***ответ***
            
            В кастомных задачах мы можем разделять метрики близости текстов на два типа: лексические и семантические. 
            
            Лексические примеры: Jaccard similarity. 
            
            Семантические примеры: cosine similarity, euclidean distance 
            
            ```python
            def cosine_similarity(vec1, vec2):
                """Вычисляет косинусную близость между двумя векторами."""
                dot_product = np.dot(vec1, vec2)
                norm_vec1 = np.linalg.norm(vec1)
                norm_vec2 = np.linalg.norm(vec2)
                return dot_product / (norm_vec1 * norm_vec2)
            
            def cosine_distance(vec1, vec2):
                """Вычисляет косинусное расстояние между двумя векторами."""
                return 1.0 - cosine_similarity(vec1, vec2)
            
            def euclidean_distance(vec1, vec2):
                """Вычисляет Евклидово расстояние между двумя векторами."""
                return np.sqrt(np.sum((vec1 - vec2) ** 2))
            
            def manhattan_distance(vec1, vec2):
                """Вычисляет Манхэттенское расстояние между двумя векторами."""
                return np.sum(np.abs(vec1 - vec2))
            
            def jaccard_index(set1, set2):
                """Вычисляет индекс Жаккарда между двумя множествами."""
                intersection = len(set1.intersection(set2))
                union = len(set1.union(set2))
                return intersection / union if union != 0 else 0
            ```
            
            Помощь в ответе: [То шо нейросети](https://t.me/toshoseti)
            
            [HyperSkill overview](https://hyperskill.org/learn/step/22997#step-title) 
            
    - [ ]  8. Объясните разницу между **косинусной** близостью и косинусным расстоянием. Какое из этих значений может быть негативным? Как вы будете их использовать?
        - ***ответ***
            
            [пост](https://t.me/grokaem_seby/215) 
            
            Косинсная похожесть/близость это метрика, которая обзначает на сколько два вектора (эмбеддинга) похожи друг на друга.  Basically, it is an angle between two vectors.
            
            **cosine similarity range, -1 to 1:**
            -1 абсолютно непохожие вектора *(python - security of code)*
            0 нет никакой корреляции *(university knowledge - work)*
            1 абсолютно похожие *(chatgpt - hype)*
            
            **cosine distance = 1 - cosine similarity**
            
            **Range 0 to 2,** 
            0 - identical vectors
            1 - no correlation
            2 - absolutely different
            
            cosine similarity - как похожи вектора
            cosine distance как они непохожи 
            
    
- **METRICS (7/7)**
    
    [classification_metrics.pdf](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/classification_metrics.pdf)
    
    - [ ]  9. Объясните precision и recall разницу простыми словами и на что вы будете смотреть при отсутствии f1 score?
        - ***ответ***
            
            Для простоты возьмем за основу эту матрицу ошибок: 
            
            ![Снимок экрана 2024-01-09 в 15.53.31.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-01-09_%25D0%25B2_15.53.31.png)
            
            $$
            \text{Precision} = \frac{\text{TP}}{\text{TP + FP}}
            
            $$
            
            $$
            \text{Recall} = \frac{\text{TP}}{\text{TP + FN}}
            $$
            
            В таком случае:
            precision = **Как сильно мы можем доверять предсказанию о беременности?**
            
            recall = **Из всех беременных, сколько мы действительно угадали?**
            
            Тогда для нас precision это показатель, где мы не хотим прихватить ничего лишнего (например, парочку беременных мужчин), а recall это показатель, чтобы не считать всех девушек подряд беременными. В случае идеального классификатора у нас не будет ни беременных мужчин, ни неправильных диагнозов.
            
            Если итоги классификатора для нас не стоят много денег/моральных убеждений, то мы можем терять в precision. Например, когда у нас небольшие затраты на потенциальных клиентов, и мы готовы предложить наши услуги не прямой ЦА.
            
            Баланс между этими двумя метриками можно держать через f1 score.
            
    - [ ]  10. В каком случае вы будете наблюдать изменение **specificity**?
        - ***ответ***
            
            [ссылка пост](https://t.me/grokaem_seby/178) 
            
            - **Sensitivity = tp / (tp + fn)** -  количество больных пациентов верно распознанных как больные к кол-у больных, но классифицированных как здоровые
            - **Specificity = tn / (fp + tn)** - количество здоровых пациентов, классифицированных как здоровые по отношению к здоровым, классифицированным как больные
            
            По сути **sensitivity** - это recall: то, как хорошо модель находит больных пациентов. 
            **Specificity** - это тоже recall, но по отношению к отрицательному классу, как хорошо модель находит здоровых пациентов. 
            
            Кратко:
            
            - **recall** - это то, какую долю объектов класса больных из всех больных нашёл алгоритм
            - **specificity** - то, какую долю из класса здоровых из всех здоровых нашёл алгоритм
            - **precision** - то сколько действительно больных из всех тех, что алгоритм обозначил больными
            
            Грубо говоря у нас разный делитель, в случае recall и specificity мы делим на то, что у нас действительно в данных, в precision на количество примеров, которые дала нам модель.
            
            Собственно вопрос о том, в каком случае мы будем смотреть на specificity - это случай, когда для нас важно определять верно негативный класс. Например, когда мы хотим ставить лекарство всем, кого модель определила как больных, но у лекарства очень много побочных эффектов и поэтому мы хотим определять здоровых пациентов идеально. 
            
            Картинка:
            
            ![Untitled](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Untitled.png)
            
    - [ ]  11. Когда вы будете смотреть на macro, а когда на micro метрики? Почему существует weighted метрика?
        - ***ответ***
            - **Macro-метрики** рассчитываются отдельно для каждого класса и затем усредняются. Это полезно, когда все классы одинаково важны.
            - **Micro-метрики** рассчитываются на основе суммы всех истинных положительных, ложных положительных и ложных отрицательных. Это полезно, когда важны индивидуальные предсказания. При этом с micro-average будет видно влияние дисбаланса классов
            - **Weighted-метрики** используются, когда классы несбалансированы, и вы хотите учесть количество образцов в каждом классе при расчете среднего.
            
            Примеры подсчета:
            
            ![Снимок экрана 2024-03-20 в 14.46.04.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-03-20_%25D0%25B2_14.46.04.png)
            
            As the figure shows, a micro-average is dominated by the more frequent class (in this case spam), since the counts are pooled. The macro-average better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important.
            [Resource on stanford](https://web.stanford.edu/%7Ejurafsky/slp3/ed3book.pdf#page=76)
            
    - [ ]  12. Что такое perplexity? С чем мы можем ее считать?
        - ***ответ***
            
            Perplexity - это недоумение, которое испытывает модель, когда генерит новое предложение. Например, когда вы хорошо знаете язык, у вас нет трудности в выборе подходящего слова, однако когда вы новичок, нужно перебрать несколько вариантов и проверить их в словаре. Так же и с языковой моделью.
            
            perplexity - это intrinsic метрика, так как не  обозначает результат на определенной задаче, а просто показывает 'как хорош язык'.
            
            С математической точки зрения подсчет метрики можно делать через:
            
            - вероятность на тестовой выборке, то есть как модель уверенно работает на тесте
            - exponential cross entropy - здесь заходим по той же схеме количества слов, среди которых выбирает модель или weighted branching factor.
            Чудесная статья про подсчет.
            
            ([https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3))Правило: чем меньше perplexity, тем лучше, интерпретируем как 'чем меньше модель сомневается, тем лучше'.
            
            Как считать? Математика это хорошо, а на питончике там что?))
            
            [пост](https://t.me/grokaem_seby/157)
            
    - [ ]  13. Что такое метрика **BLEU**?
        - ***ответ***
            
            BLEU (Bilingual Evaluation Understudy) – это метрика для оценки машинного перевода. Она сравнивает машинно переведенный текст с одним или несколькими эталонными переводами, учитывая точность выбора слов и грамматическую правильность.
            
            $$
            p_n = \frac{\sum_{\text{n-gram} \in \text{hypothesisCountmatch}} \text{match}(n\text{-gram})}{\sum_{\text{n-gram} \in \text{hypothesisCount}} \text{match}(n\text{-gram)}}
                 = \frac{\sum_{\text{n-gram} \in \text{hypothesisCountmatch}} \ell_{\text{n-gram}}^\text{hyp}}{\sum_{\text{n-gram} \in \text{hypothesisCount}} \ell_{\text{n-gram}}^\text{hyp}}
            
            $$
            
            Согласно формуле, BLEU считает отношение совпадающих n-gram на все кол-о n-gram в **гипотезе (сгенерированном тексте).** 
            
            Итоговая формула это взвешенное среднее на разные виды n-gram с учетом BP - brevity penalty. Данный параметр позволяет ограничивать гипотезы (сгенерированные тексты), которые длиньше чем исходный текст. Однако для задачи перевода это не всегда актуально. 
            
            Функция для **n-gram word precision** на все предложения в комбинации с **brevity penalty**. 
            
            $$
            \text{BLEU}_N = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
            $$
            
            где формула для BP:
            
            $$
            \text{BP} = \begin{cases}1 & \text{if } \ell_{\text{hyp}} > \ell_{\text{ref}} \\e^{1 - \frac{\ell_{\text{ref}}}{\ell_{\text{hyp}}}} & \text{if } \ell_{\text{hyp}} \leq \ell_{\text{ref}}\end{cases}
            $$
            
            Ограничения:
            Because BLEU is a word-based metric, it is very sensitive to word tokenization,
            making it impossible to compare different systems if they rely on different tokenization standards, and doesn’t work as well in languages with complex morphology.
            
            [Video hugging face](https://www.youtube.com/watch?v=M05L1DhFqcw)
            
            [Original paper](https://aclanthology.org/P02-1040.pdf) 
            
    - [ ]  14. Объясните разницу между разными видами **ROUGE** метрики?
        - ***ответ***
            
            [video explanation](https://www.google.com/search?q=rouge+metrics&rlz=1C5CHFA_enRU1054RU1054&oq=rouge+metrics+&gs_lcrp=EgZjaHJvbWUyCggAEEUYFhgeGDkyBwgBEAAYgAQyBwgCEAAYgAQyCAgDEAAYFhgeMggIBBAAGBYYHjIICAUQABgWGB4yCAgGEAAYFhgeMggIBxAAGBYYHjIICAgQABgWGB4yCAgJEAAYFhge0gEINTk5MWowajeoAgCwAgA&sourceid=chrome&ie=UTF-8#fpstate=ive&vld=cid:af70f45b,vid:TMshhnrEXlg,st:0)
            
            [paper](https://aclanthology.org/W04-1013.pdf) 
            
            - **ROUGE-N**
                
                ROUGE 1 - F1 score for the matching words in the candidate and reference. ROUGE - 2 - the same but for bigrams. ROUGE-N - the N-gram overlap between candidate and references.  
                
                Precision - n-grams in the prediction that are also in the reference
                
                Recall - n-grams in the reference that are also in the prediction. 
                
                **Why important to take recall?** 
                Consider the generated example “I really really loved reading reading the Hunger Games”. The recall will be perfect, however it is not the best candidate for us. 
                
                $$
                \text{ROUGE-1}_{\text{F1}}=2*\frac{\text{recall}*\text{precision}}{\text{recall} + \text{precision}}
                $$
                
                $$
                \text{ROUGE-1}_{\text{precision}}= \frac{\text{unigram cand. } \cap \text{ unigram ref.}}{|\text{unigram cand.}|}
                $$
                
                $$
                \text{ROUGE-1}_{\text{recall}} = \frac{\text{unigram cand. } \cap \text{ unigram ref.}}{|\text{unigram ref.}|}
                $$
                
                ![Снимок экрана 2023-11-15 в 11.23.03.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-11-15_%25D0%25B2_11.23.03.png)
                
                **Rouge-1 example calculation:**
                
                $$
                \text{ROUGE-1}_{\text{recall}} = \frac{\text{unigram cand. } \cap \text{ unigram ref.}}{|\text{unigram ref.}|} = \frac{\text{I, loved, reading, the, Hunger, Games}} {|\text{ len(references)}|} = \frac{6} {6}
                $$
                
                $$
                \text{ROUGE-1}_{\text{precision}} = \frac{\text{unigram cand. } \cap \text{ unigram ref.}}{|\text{unigram ref.}|} = \frac{\text{I, loved, reading, the, Hunger, Games}} {|\text{ len(candidate)}|} = \frac{6} {7}
                $$
                
                $$
                \text{ROUGE-1}_{\text{F1}}=2*\frac{\text{recall}*\text{precision}}{\text{recall} + \text{precision}} = \frac{12} {13}
                $$
                
            - **ROUGE-L**
                
                ROUGE-L measures the **longest common subsequence (LCS - the sequence in the same order but not necessarily continuous)** between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the length of the LCS. 
                
                Can be extended with ROUGE-W, that assigns more weight to consecutive LCSes. 
                
                $$
                \text{ROUGE-L}_{\text{recall}} = \frac{\text{LCS}(\text{cand., ref.})}{\text{\#words in ref.}}
                $$
                
                $$
                \text{ROUGE-L}_{\text{precision}} = \frac{\text{LCS}(\text{cand., ref.})}{\text{\# words in cand.}}
                $$
                
                $$
                \text{ROUGE-L}_{\text{F1}} = 2*\frac{\text{recall}*\text{precision}}{\text{recall} + \text{precision}}
                $$
                
                LCS in the example above is “I loved reading the Hunger Games”, length of the LCS is 6. 
                
                So, 
                
                $$
                \text{ROUGE-L}_{\text{recall}} = \frac{\text{unigram cand. } \cap \text{ unigram ref.}}{|\text{unigram ref.}|} = \frac{\text{LCS}} {|\text{ len(references)}|} = \frac{6} {6}
                $$
                
                $$
                \text{ROUGE-L}_{\text{precision}} = \frac{\text{unigram cand. } \cap \text{ unigram ref.}}{|\text{unigram ref.}|} = \frac{\text{I, loved, reading, the, Hunger, Games}} {|\text{ len(candidate)}|} = \frac{6} {7}
                $$
                
                $$
                \text{ROUGE-L}_{\text{F1}}=2*\frac{\text{recall}*\text{precision}}{\text{recall} + \text{precision}} = \frac{12} {13}
                $$
                
            - **ROUGE-Skip gram**
                
                ROUGE-S is a skip-gram concurrence metric: it considers n-grams that appear in the reference text and allows the words **to be separated by one** or more words in the model output (but they must still appear in order).
                
                N - number of unigrams to skip 
                
                For example:
                *Police killed the gunman - gives such skip-grams: (“police killed”, “police the”, “police gunman”, “killed the”, “killed gunman”, “the gunman”) N = 2* 
                
                Then we calculate the same skip-grams for generated summaries and make the same calculation. 
                
            
            **Calculation:**
            
            ```python
            import evaluate
            
            rouge = evaluate.load('rouge')
            
            predictions = ["I really really loved reading reading the Hunger Games", 
            						"Police killed the gunman"]
            references = ["I really loved reading  the Hunger Games",
            						"the gunman killed the police"]
            results = rouge.compute(predictions=predictions, references=references)
            
            # {'rouge1': 0.8819444444444445,
            #  'rouge2': 0.7142857142857143,
            #  'rougeL': 0.6597222222222223,
            #  'rougeLsum': 0.6597222222222223}
            ```
            
    - [ ]  15. В чем отличие BLUE от ROUGE?
        - ***ответ***
            
            **BLEU (Bilingual Evaluation Understudy)** - метрика, которую мы чаще всего используем в задаче перевода. BLEU метрика взвешивает разницу между переведенным и верным переводом. Чаще всего ее измеряют с помощью n-gram значений, сравнивая два текста. 
            
            **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** - метрика для задачи суммаризации. Метрика считается также, сравнивая два текста (сгенерированный summary и исходный summary), по кол-у overlaped n-gram. 
            
            То, что объединяет две метрики - это основа на пересекающихся n-gram’ах. 
            
            Главные отличия:
            
            | **BLEU** | **ROUGE** |
            | --- | --- |
            | precision-orientated | recall-orientated |
            
            [подробное описание](https://clementbm.github.io/theory/2021/12/23/rouge-bleu-scores.html) 
            
    
- **WORD2VEC(9)**
    - [ ]  16. Объясните как учится **Word2Vec**? Какая функция потерь? Что максимизируется?
        - ***ответ***
            
            Word2Vec (CBOW) обучается предсказывать слово по контексту, таким образом  
            
            [лекция_2.pdf](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/%25D0%25BB%25D0%25B5%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F_2.pdf)
            
    - [ ]  17. Какие способы получения эмбеддингов знаете? Когда какие будут лучше?
        - ***ответ***
    - [ ]  18. В чем отличие между static и contextual эмбеддингов?
        - ***ответ***
    - [ ]  19. Какие две основные архитектуры вы знаете и какая из них учится быстрее?
        - ***ответ***
            
            **CBOW - continuous bag-of-words**
            Each word is represented as a dense vector in a continous vector space. - guess the word by its context. 
            
            Skip-gram - guess the context by the word. 
            
            Skip-gram будет обучаться быстрее так как предсказывает только одно слово на основе контекста.
            
            ![Screenshot 2024-07-23 at 12.04.53.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-23_at_12.04.53.png)
            
            ![Screenshot 2024-07-23 at 12.03.10.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-23_at_12.03.10.png)
            
    - [ ]  20. В чем разница между Glove, ELMO, FastText и **Word2Vec**?
        - ***ответ***
    - [ ]  21. Что такое negative sampling и зачем он нужен? Какие еще трюки у word2vec знаете и как можете применять у себя?
        - ***ответ***
    - [ ]  22. Что такое dense и sparse эмбеддинги? Приведите примеры.
        - ***ответ***
    - [ ]  23. Почему может быть важна размерность эмбеддинга?
        - ***ответ***
    - [ ]  24. Какие проблемы могут возникнуть при обучении **Word2Vec** на коротких текстовых данных, и как можно с ними справиться?
        - ***ответ***
- **RNN & CNN(7/7)**
    - [ ]  25. Сколько обучающих параметров в простой 1 слойной **RNN**?
        - ***ответ***
            
            ![Screenshot 2024-07-22 at 10.41.24.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-22_at_10.41.24.png)
            
            Для примера возьмем такого рода однослойную RNN сеть. В ней у нас есть gates (что обычно относится к более сложным RNN сетям по типу LSTM) У каждого gate есть weight матрицы Wf, Wi, Wo, Wc (такие же для U и biases): forget, input, output, cell. Все эти матрицы time-independent, что означает, что каждая матрица используется на всех этапах для всех inputs timesteps. 
            
            Кол-о весов = 4 * [output_dim * input_dim] + 4 * [output_dim * output_dim] + 4 * [output_dim * 1] 
            
            пример подсчета:  [прекрасная статья](https://towardsdatascience.com/tutorial-on-lstm-a-computational-perspective-f3417442c2cd#b10c) 
            
    - [ ]  26. Как обучается RNN?
        - ***ответ***
            
            На каждом time-step этапе rnn делает update скрытого состояния, если на каждом этапе есть y_t (например, при текстовой генерации), то подсчет loss function на t временном этапе. Далее идет подсчет backpropagation through time - unroll the network through time. 
            
            Для каждого временного шага  t :
            
            1. Входное значение  x_t  подаётся на вход RNN.
            2. Вычисляется новое состояние  h_t  на основе предыдущего состояния  h_{t-1}  и текущего входного значения  x_t .
            3. Вычисляется выходное значение  y_t .
            
            ![Fully_connected_Recurrent_Neural_Network.webp](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Fully_connected_Recurrent_Neural_Network.webp)
            
            [detailed calculation over the gradients](https://arunmallya.github.io/writeups/nn/lstm/index.html#/8) 
            
    - [ ]  27. Какие проблемы есть в RNN?
        - ***ответ***
            1. **Vanishing gradient over time** - главная задача использования RNN - решение длинных зависимостей, однако при подсчете backpropagation по времени, на первые временные токены не будет доходить градиент при длинных последовательностях. 
            2. **Longer training - parallelization methods are not easily applicable** 
                
                > *Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht as a function of the previous hidden state h(t-1). This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.*
                > 
    - [ ]  28. Какие виды RNN сетей вы знаете? Объясните разницу между **GRU** и LSTM?
        - ***ответ***
            
            Главное отличие - это то, как LSTM и GRU понимают memory cell. Для LSTM memory cell и hidden state это две разных компоненты. In GRU, the memory cell это candidate activation vector, который мы апдейтим. 
            
            ![Screenshot 2024-07-23 at 09.45.35.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-23_at_09.45.35.png)
            
            - **vanilla RNN -** update without any gates
            ****[code, math example](https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/recurrent-neural-network/recurrent_neural_networks)
            - **LSTM
            Имеет три главных gates:** forget, input and output gate.
                - input gate - updates the cell state given the previous hidden state and current input.
                - forget gate -  decides **what information should be thrown away or kept also given hidden state and current input.** Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.
                - output gate - The output gate decides what the **next hidden state should be based on the previous hidden state, current input and modified cell state.** 
                Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions.
            - **GRU** 
            reset and update gates only
            *At each time step, the GRU computes a “candidate activation vector” that combines information from the input and the previous hidden state. This candidate vector is then used to update the hidden state for the next time step.*
                - update gate - forget + input gate → what info to throw away from the current info and what info to add
                - reset gate - how much of the previous hidden state to forget according to the current input - how much the previous hidden state is reset.
                
                В GRU мы объединили единое скрытое состояние для передачи информации, в то время как у LSTM было два состояния: отдельное ячейки и общее скрытое состояние. 
                Таким образом, GRU имеет меньше параметров и часто обучается быстрее. 
                
            
            ![Screenshot 2024-07-23 at 11.19.33.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-23_at_11.19.33.png)
            
            [comparison table link](https://www.shiksha.com/online-courses/articles/rnn-vs-gru-vs-lstm/#:~:text=GRUs%20are%20simplified%20version%20of,and%20accessing%20long%2Dterm%20dependencies.)
            
            [visualized guide](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) 
            
    - [ ]  29. Какие параметры мы можем тюнить в такого вида сетях?
        - ***ответ***
            1. Кол-о слоев 
            2. Bi-direction or not 
            3. Number of Hidden Units
    - [ ]  30. Что такое затухающие градиенты для RNN? И как вы решаете эту проблему?
        - ***ответ***
            
            Vanishing gradient проблема обозначает, что градиент становится очень маленьким, и мы не можем обновить параметрсы значительно, чтобы что-то выучить. В то время как взрыв градиента обозначает обратную ситуацию, когда градиент становится очень большой и мы “перепрыгнем” минимум функции. 
            
            ![Screenshot 2024-07-23 at 11.52.49.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-23_at_11.52.49.png)
            
            ![Screenshot 2024-07-23 at 11.54.56.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-23_at_11.54.56.png)
            
            Для решения проблемы мы можем использовать как и более сложные сети: LSTM, GRU, так и методы, которые применимы к другим видам сетей: 
            
            - gradient clipping
            - activation function - поменять ReLU на LeakyReLU и альтернативы:
            
            > It suffers from a problem known as *dying* ReLus wherein some neurons just die out, meaning they keep on throwing 0 as outputs with the advancement in training. 
            [link](https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/)
            > 
            - batch norm
    - [ ]  31. Зачем в NLP Convolutional neural network и как вы его можете использовать? С чем вы можете сравнить cnn в рамках парадигмы attention?
        - ***ответ***
            
            CNN могут обрабатывать текст окном, что позволяет захватывать local patterns. 
            
            Тем самым CNN могут полезны для извлечения локальных паттернов, например, n-gramы. 
            
            ![Screenshot 2024-07-23 at 11.44.33.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-23_at_11.44.33.png)
            
            [best explanation ever](https://arc.net/l/quote/mwdycwik)
            
            CNN для текста можно сравнить с local attention как с monotonic, так и с predictive alignment. В случае с CNN агрегация идет относительно pooling и мы можем уменьшать карту активации, а в случае с attention мы будем агрегировать относительно функции внимания, но карта не будет меняться в размерности. 
            
            [link](https://medium.com/nlplanet/two-minutes-nlp-visualizing-global-vs-local-attention-c61b42758019)
            
            ![Screenshot 2024-07-23 at 11.46.04.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-07-23_at_11.46.04.png)
            

# **NLP and TRANSFORMERS**

- **ATTENTION AND TRANSFORMER ARCHITECTURE (13/15)**
    - [ ]  32. Как считаете **attention**? (доп. для какой задачи его предложили? и почему?)
        - ***ответ***
            
            [лекция_attention.pdf](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/%25D0%25BB%25D0%25B5%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F_attention.pdf)
            
            [Attention intro](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#attention_idea)
            
            **You compare the query with the keys to get scores/weights for the values (Сравниваем query с keys, чтобы получить веса для обновления values)** 
            
            ![Screenshot 2024-04-21 at 14.44.39.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-04-21_at_14.44.39.png)
            
            - key - относительно кого считаем, исходный текст на языке оригинала
            - query - для кого считаем, переведенный текст на таргет языке
            - value - на чем мы считаем, снова исходный текст
            
            Вернулись к фразе. Понимаем суть - мы должны сделать такие вектора слов в этом заданном контексте, чтобы каждое слово брало информацию только от важного для себя. 
            
            ![Screenshot 2024-04-21 at 14.48.56.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-04-21_at_14.48.56.png)
            
            [A bit more hard-level description of attention formula](https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/)
            
            Внутри функция для attention может быть разной. Главное - нам нужны скоры, чтобы обновить наши эмбеддинги.
            
            1. **dot product** ([https://arxiv.org/pdf/1508.04025.pdf](https://arxiv.org/pdf/1508.04025.pdf)) - базовая и самая используемая функция: просто перемножаем)
                
                допом можно делать scaled dot product ([https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)) - это когда вы перед тем, как добавлять softmax, нормализуете на квадратный корень от dimension эмбеддингов. Это нужно для стабилите, когда dimension большой))
                
            2. **additive attention** ([https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)) он же concat, soft and global (Bahdanau attention) - родоначальник attention, скор считается с помощью дополнительной фидфорвад сети
            одна из имплементаций ([https://techkluster.com/ai/attention-luong-vs-bahdanau/](https://techkluster.com/ai/attention-luong-vs-bahdanau/))
                
                история от Андрея Карпатова, как ему Bahdanau рассказал, как он придумал attention ([https://youtu.be/XfpMkf4rD6E?t=1141](https://youtu.be/XfpMkf4rD6E?t=1141))
                
            3. **Luong-Attention он же general, location based и любимый dot product (они все разные)**
            Немного про различия ([https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html))
    - [ ]  33. Сложность attention? Сравните с сложностью в rnn?
        - ***ответ***
            
            Computation:
            
            **Attention** - computational complexity - O(n²*d) 
            
            **RNN**: O(n * d * d), более эффективна для длинных последовательностей
            
            где d - это dimension, n - длина последовательности 
            
            ![Screenshot 2024-04-21 at 20.45.45.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-04-21_at_20.45.45.png)
            
            [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)
            
            *In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations.* 
            Как следствие - RNN с точки зрения computation больше подходят для длинных последовательностей, но при этом теряют смысл из-за vanishing gradient problem. 
            
    - [ ]  34. Сравните RNN и a**ttention**? В каком случае будете использовать attention, а когда RNN?
        - ***ответ***
            
            **RNN:** 
            
            - **Sequential processing**: sentences must be processed word by word. → NO parallel computation
            - **Past information retained through past hidden states**: sequence to sequence models follow the Markov property: each state is assumed to be dependent only on the previously seen state. → Vanishing gradient problem
            
            **Attention:**
            
            - **Non sequential**: sentences are processed as a whole rather than word by word. → Parallel computation
            
            О том, что выбрать лучше сказать не так однозначно, так как все зависит от задачи и данных. Однако мы уже упоминали ограчение с длинными последовательностями для attention, можно добавить, что возможно использование комбинации и attention, и rnn. Это один из подходов. 
            
    - [ ]  35. Напишите attention с нуля.
        - ***ответ***
            
            ```
            def scaled_dot_product_attention(query, key, value):
                dim_k = key.size(-1)
                scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
            
                weights = F.softmax(scores, dim=-1)
                value_emb = torch.bmm(weights, value)
                return value_emb
            ```
            
    - [ ]  36. Объясните маскирование в attention.
        - ***ответ***
            
            Если говорить кратко, то в encoder transformer части мы используем только self-attention, в decoder части в первом transformer блоке мы используем также self-attention с маскированием, а вот во втором cross attention. 
            
            Мы можем разделить два понимания маскирования. Если например у нас есть паддинги, мы все равно будем их маскировать в attention и encoder. Почему? Если у нас последовательности разной длины и вы используете padding, то в attention части нам не нужно брать 'влияние' padding токенов.
            [masked softmax implementation](https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html)
            
            В causal attention мы используем другой вид маскирования, мы маскируем только будущие токены. Почему? Потому что в causal language modelling мы делаем предсказания только на основе предыдущих примеров.
            
            [интерактивная визуализация](https://bert-vs-gpt2.dbvis.de/#section-1.1) для bert и gpt эмбеддингов
            
            [статья про интерпретацию](https://aclanthology.org/W19-4808.pdf)
            
    - [ ]  37. Какая размерность у матриц self-attention?
        - ***ответ***
            
            Размерность матрицы self-attention зависит от числа входных элементов и параметров модели. Обычно она имеет форму (seq_length, seq_length), где seq_length - это длина последовательности, для которой применяется механизм self-attention. Таким образом, если у нас есть seq_length элементов во входной последовательности, мы имеем квадратную матрицу размером seq_length x seq_length.
            
    - [ ]  38. В чем разница между **BERT** и **GPT в рамках подсчета attention?**
        - ***ответ***
            - BERT использует механизм self-attention для обработки каждого слова (токена) в контексте всех остальных слов в предложении. В BERT используется masking, но он рандомен (могут использоваться также обоснованные эвристики, span и тд). Соответственно, у нас есть нарушение casual lm.
            - GPT также использует self-attention, но с masking. Это важно, так как позволяет не учитывать следующие токены и сохраняется основа casual lm.
    - [ ]  39. Какая размерность у эмбединового слоя в трансформере?
        - ***ответ***
            
            Размерность эмбеддингового слоя (embedding layer) в трансформере определяется двумя ключевыми параметрами: размером словаря (vocab size) и размерностью эмбеддингов (embedding dimension).
            
            1. **Размер Словаря (Vocab Size)**: Это количество уникальных токенов или слов, которое может обрабатывать модель. 
            2. **Размерность Эмбеддингов (Embedding Dimension)**: Это количество признаков, используемых для представления каждого токена. Эта размерность обычно является фиксированной для данной модели и соответствует размеру скрытых слоёв модели.
            
            Таким образом, матрица эмбеддингов в трансформере имеет размерность, равную **Размеру Словаря x Размерности Эмбеддингов**. Эта матрица используется для преобразования индексов токенов в их векторные представления, которые затем подаются в модель трансформера.
            
            Важные мысли - мы используем весь словарь, именно поэтому началось развитие tokenizers, так как update на все слова в мире сделать невозможно. 
            
            Какие чаще всего используются размерности? 
            
            Меняется ли размерность вектора? 
            Один из способов понимания эмбеддингов - это осознание, что это фичи. Во многих задачах мы делаем уменьшение и/или возврат к исходной размерности. Такое бывает важно как раз чтобы “выучить” главную информацию. Однако почему это не происходит в transformers? 
            
    - [ ]  40. Почему эмбеддинги называются контекстуальными? Как это работает?
        - ***ответ***
            
            В прошлом [посте](https://t.me/grokaem_seby/132) мы обсудили, что слова мы представляем как вектора и с этим неплохо  справлялся [word2vec](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010) ии glove , но на арену вышел какой-то bert и все считают его теперь 'пупом земли'. За что?
            
            0. эмбеддинги берта имеют разный контекст. Так, например, для mouse у нас будет в word2vec один эмбеддинг, а у bert и на mouse как компьютерную мышь, и на mouse как животное, и мб даже на Микки Маус тоже)) [подробнее](http://ai.stanford.edu/blog/contextual/)
            
            Почему это происходит? Когда мы прогоняем через модель наше предложение - все эмбеддинги токенов обновляются относительно друг друга. Собственно это и является контекстуальными эмбеддингами. 
            
            1. oov - out of vocabulary или неизвестные доселе слова лучше обрабатываются с помощью той же word piece tokenization.  Существуют и другие способы токенизации, о них позже.
            
            Важные штуки:
            
            - из одного слова мы можем получить несколько токенов и их мы не комбинируем. Я предполагаю, что за счет этого может лучше учиться 'синтаксис' предложения.
            - разделяем виды #эмбеддингов:
            1. segment embeddings - для text similarity task, показывает, где какое предложение,
            2. token embeddings - то, что обсудили сейчас
            3. positional embeddings - необходимые, так как мы получаем все предложение сразу. (ну супер крутое [видео](https://www.youtube.com/watch?v=dichIcUZfOw) про это)
            [вопросы](https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/)  про bert
    - [ ]  41. Что используется в трансформере **layer norm** или **batch norm** и почему?
        - ***ответ***
            
            Зачем вообще нужна какая-то нормализация? 
            
            **Internal Covariate Shift** - the change in the distribution of network activations due to the change in network parameters during training. Простыми словами это изменение распределения инпутов каждого слоя из-за изменения весов, что приводит к необходимости более тонкой настройки и более долгому coverage. [Подробнее](https://medium.com/analytics-vidhya/internal-covariate-shift-an-overview-of-how-to-speed-up-neural-network-training-3e2a3dcdd5cc).
            
            **BATCH NORM** - отдельный слой, который изменяет распределение относительно mean и variance минибатча и делает scale и shift относительно обучаемых параметров betta и gamma.
            **Зачем нужны betta и gamma?**
            
            - как говорят авторы (3 страница) это позволяет 'сохранять знания слоя' и тем самым каждый слой 'продолжает решать свою задачу' и распределения близки друг к другу, но не идентичны.
            статья
            [отличный разбор](https://arxiv.org/pdf/1502.03167.pdf)
            [почему работает](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)
            [минусы и альтернативы](https://towardsdatascience.com/batch-norm-explained-visually-why-does-it-work-90b98bcc58a0)
            
            **LAYER NORM** - такая же нормализация только с подсчетом статистик относительно слоя, а не батча. [Видео](https://www.youtube.com/watch?v=2V3Uduw1zwQ) пример.
            
            Собственно разница кроме подсчета еще в чем?
            
            1. в layer normalization нет зависимости от размера батча, а при batch norm размер должен быть не супер маленьким, чтобы получать более менее репрезентативные статистики
            2. Для rnn можно использовать layer normalization между степами, если использовать batch norm, то только измененный. [обсуждение](https://stackoverflow.com/questions/45493384/is-it-normal-to-use-batch-normalization-in-rnn-lstm) тут.
                
                ![Screenshot 2024-04-21 at 21.19.40.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-04-21_at_21.19.40.png)
                
            
            improve layer norm
            
            Однако, как и во всех темах сейчас находятся доказательства возможного изменения и улучшения. Например, [статья](https://proceedings.mlr.press/v119/shen20e/shen20e.pdf) и ее модификации для Power Normalization. 
            
            Также есть [мнение](https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm):
            
            *A less known issue of Batch Norm is that how hard it is **to parallellize batch-normalized models**. Since there is dependence between elements, there is additional need for synchronization across devices. While this is not an issue for most vision models, which tends to be used on a small set of devices, Transformers really suffer from this problem, as they rely on large-scale setups to counter their quadratic complexity. In this regard, layer norm provides some degree of normalization while incurring no batch-wise dependence.*
            
    - [ ]  42. Зачем в трансформерах **PreNorm** и **PostNorm**?
    - [ ]  43. Объясните разницу между **soft** и **hard** (local/global) attention?
        - ***ответ***
            
            Классический attention 2017 имеет главную пользу - смотреть на всех, но в условиях длинных последовательностей это может быть не выгодно, потому что сложность подсчета attention зависит от длины последовательности квадратично, так что есть:
            
            - soft VS hard ([обзор](http://proceedings.mlr.press/v37/xuc15.pdf))- предложен в image captioning таске, нам нужно в hard attention выбирать один патч, а оптимизация через variance reduction or reinforcement learning
            - global VS local - в глобал мы берем все стейты  от encoder, в local предсказываем позициии токенов, на которые будем смотреть.
            В NLP мы используем именно global VS local
            
            Local attention: для p позиции мы выбираем окно h и считаем веса и собственно context vector только относительно hidden states токенов из окна.  
            [Обзор разницы](https://www.analyticsvidhya.com/blog/2023/06/learn-attention-models-from-scratch/) 
            
            ![Снимок экрана 2024-03-15 в 11.16.12.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-03-15_%25D0%25B2_11.16.12.png)
            
    - [ ]  44. Объясните multihead attention.
        - ***ответ***
            
            В attention мы можем засплитить весь наш эмбеддинг и прогнать каждую часть через разные матрицы - basically it is multihead attention, где голова это как раз тот самый сплит. Из плюсов, которые мы получаем - это параллелизм и разнообразные представления. 
            
            ![[https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-04-20_at_20.09.52.png)
            
            [https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)
            
            Результаты этих независимых механизмов внимания затем конкатенируются и линейно трансформируются в требуемое измерение. 
            
            Важно, о чем многие забывают при создании - это то, чтобы dim эмббединга делился на количество голов. 
            
    - [ ]  45. Какие другие виды механизмов внимания вы знаете? На что направлены эти модификации?
        - ***ответ***
            
            
    - [ ]  46. На сколько усложнится self-attention при увеличении числа голов?
        - ***ответ***
            
            Согласно стандартной имплементации, кол-ов голов не увеличивает кол-о обучаемых параметров, так как исходное d делится на кол-о голов. 
            
            A model of dimensionality *d* with a single attention head would project embeddings to a single triplet of *d*-dimensional query, key and value tensors (each projection counting *d2* parameters, excluding biases, for a total of *3d*2).
            
            A model of the same dimensionality with *k* attention heads would project embeddings to *k* triplets of *d/k*-dimensional query, key and value tensors (each projection counting *d×d/k=d2/k* parameters, excluding biases, for a total of *3kd2/k=3d2*).
            
    
- **TRANSFORMER MODEL TYPES (1/7)**
    - [ ]  47. Почему BERT во многом проигрывает **RoBERTa** и что вы можете взять у RoBERTa?
        - ***ответ***
            
            [пост](https://t.me/grokaem_seby/244)
            
            BERT'e мы брали один sequence, брали 10 разных маск, дублировали данные 10 раз, чтобы одна sequence получала 10 разных маск за эпоху. Тем самым при обучении модель видела одну и ту же маску n-epochs times.
            
            Это не оптимально в ситуации, когда у нас гигабайты памяти и длинные предложения. Маскировать нужно по-разному. Поэтому на каждый train step генерится своя рандомная маска.
            
    - [ ]  48. Что такое **T5** и **BART** модели? Чем они отличаются?
    - [ ]  49. Что такое **task-agnostic** модели? Приведите примеры.
    - [ ]  50. Объясните transformer модели сравнивая BERT, GPT и T5.
    - [ ]  51. Какая большая проблема есть в моделях BERT, GPT и тд относительно знаний модели? Как это можно решать?
    - [ ]  52. Как работает decoder like а-ля GPT на обучении и инференсе. В чем разница?
    - [ ]  53. Объясните разницу между головами и слоями в трансформер моделях.
    
- **POSITIONAL ENCODING (5/6)**
    - [ ]  54. Почему в эмбеддингах transformer моделей с attention теряется информациях о позициях?
        - ***ответ***
            
            В классическом transformer мы обрабатываем каждый токен (его эмбеддинг) относительно других токенов. Без позициональной информации (которая нам была доступна с rnn) возможна ситуация, где два одинаковых токена на разных позициях получат одинаковые репрезентации.
            
            [пост](https://t.me/grokaem_seby/255)
            
    - [ ]  55. Объясните подходы для позициональных эмбеддингов и их плюсы и минусы.
        - ***ответ***
    - [ ]  56. Почему нельзя просто добавить эмбеддинг с индексом токена?
        - ***ответ***
            
            Если мы будем как-то добавлять новый эмбеддинг, для нас важно, чтобы позициональная информация не была "сильнее" чем семантическая. То есть чтобы в пространстве наши эмбеддинги не сильно смещались из-за позициональных эмбеддингов. А так будет происходить если мы на 100 индекс добавим позиционный эмбеддинг с сотнями. Это хорошо показано в этом [видео](https://www.youtube.com/watch?v=1biZfFLPRSY&t=307s).
            
    - [ ]  57. Почему мы не учим positional embeddings?
        - ***ответ***
            
            Почему не учим? Учим. Таким образом мы лишь добавляем новый слой обычного эмбеддинга в модель.
            [paper](https://arxiv.org/pdf/2010.04903.pdf), который объясняет разницу и эффекты от learned and predefined.
            
    - [ ]  58. Что такое **relative** и **absolute** positional encoding?
        - ***ответ***
            
            В обычных эмбеддингах мы добавляли absolute positional embedding к текущему, то есть не учитывали позиции других эмбеддингов при создании текущего напрямую в positional embedding. Способов имплементации relative encoding довольно много. Интуиция за этой технологией [в этом видео](https://www.youtube.com/watch?v=DwaBQbqh5aE)  и объясняется обучением новых эмбеддингов отношений между позициями. То есть у каждой позиции есть дополнительные n позициональных эмбеддингов (для values и для keys), которые указывают на соотношение с остальными эмбеддингами (n=длина последовательности). В дополнение у нас есть clip на макс длину.
            
            [paper](https://arxiv.org/pdf/1803.02155.pdf) 
            
    - [ ]  59. Подробно объясните принцип работы **rotary** positional embeddings
        - ***ответ***
            
            Rotary positional embeddings - это техника, где мы не создаем новые эмбеддинги, чтобы заметчить позиции, а делаем трансформацию, которая помогает добавить relative отношения между эмбеддингами на разных позициях. Трансформация схожа с тем, что мы видели для sin-cosine, однако теперь у нас происходит этот rotation эмбеддингов внутри attention. Это клево можно посмотреть в коде тут ([https://nn.labml.ai/transformers/rope/index.html#:~:text=Rotary Positional Embeddings (RoPE) encode,incorporates explicit relative position dependency.)](https://nn.labml.ai/transformers/rope/index.html#:~:text=Rotary%20Positional%20Embeddings%20(RoPE)%20encode,incorporates%20explicit%20relative%20position%20dependency.)).
            
    
- **PRETRAINING (4)**
    - [ ]  60. Как обучается **causal language modelling**?
    - [ ]  61. Когда мы используем предобученную модель?
    - [ ]  62. Как обучить transformer с нуля? Объясните свой пайплайн и в каком случае вы будете этим заниматься.
    - [ ]  63. Какие модели кроме BERT и GPT по различным задачам предобучения вы знаете?
    
- **TOKENIZERS (9)**
    - [ ]  64. Какие виды **токенайзеров** вы знаете? Сравните их.
    - [ ]  65. Можете ли вы расширять токенайзер? Если да, то в каком случае вы будете этим заниматься? Когда вы будете переобучать токенайзер? Что необходимо сделать при добавлении новых токенов?
    - [ ]  66. Чем обычные токены отличаются от **специальных** токенов?
    - [ ]  67. Почему в трансформерах не используется лемматизация? И зачем нам нужны токены?
    - [ ]  68. Как обучается токенизатор? Объясните на примерах **WordPiece** и **BPE**.
    - [ ]  69. На какой позиции стоит CLS вектор? Почему?
    - [ ]  70. Какой токенизатор используется в BERT, а какой в GPT?
    - [ ]  71. Объясните как современные токенизаторы обрабатывают out-of-vocabulary words?
    - [ ]  72. На что влияет tokenizer vocab size? Как вы будете его выбирать в случае нового обучения?
    
- **TRAINING (1/14)**
    - [ ]  73. Что такое дисбаланс классов? Как это можно увидеть? Назовите все подходы для решения этой проблемы.
    - [ ]  74. Можно ли использовать на инференсе dropout и почему?
    - [ ]  75. В чем отличие оптимизатора **Adam** от AdamW?
        - ***ответ***
            
            
    - [ ]  76. Как изменяются потребляемые ресурсы при **gradient accumulation**?
        - ***ответ***
    - [ ]  77. Как оптимизировать потребление ресурсов при обучении?
    - [ ]  78. Какие знаете способы распределенного обучения?
    - [ ]  79. Что такое текстовые аугментации? Назовите все методы, что знаете.
        - ***ответ***
    - [ ]  80. Почему стал реже использовать паддинг? Что делают вместо этого?
    - [ ]  81. Объясните как работает **warm-up**?
    - [ ]  82. Объясните концепцию **gradient clipping**?
        - ***ответ***
    - [ ]  83. Как работает **teacher forcing**, приведите примеры?
    - [ ]  84. Как и почему нужно использовать **skip connection**?
        - ***ответ***
    - [ ]  85. Что такое адаптеры? Где и как мы можем их использовать?
        - ***ответ***
            
            Адаптеры - это техника, при которой мы добавляем bottleneck модули и делаем update только их весов, при этом мы замораживаем все остальные веса модели. Наиболее частый подход adapter module это layer norm → feed forward down → non-linearity → feed forward up → layer norm. 
            
            На картинке представлен пример, как мы добавляем adapter в трансформер модель: 
            
            ![image.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/image.png)
            
            Методов для улучшения и модификации адаптеров много: 
            
            1. Adapter Fusion, [paper](https://arxiv.org/pdf/2005.00247.pdf) - эффективный метод слияния multitask адаптеров
            2. Adapter Split - когда мы сплитим входную последовательность для обработки на двух и более адаптерах 
            3. Adapters as identity functions without a bottleneck 
            
            Adapters add extra layers to the original model, which increases the neural network's depth and causes additional latency during the inference phase.
            
            **Плюсы:** 
            
            1. Меньшее количество весов модели, которые мы тренируем 
            2. Может быть использовано для domain adaptation 
            3. Также может быть использовано для incremental learning, чтобы продолжать обучение
            4. Также помогает ускорить обучение, так как мы обучаем меньше весов. 
            
            **Минусы:** 
            
            1. Из-за того, что мы добавляем модули sequentially - один за другим, нам необходимо на инференсе ждать выхода предыдущего adapter’а перед проходом. Расширение этого метода - это AdapterDrop, когда мы выкидываем определенные adapter modules, если они не получают  активации, [paper](https://arxiv.org/abs/2010.11918) 
            2. Те же самые минусы как и всегда с overfitting. 
            
            Adapters используются в случае, когда необходимо зафайнтюнить модель на downstream task/domain и при этом мы ограничены в ресурсах для обучения. 
            
    - [ ]  86. Объясните концепции **metric learning**. Какие подходы вам известны?
        - ***ответ***
            
            
- **INFERENCE (3/4)**
    - [ ]  87. За что отвечает температура в softmax? Какую вы будете выставлять?
        - ***ответ***
            
            **Temperature** 
            
            [link](http://www.kasimte.com/2020/02/14/how-does-temperature-affect-softmax-in-machine-learning.html)
            
            [language example](https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/)
            
            [пост](https://t.me/grokaem_seby/120) 
            
            Temperature is a hyper-parameter which is applied to logits to affect the final probabilities from the softmax.
            
            - A low temperature (below 1) makes the model **more confident.**
            - A high temperature (above 1) makes the model **less confident.**
            
            The logits layer is often followed by a *softmax* layer, which turns the logits back into probabilities (between 0 and 1). From StackOverflow:
            
            ```python
            def softmax(logits):
                bottom = sum([math.exp(x)for xin logits])
                softmax = [math.exp(x)/bottomfor xin logits]
            return softmax
            
            softmax(logits), sum(softmax(logits))
            
            # ([0.25, 0.75], 1.0)
            
            low_temp = 0.5
            logits_low_temp = [x/low_temp for x in logits]
            softmax(logits_low_temp), sum(softmax(logits_low_temp))
            # ([0.1, 0.9], 1.0)
            
            low_temp = 5
            logits_high_temp = [x/low_temp for x in logits]
            #([0.44528931866219296, 0.5547106813378071], 1.0)
            ```
            
    - [ ]  88. Объясните виды **sampling** при генерации? **top-k**, top-p, **nucleus sampling**?
        - ***ответ***
            
            **Top-K sampling** 
            
            In *Top-K* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words.
            
            ![Untitled](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Untitled%201.png)
            
            [math and code](https://nn.labml.ai/sampling/nucleus.html) 
            
            **Nucleus sampling** 
            
            *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. The probability mass is then redistributed among this set of words. This way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution.
            
            ![Untitled](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Untitled%202.png)
            
    - [ ]  89. Какая сложность у **beamsearch** и как он работает?
        - ***ответ***
            
            Beam search - это алгоритм поиска, который расширяет граф относительно ограниченного набора. На каждом этапе мы для каждой ноды мы добавляем новый сет гипотез n размера. Сохраняя только top k кандидатов относительно всех других.  
            
            Complexity: O(T * B * m * logB), where B - beam size, T - total number of steps and m - number of candidates 
            
            Beam size K controls tradeoff between efficiency and accuracy
            • K = 1 is greedy search (O(nb) time)
            • K = ∞ is BFS (O(b
            n) time)
            
            ![Screenshot 2024-08-09 at 10.48.54.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-08-09_at_10.48.54.png)
            
            [excellent explanation paper for ctc loss](https://distill.pub/2017/ctc/)  
            
    - [ ]  90. Что такое sentence embedding? Какими способами вы можете его получить?
        - ***ответ***
            1. CLS vector 
            2. Pooling 
            3. Mean 
            4. Conv on the top 

## **Large Language Models (LLM)**

- **LLM base questions (2/10)**
    - [ ]  91. Как работает **LoRA**? Как вы будете выбирать параметры? Представьте, что мы хотим дообучить большую языковую модель, делаем LORA с маленьким R, но модель все равно не лезет по памяти, что еще можно сделать?
        - ***ответ***
            
            LoRA или Low Rank Adaptation - техника, при которой мы вместо того, чтобы добвлять модули как в adapter или файнтюнить всю матрицу весов - добавляем новые веса к нашей исходной матрице. При эту новую матрицу мы раскладываем по SVD, где r - это ранг матрицы, который для нас будет являться гиперпараметром. A and B - это матрицы, которые мы как раз обучаем. 
            
            $$
             \mathbf{W} \in \mathbb{R}^{d \times k}
            $$
            
            $$
            \Delta \mathbf{W} = \mathbf{A} \mathbf{B}
            $$
            
            $$
            \mathbf{A} \in \mathbb{R}^{d \times r} \; \mathbf{B} \in \mathbb{R}^{r \times k}
            $$
            
            [video explanation](https://www.youtube.com/watch?v=KEv-F5UkhxU) 
            
            ![image.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/image%201.png)
            
            Если модель все же не помещается, то можно применить другие техники такие как: 
            
            QLoRA - Quantized LoRA
            
            QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).
            [explained](https://medium.com/@ayyucedemirbas/qlora-4444944c20bd) , [еще одно более подробное объяснение](https://medium.com/@raniahossam/comprehensive-guide-to-adapters-lora-qlora-longlora-with-implementation-de003be30352) 
            
            1. block-wise k-bit quantization для модели, чтобы уменьшить memory spikes, перекидывая state оптимизатора на cpu. 
            2. Paged Optimizers - works by partitioning the model parameters into small groups and processing each group separately using a separate optimizer. During each iteration of the optimization process, only one group of parameters is loaded into memory at a time, while the others remain on disk. This enables QLORA to finetune large models on a single GPU without running out of memory.
            
            - этапы обучения
                
                ### 1. Quantization
                
                - **4-bit NormalFloat (NF4)**: A new data type that quantizes weights to 4 bits.
                - **Double Quantization**: Quantizes the quantization constants themselves, further reducing memory usage.
                
                ### 2. LoRA
                
                - Adds trainable low-rank matrices to frozen quantized weights.
                - Updates only a small number of parameters during fine-tuning.
                
                ### 3. Paged Optimizers
                
                - Uses CPU memory to store optimizer states, reducing GPU memory usage.
                
                ### 4. Gradient Checkpointing
                
                - Trades computation for memory by recomputing activations during backpropagation.
            
    - [ ]  101. Объясните пользу parameter efficient finetuning над полным finetuning модели?
        - ***ответ***
    - [ ]  92. В чем отличие **prefix tuning** от **p-tuning** и от **prompt tuning**?
        - ***ответ***
            
            Кратко:
            
            prompt-tuning = добавляем виртуальные токены в начало промпта
            p-tuning = добавляем виртуальные токены куда-нибудь в промт (например, в середину)
            prefix-tuning = добавляем префиксы во все слои. 
            
            > **Prompt Tuning**: Tunes a set of concatenated input embeddings vectors (generally called "soft prompts", but not referring to the soft prompts here). Initially applied to T5-LM models.
            > 
            
            > **Prefix Tuning**: Tunes KV cache (soft prefixes) for every layer, and can be casually described as "prompt tuning, but in every layer", although that is slightly inaccurate. In practice, uses an auxiliary MLP to generate the soft prefixes to help training. Initially applied to GPT-2 and BART models.
            > 
            
            > **P-Tuning**: Uses LSTMs to generate soft prompts (not prefixes). Initially applied to GPT-2 and BERT/RoBERTa/MegatronLM models.
            > 
            
            > **P-Tuning v2:** Essentially Prefix Tuning applied to BERT-type models.
            > 
            
            Не кратко:
            
            1. **Prompt-tuning** 
                
                Transform the idea to a fixed prompt of special tokens where only the embeddings of these tokens can be updated. The new conditional generation, where the gradient updates are applied only to trainable parameters
                
                $$
                \theta_P
                $$
                
                $$
                \Pr{\theta;\theta_P}(Y \mid [P; X])
                $$
                
                ![Screenshot 2024-08-12 at 11.29.03.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-08-12_at_11.29.03.png)
                
                [paper: The power of Scale for Parameter-Efficient Prompt Tuning](https://aclanthology.org/2021.emnlp-main.243/) 
                
            2. **P-tuning** 
            Метод, при котором мы добавляем не дискретные токены, а эмбеддинги, которые мы получили через prompt encoder. Сами эмбеддинги добавляются как в начало, так и между x input и y output. 
                
                Function f is an external embedding function that map the template to (h0, hi…) - original embedding function. Only the embeddings Pi are updated. 
                
                $$
                f: [P_i] => h_i
                $$
                
                ![Screenshot 2024-08-12 at 10.53.07.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-08-12_at_10.53.07.png)
                
                [paper: GPT understands it, too](https://arxiv.org/abs/2103.10385)
                
            3. **Prefix Tuning** 
                
                Похожий подход, при котором мы добавляем prefix, который на самом деле является task-specific one. В prefix tuning мы не создаем новые токены и prefix эмбеддинги на относятся к каким-то виртуальным токенам.  
                ****
                
                ![Screenshot 2024-08-12 at 11.22.17.png](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c/Screenshot_2024-08-12_at_11.22.17.png)
                
                [paper: Prefix-Tuning: optimizing continuous Prompts for generation](100%20questions%20about%20NLP%20fe80057c45c0461eb937128eb248979c.md)
                
    - [ ]  93. Объясните **scaling law**.
    - [ ]  94. Объясните все этапы обучения LLM. От какого из этапов мы можем отказывать и в каких случаях.
    - [ ]  95. Как работает **RAG**? Чем он отличается от few-shot KNN?
    - [ ]  96. Какие методы квантизации вы знаете? Можем ли мы тюнить квантизованные модели?
    - [ ]  97. Как вы сможете предотвращать катастрофическое забывание у LLM?
    - [ ]  98. Объясните принцип работы **KV cache**, **Grouped-Query Attention** и **MultiQuery Attention**.
    - [ ]  99. Объясните технологию, которая стоит за MixTral, в чем ее плюсы и минусы?
    - [ ]  100. Че как сам? Как дела?
- **LLM quantization (3)**
    - [ ]  102. В каких случаях вы будете использовать квантизацию в resource-constrained environments? И как будете выбирать параметры для квантизации. 
    [о квантизации](https://habr.com/ru/companies/yandex/articles/800945/) 
    [квантизации и LLM](https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/)
    - [ ]  103. Объясните концепцию quantization-aware обучения и чем она отличается от post-quantization техник?
    - [ ]  104. Объясните разницу и плюсы EDGE inference. 
    [ресурс](https://infohub.delltechnologies.com/en-US/p/inferencing-at-the-edge/)
    [ресурс 2](https://www.asianometry.com/p/the-hard-problems-of-edge-ai-hardware)
- **ANALYSE questions  дополнительные**
    1. Назовите наиболее важные на ваш взгляд современные тренды в NLP.
    2. Генеративная decoder-only модель выучила некоторые "плохие" паттерны в данных. В результате иногда она генерирует "плохие" тексты. Как сделать так, чтобы она их не генерировала? (Вопрос намеренно поставлен широко. От стажёра/джуна 1-2 гипотезы, от мидла - 2-3 возможных подхода, от сеньора - 4-5 подходов с разбором их достоинств и недостатков)
    3. Допустим нам надо обучить классификатор на k классов, из разметки есть о 10 примеров на каждый, как учить? 
    Предполагается, что либо больше про синтетические данные кандидат расскажет, как их генерить и тд, либо в идеальном случае про few-shot learning.
    4. Может ли быть такое, что модель дает вероятность какого-то класса 90+%, но при этом все равно ошибается? 
    5. Назовите последнюю статью, которую вы читали?