```toc
```


# BERT (Bidirectional Encoder Representations from Transformers)

## Описание
BERT — это модель на основе трансформеров, разработанная для обработки естественного языка. Она отличается тем, что учитывает контекст слова не только слева направо (как это делается в традиционных моделях), но и справа налево. Это достигается благодаря использованию двухстороннего (bidirectional) подхода, что позволяет модели лучше понимать контекст слов в предложении.

## Как работает

BERT состоит из многослойного энкодера, основанного на архитектуре трансформеров. Основные элементы:

1. **Входные данные**: Входное предложение токенизируется и добавляются специальные токены:
   - `[CLS]` — в начало предложения.
   - `[SEP]` — разделяет предложения (используется для задач, где модель работает с парами предложений).

2. **Токенизация**: Используется метод WordPiece, который разбивает слова на подслова (subwords), что помогает справляться с редкими или неизвестными словами.

3. **Эмбеддинги**:
   - **Токен-эмбеддинги**: Векторные представления каждого токена.
   - **Position embeddings**: Вектора, которые кодируют позиционную информацию токенов.
   - **Segment embeddings**: Вектора, указывающие, к какому из предложений (A или B) относится данный токен.

4. **Энкодер**: Многослойная модель на основе самовнимания (self-attention), которая позволяет модели учитывать контекст токенов со всех направлений (справа и слева одновременно).

## Как учится

BERT обучается в два этапа:

1. **Предварительное обучение (Pre-training)**:
   - **Masked Language Modeling (MLM)**: Некоторое количество токенов входной последовательности случайно заменяется на `[MASK]`, и задача модели — предсказать эти замаскированные токены, используя контекст.
   - **Next Sentence Prediction (NSP)**: Модель обучается определять, является ли второе предложение продолжением первого или нет.

2. **Тонкая настройка (Fine-tuning)**:
   - После предварительного обучения, BERT адаптируется под конкретную задачу (например, классификацию текста, ответы на вопросы и т.д.). В этом этапе участвуют все параметры модели, которые настраиваются под задачу с использованием размеченных данных.

## Применение

BERT используется для решения различных задач NLP, таких как:
- Классификация текста
- Ответы на вопросы
- Распознавание именованных сущностей (NER)
- Перевод текста
- Анализ сентимента

Его универсальность и способность учитывать контекст делает его одной из самых популярных моделей в NLP.



# BART (Bidirectional and Auto-Regressive Transformers)

## Описание
BART — это модель на основе трансформеров, разработанная Facebook AI, которая сочетает в себе идеи двухстороннего (bidirectional) кодирования, как у BERT, и авто-регрессионного (auto-regressive) декодирования, как у GPT. Это позволяет BART эффективно выполнять задачи, связанные как с пониманием текста, так и с его генерацией.

## Как работает

BART состоит из двух частей:

1. **Энкодер (Encoder)**:
   - Двусторонний (bidirectional) энкодер, аналогичный тому, который используется в BERT.
   - Энкодер принимает на вход весь текст целиком и создает контекстные представления для каждого токена, учитывая как левый, так и правый контексты.

2. **Декодер (Decoder)**:
   - Авто-регрессионный (auto-regressive) декодер, аналогичный тем, что используются в GPT.
   - Декодер генерирует текст последовательно, предсказывая каждый следующий токен на основе предыдущих.

## Как учится

BART обучается в формате задачи восстановления текста:

1. **Обфускация (Corruption)**:
   - В процессе обучения оригинальный текст искажается с помощью одного из методов (например, перетасовки слов, удаления фраз, маскирования или добавления шума).

2. **Восстановление (Denoising)**:
   - Модель обучается восстанавливать оригинальный текст из искаженной версии. Для этого энкодер создает контекстные представления для искаженного текста, а декодер восстанавливает исходный текст по этим представлениям.

## Применение

BART универсальна и может быть использована для решения различных задач NLP:

- **Суммаризация текста**: Генерация краткого содержания исходного текста.
- **Перевод текста**: Перевод текста с одного языка на другой.
- **Восстановление текста**: Исправление искажения или удаления в тексте.
- **Генерация текста**: Создание нового текста на основе заданных условий.
- **Ответы на вопросы (Question Answering)**: Ответы на вопросы на основе текста.

## Преимущества

- **Гибкость**: BART может решать как задачи, связанные с пониманием текста (аналогично BERT), так и задачи генерации текста (аналогично GPT).
- **Эффективность**: За счет двустороннего энкодера и авто-регрессионного декодера модель эффективно справляется с задачами восстановления и генерации текста.
- **Универсальность**: Подходит для широкого круга задач NLP, от суммаризации до перевода и генерации текста.

BART является мощной моделью, объединяющей преимущества и BERT, и GPT, что делает ее отличным выбором для сложных задач обработки естественного языка.



# RAG (Retrieval-Augmented Generation)

## Описание
RAG (Retrieval-Augmented Generation) — это гибридная модель, которая сочетает в себе возможности генеративных моделей и подходов на основе поиска информации. Она разработана для того, чтобы генерировать ответы или текст, используя как информацию из внешних источников, так и собственные генеративные способности модели.

## Как работает

RAG состоит из двух основных компонентов:

1. **Поисковая модель (Retriever)**:
   - Используется для поиска релевантной информации из большого корпуса текстов (например, Wikipedia, база данных документов).
   - Входной запрос преобразуется в эмбеддинг, который затем сравнивается с эмбеддингами документов в базе данных.
   - Возвращается топ-N документов, наиболее соответствующих запросу.

2. **Генеративная модель (Generator)**:
   - Генеративная модель, например, основанная на трансформерах (как GPT или BART), используется для создания текста.
   - На вход генеративной модели подаются как сам запрос, так и информация, полученная от поисковой модели.
   - Модель использует эту информацию для генерации более точных и контекстуально релевантных ответов.

## Как учится

Обучение RAG происходит в два этапа:

1. **Обучение поисковой модели**:
   - Поисковая модель обучается на задаче поиска релевантных документов для входного запроса.
   - Часто используется контрастивное обучение, где модель учится отличать правильные документы от нерелевантных.

2. **Обучение генеративной модели**:
   - Генеративная модель обучается на задаче генерации текста на основе как запроса, так и информации из найденных документов.
   - Модель может быть предобучена на больших корпусах текстов и затем дообучена совместно с поисковой моделью для конкретных задач.

## Применение

RAG используется в задачах, где необходимо генерировать информативные ответы на основе внешних источников данных, например:
- Ответы на вопросы (Question Answering)
- Генерация текстов на основе данных
- Помощь в исследованиях и генерации контента
- Чат-боты, которые нуждаются в доступе к внешним базам знаний

## Преимущества

- **Точность**: Благодаря интеграции информации из внешних источников, RAG может генерировать более точные и информативные ответы, чем чисто генеративные модели.
- **Гибкость**: Возможность использования различных баз данных и источников для поиска информации.
- **Универсальность**: Подходит для множества задач, связанных с поиском информации и генерацией текста.

RAG — мощный инструмент для задач, где требуется сочетание поиска информации и генерации текста на ее основе, что делает его особенно полезным в сложных задачах обработки естественного языка.


# P-Tuning

## Описание
P-Tuning (Prompt Tuning) — это методика настройки больших языковых моделей с использованием специальных текстовых подсказок (prompts), которые добавляются к входным данным для улучшения качества выполнения модели на конкретных задачах. Эта техника позволяет эффективно адаптировать предобученные модели к новым задачам без значительной дообучения всей модели.

## Как работает

Основная идея P-Tuning заключается в том, что вместо тонкой настройки всех параметров модели для новой задачи, к исходным данным добавляется параметризованная текстовая подсказка, которая помогает модели лучше понимать задачу.

1. **Текстовые подсказки (Prompts)**:
   - К входному тексту добавляются специальные шаблоны или подсказки, которые описывают задачу.
   - Эти подсказки могут быть фиксированными или обучаемыми (параметризованными).

2. **Параметризованные подсказки**:
   - В P-Tuning подсказки обучаются так же, как и параметры модели. Эти обучаемые подсказки вставляются в начало или конец входной последовательности.
   - Параметры подсказок оптимизируются в процессе обучения на целевой задаче.

3. **Минимальное обновление модели**:
   - Основная модель остается неизменной, а оптимизация проводится только для параметризованных подсказок.
   - Это делает метод очень эффективным с точки зрения вычислительных ресурсов, особенно для работы с большими моделями.

## Применение

P-Tuning используется в следующих сценариях:

- **Классификация текста**: Адаптация модели для решения задач классификации на основе текстовых подсказок.
- **Ответы на вопросы (Question Answering)**: Настройка модели для более точного ответа на вопросы.
- **Анализ сентимента**: Улучшение модели для определения тональности текста.
- **Перенос обучения на новые задачи**: Быстрая адаптация модели к новым задачам без значительной дообучения.

## Преимущества

- **Экономия ресурсов**: Обучение и адаптация модели требуют значительно меньше вычислительных ресурсов, так как обновляются только подсказки, а не вся модель.
- **Улучшение качества**: Хорошо настроенные текстовые подсказки могут значительно улучшить результаты модели на конкретных задачах.
- **Удобство в использовании**: P-Tuning позволяет быстро адаптировать уже предобученные модели к новым задачам, что ускоряет процесс разработки.

P-Tuning — это мощный инструмент для настройки и адаптации больших языковых моделей, особенно полезный в условиях ограниченных вычислительных ресурсов или при работе с новыми задачами.
