```toc
```


# BERT (Bidirectional Encoder Representations from Transformers)

## Описание
BERT — это модель на основе трансформеров, разработанная для обработки естественного языка. Она отличается тем, что учитывает контекст слова не только слева направо (как это делается в традиционных моделях), но и справа налево. Это достигается благодаря использованию двухстороннего (bidirectional) подхода, что позволяет модели лучше понимать контекст слов в предложении.

## Как работает

BERT состоит из многослойного энкодера, основанного на архитектуре трансформеров. Основные элементы:

1. **Входные данные**: Входное предложение токенизируется и добавляются специальные токены:
   - `[CLS]` — в начало предложения.
   - `[SEP]` — разделяет предложения (используется для задач, где модель работает с парами предложений).

2. **Токенизация**: Используется метод WordPiece, который разбивает слова на подслова (subwords), что помогает справляться с редкими или неизвестными словами.

3. **Эмбеддинги**:
   - **Токен-эмбеддинги**: Векторные представления каждого токена.
   - **Position embeddings**: Вектора, которые кодируют позиционную информацию токенов.
   - **Segment embeddings**: Вектора, указывающие, к какому из предложений (A или B) относится данный токен.

4. **Энкодер**: Многослойная модель на основе самовнимания (self-attention), которая позволяет модели учитывать контекст токенов со всех направлений (справа и слева одновременно).

## Как учится

BERT обучается в два этапа:

1. **Предварительное обучение (Pre-training)**:
   - **Masked Language Modeling (MLM)**: Некоторое количество токенов входной последовательности случайно заменяется на `[MASK]`, и задача модели — предсказать эти замаскированные токены, используя контекст.
   - **Next Sentence Prediction (NSP)**: Модель обучается определять, является ли второе предложение продолжением первого или нет.

2. **Тонкая настройка (Fine-tuning)**:
   - После предварительного обучения, BERT адаптируется под конкретную задачу (например, классификацию текста, ответы на вопросы и т.д.). В этом этапе участвуют все параметры модели, которые настраиваются под задачу с использованием размеченных данных.

## Применение

BERT используется для решения различных задач NLP, таких как:
- Классификация текста
- Ответы на вопросы
- Распознавание именованных сущностей (NER)
- Перевод текста
- Анализ сентимента

Его универсальность и способность учитывать контекст делает его одной из самых популярных моделей в NLP.


# RAG (Retrieval-Augmented Generation)

## Описание
RAG (Retrieval-Augmented Generation) — это гибридная модель, которая сочетает в себе возможности генеративных моделей и подходов на основе поиска информации. Она разработана для того, чтобы генерировать ответы или текст, используя как информацию из внешних источников, так и собственные генеративные способности модели.

## Как работает

RAG состоит из двух основных компонентов:

1. **Поисковая модель (Retriever)**:
   - Используется для поиска релевантной информации из большого корпуса текстов (например, Wikipedia, база данных документов).
   - Входной запрос преобразуется в эмбеддинг, который затем сравнивается с эмбеддингами документов в базе данных.
   - Возвращается топ-N документов, наиболее соответствующих запросу.

2. **Генеративная модель (Generator)**:
   - Генеративная модель, например, основанная на трансформерах (как GPT или BART), используется для создания текста.
   - На вход генеративной модели подаются как сам запрос, так и информация, полученная от поисковой модели.
   - Модель использует эту информацию для генерации более точных и контекстуально релевантных ответов.

## Как учится

Обучение RAG происходит в два этапа:

1. **Обучение поисковой модели**:
   - Поисковая модель обучается на задаче поиска релевантных документов для входного запроса.
   - Часто используется контрастивное обучение, где модель учится отличать правильные документы от нерелевантных.

2. **Обучение генеративной модели**:
   - Генеративная модель обучается на задаче генерации текста на основе как запроса, так и информации из найденных документов.
   - Модель может быть предобучена на больших корпусах текстов и затем дообучена совместно с поисковой моделью для конкретных задач.

## Применение

RAG используется в задачах, где необходимо генерировать информативные ответы на основе внешних источников данных, например:
- Ответы на вопросы (Question Answering)
- Генерация текстов на основе данных
- Помощь в исследованиях и генерации контента
- Чат-боты, которые нуждаются в доступе к внешним базам знаний

## Преимущества

- **Точность**: Благодаря интеграции информации из внешних источников, RAG может генерировать более точные и информативные ответы, чем чисто генеративные модели.
- **Гибкость**: Возможность использования различных баз данных и источников для поиска информации.
- **Универсальность**: Подходит для множества задач, связанных с поиском информации и генерацией текста.

RAG — мощный инструмент для задач, где требуется сочетание поиска информации и генерации текста на ее основе, что делает его особенно полезным в сложных задачах обработки естественного языка.
