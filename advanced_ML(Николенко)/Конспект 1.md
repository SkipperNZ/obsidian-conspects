[[Adv_Ml (Николенко)]]

https://www.youtube.com/watch?v=Zcu5vtaDhQE&t=2s&ab_channel=VKTeam

Лекция 1

Понятия которые пригодятся в курсе:
**Дискретные случайные величины** - у них есть некоторый набор исходов, у исходов есть вероятности, которые в сумме дают единицу.
 **Непрерывные случайные величины** - когда набор/исходно-вероятностное пространство - непрерывное пространство (в нашем случае это всегда евклидово пространство Rn или его часть) и тогда мы будем ненавязчиво переходить к плотности этого распределения P(x) интеграл которой равен единице и которая даёт вероятность тех или иных подмножеств.


Основные понятия.

Бывают случайные величины, у них бывают исходы, бывают вероятности исходов, бывают события(множества исходов)
![[Pasted image 20220124205444.png]]

$\large p(x,y)$ - совместная вероятность. Это вероятность того, что и $\large x$ такой выпадет и $\large y$ такой выпадет 

А в случае дискретных исходов $\large p(x = a, y = b)$ - вероятность что $\large x$ равен $\large a$  и при этом $\large y$ равен $\large b$

 Самое главное, что можно делать с совместными распределениями - их можно **маргинализировать**

Маргинализация  означает,  что если мы хотим из совместного распределения получить $\large p(x)$ то должны проинтегрировать:
$$\large
p(x) = \int{p(x,y)dy}
$$
Эта процедура называется маргинализацией
И по сути хорошо напрягшись мы можем всё машинное обучение можно представить в виде задач маргинализации.
Этому будет посвящён весь курс.

Условная вероятность:
То как меняется распределение $\large x$ после того как мы узнаём конкретное значение $\large y$ 
$$\large
p(x|y)= \frac{p(x,y)}{p(y)}
$$
Связь с машинным обучением: 
Машинное обучение - это когда мы хотим например провести прямую(найти параметры этой прямой) в зависимости от значения $\large y$ в точках. То есть мы хотим посмотреть что станет с распределением на параметрах нашей модели, когда мы получаем $\large y$ после того как мы получим данные.


**Формула Байеса**:
$$\large
p(x,y) = p(x|y)p(y)= p(y|x) p(x)
$$
Это равенство приводит нас к вот такому результату:
$$\large
p(x|y)= \frac{p(y|x)p(x)}{p(y)}
$$
Почему это главная формула в машинном обучении. 
Запишем то же самое но с другими буквами:

$$\large \tag{1337}
p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)}
$$
$\theta$ - параметры модели
D - данные

То есть $\large p(\theta | D)$ - как раз та условная вероятность, которую мы хотим оценить. То что мы будем думать про параметры модели, после того как учтем все имеющиеся данные.  (например в регрессии то, что мы будем думать о наклоне и смещении прямой, после того как добавим все точки данных и учтем всё то, что они нам говорят)

Это распределение $\large p(\theta | D)$ называется **апостериорным** распределением. (posterior distribution)   и это наша основная цель.
Как правило первая задача машинного обучения найти это апостериорное распределение. 

И теорема байеса (формула 1337) как раз говорит нам что это апостериорное распределение можно представить в таком виде. 
Где:

$\large p(D | \theta)$ - правдоподобие (обычно и есть наша модель) likehood
То есть под заданием модели машинного обучения, как правило подразумеваем задание формулы правдоподобия. ( например в линейной регрессии  основное предположение что данные которые мы видим берутся вот от куда. есть фиксированный $\large x$, а  $\large y$  получается прибавлением нормально распределённого шума)

$\large p(\theta)$ - второй сомножитель априорное распределение (prior)
Это то, что мы думали о параметрах этого распределения, до того как мы начали получать данные. 

$\large p(D)$ - evidence (Он не особо нужен а получить его можно интегрированием)

Теорему Байеса можно (и даже нужно) приписать в виде:
$$\large
p(\theta | D) \propto {p(D | \theta) p(\theta)}
$$
Где $\large \sim \ , \   \propto$   - знаки пропорциональности

Для заметки: $\large p(D) = \int p(D|\theta)p(\theta)d\theta$ 


Что нам нужно в машинном обучении, раньше говорилось что нужно $\large p(\theta | D)$, но по факту нужно отличать котиков от пёсиков, а эта вероятность, явно промежуточный шаг. 
Соответственно, на самом деле в машинном обучении чаще всего будем рассматривать задачи: 

1) Максимизация правдоподобия - max likelihood
$$ \large
\theta_{ml}= \arg\max_{\theta} p(D|\theta)
$$
Для некоторых моделей всё на этом и останавливается, но мы иногда хотим большего, хотим продвинуться к апостериорному распределению и найти максимальную апостериорную гипотезу.

2) MAP - maximum a posteriori
$$ \large
\theta_{map}= \arg\max_{\theta} p(\theta | D) = \arg\max_{\theta} p(D | \theta)p(\theta)
$$
на самом деле (на примере линейной регрессии) нам не очень интересна прямая с параметрами модели, нам надо для x найти y.

3) Задача построения предсказания, то есть задача поиска предсказательного распределения predictive distribution
Это распределение следующей точки данных, при условии имеющихся точек:
$$\large \tag{нач}
p(x|D) = \int p(x,\theta | D) d\theta = 
\int p(x|\theta , D) \cdot p(\theta|D) d\theta =
$$
$\large p(\theta|D)$ - и есть наше апостериорное распределение.
$\large p(x|\theta , D)$ - для всех задач МО делается предположение, что новая точка данных из модели будет порождаться (если мы знаем её параметры) независимо от данных

Что такое независимые случайные величины:
Безусловная независимость:
$$\large
p(x,y) = p(x) \cdot p(y)
$$
Условная независимость: 
$$\large
p(x,y | z) = p(x|z) \cdot p(y|z)
$$
Предположение, которое мы практически всегда делаем в МО заключается в том что **при известных параметрах модели, данные получаются независимо**
Обычно это выглядит на уровне правдоподобия:
Правдоподобие датасета при условии параметров мы всегда записываем как произведение правдоподобия отдельных точек данных. 
$$\large
p(D|\theta)= \prod\limits_{d \in D} p(d|\theta)
$$
То есть из предположения независимости можно убрать D
$$\large \tag{продолжение}
 = \int p(x| \theta)p(\theta | D) d \theta
$$
Где 
$\large p(x|D)$ - правдоподобие одной точки которую мы хотим оценить. (вероятность что картинка содержит котика или вероятность того, что монетка упадёт решкой итд)

Иногда в литературе поиск такого предсказательного распределения 3) называют Байесовским выводом. 

Как видим, не так уж легко его посчитать.

Понятие математического ожидания:
$\large \mathbb{E}$ - знак мат.ожидания
$$\large
p(x|D) = \mathbb{E}_{p(\theta |D)}[p(x|\theta)]
$$

Конкретно нам нужна простая формулировка:

$$\large
\mathbb{E}_{p(x)}[f(x)] = \int f(x)\cdot p(x)dx
$$

Как считать ожидание: 
в идеале посчитать интеграл, но в реальности  это трудно,  в такой ситуации нужно приблизить ожидание через выборку (понакидовать точек из этого распределения и усреднить) 

$$\large
p(x|D) = \mathbb{E}_{p(\theta |D)}[p(x|\theta)] \approx 
\frac{1}{R} \sum\limits_{r=1}^{R}p(x|\theta^{(r)})
$$
Где $\large \theta^{(r)}$ - взята из $\large p(\theta |D)$ 


Пару слов по слайдам: 

Пусть некий тест на какую-нибудь болезнь имеет вероятность успеха 95% (т.е. 5% — вероятность как позитивной, так и негативной ошибки).
Всего болезнь имеется у 1% респондентов (отложим на время то, что они разного возраста и профессий).
Пусть некий человек получил позитивный результат теста (тест говорит, что он болен). С какой вероятностью он действительно болен?
Ответ: 16%

Доказательство:
Обозначим через t результат теста, через d — наличие болезни.
$$\large
p(t=1)=p(t=1|d=1)p(d=1)+p(t=1|d=0)p(d=0)
$$
используя теорему Байеса:
$$\large
p(d=1|t=1) = \frac{p(t=1|d=1)p(d=1)}{p(t=1|d=1)p(d=1)+p(t=1|d=0)p(d=0)}
$$
$$\large
= \frac{0.95 \cdot 0.01}{0.95 \cdot 0.01 + 0.05 \cdot 0.99} = 0.16
$$

Вероятность  из классического учебника - то к чему стремится вероятность при очень большом количестве подбрасываний монетки.
Обычно в классической теории вероятностей, происходящей из физики, вероятность понимается как предел отношения количества определённого результата эксперимента к общему количеству экспериментов. Стандартный пример: бросание монетки


Но мы хотим рассуждать о вероятности по одному примеру. 
* Сборная России победит на чемпионате мира по футболу в N году
* «Одиссею» написала женщина; 
* Керенский бежал за границу в женском платье;
* ...

Но о «стремящемся к бесконечности количестве экспериментов» говорить бессмысленно — эксперимент здесь ровно один.

Здесь вероятности уже выступают как степени доверия (degrees of belief ). Это байесовский подход к вероятностям (Томас Байес так понимал).

К счастью, и те, и другие вероятности подчиняются одним и тем же законам; есть результаты о том, что вполне естественные аксиомы вероятностной логики тут же приводят к весьма узкому классу функций (Cox, 19)

Прямые и обратные задачи в тории вероятности 

* Прямая задача: в урне лежат 10 шаров, из них 3 чёрных. Какова вероятность выбрать чёрный шар?
* Или: в урне лежат 10 шаров с номерами от 1 до 10. Какова вероятность того, что номера трёх последовательно выбранных шаров дадут в сумме 12?
* Обратная задача: перед нами две урны, в каждой по 10 шаров, но в одной 3 чёрных, а в другой — 6. Кто-то взял из какой-то урны шар, и он оказался чёрным. Насколько вероятно, что он брал шар из первой урны?
* Заметьте, что в обратной задаче вероятности сразу стали байесовскими (хоть здесь и можно переформулировать через частоты).


**Приступим к примеру с монеткой:**

Монетка - такая модель в которой 2 исхода - орёл и решка.

у неё всего 1 параметр $\theta = p(орел)$ - вероятность орла
$1 - \theta = р(решка)$ 

D = htthhttt  - данные  h head- орёл, t tail - решка

Что бы задать вероятностную модель надо задать её правдоподобие:
$$\large
p(D|\theta) = \prod\limits_{d \in D} p(d|\theta) =
\theta^{n} (1 - \theta)^{m} 
$$
Где n - число орлов, m - число решек 


$$\large
\theta_{ML} = \arg \max\limits_{\theta} p(D|\theta)
$$
Если учесть что в наших данных 3 орла и 5 решек, должно получиться $\large \frac{n}{n+m}$  проверим это:
$$\large
\frac{dp(D|\theta)}{d\theta} = n\theta^{n-1}(1-\theta)^{m} - m\theta^{n}(1-\theta)^{m-1} 
$$
Считаем экстремум приравнивая к нулю:

$$\large
n\theta^{n-1}(1-\theta)^{m} - m\theta^{n}(1-\theta)^{m-1} =0
$$

$$\large
\theta^{n-1}(1-\theta)^{m-1}(n(1- \theta) -m\theta) =0
$$
Получаем 3 корня 
$$\large
\theta = 0,1, \frac{n}{n+m}
$$
отбрасываем 0 и 1 так как в датасете есть и орлы и решки.
максимум правдоподобия даёт именно 3тий корень.



А теперь самое интересное, берем новую монетку и подбрасываем, 
выпадает орёл. 
Какая теперь гипотеза макс правдоподобия ? - вероятность орла 1
$$\large
p(d|\theta) = \theta -> \max
$$
$$\large
\theta_{ml} -> \infty
$$

Тета не совсем вероятность - это параметр нашей модели, и тогда накладывая  ограничение получаем 

$$\large
\theta_{ml} = 1
$$
С точки зрения максимального правдоподобия всё так, но у нас еще есть априорное распределение, куда мы можем записать наше представление о монетке
Априорное распределение:
$$\large
p(\theta) =  
$$
![[Pasted image 20220131181356.png]]

вообще у $\large p(\theta)$ - форма колокола, но пока упрощённо рассматриваем равномерное распределение 


Апостериорное распределение теперь
$$\large
p(\theta|D) \propto p(D|\theta)p(\theta) = 
 \begin{cases}
 \theta^{n}(1-\theta)^{m}, \theta \in [0, 1] \\ 
 0, \theta \notin [0, 1]

 \end{cases}

$$
Надо пронормировать: 
$$\large
p(D) =\int p(D|\theta)p(\theta)d\theta = \int\theta^{n}(1-\theta)^{m}d\theta = B(n+1, m+1) =
$$
$$\large
= \frac{Г(n+1)Г(m+1)}{Г(n+m+2)} = \frac{n!m!}{(n+m+1)!}
$$
$$\large
B(\alpha,\beta)= \frac{Г(\alpha)Г(\beta)}{Г(\alpha + \beta)}
$$

Полностью апостериорное распределение выглядит так: 
$$\large
p(\theta|D) = 
\begin{cases}
 \frac{n!m!}{(n+m+1)!} \theta^{n}(1-\theta)^{m}, \theta \in [0, 1] \\ 
 0, \theta \notin [0, 1]

 \end{cases}

$$
Тогда максимальная апостериорная гипотеза максимизируется там же:
$$\large
\theta_{MAP} = \frac{n}{n+m}
$$

Пока всё так же, но мы знаем что D, состоящее из одного орла приходит из распределения известной формы

![[Pasted image 20220131183122.png]]


(3) Если будем бросать монетку еще раз, то узнаем чем она выпадет
$$\large
p(x|D) = \int p(x|\theta)p(\theta|D)d\theta =
$$
фиксируя x:
$$\large
p(орёл |D) = \int p(орёл|\theta)p(\theta|D)d\theta =
$$
$$\large
= \int\limits_{0}^{1} \theta \cdot \frac{(n+m+1)!}{n!m!}\theta^{n}(1-\theta)^{m} d\theta = 
$$
$$\large
= \frac{(n+m+1)!}{n!m!} \int\limits_{0}^{1} \theta^{n+1}(1-\theta)^{m} d\theta = 
$$
$$\large
= \frac{(n+m+1)!}{n!m!} \frac{(n+1)! \ \ m!}{(n+m+2)!} 
$$

Получилось правило Лапласа laplase's rule:
$$\large
p(орёл|D) = \frac{n+1}{n+m+2}
$$

Ответ получается менее уверенным 



Подводя промежуточный итог:

1) увидели теорему Байеса
2) Увидели как она используется в машинном обучении
3) Сформулировали основные задачи машинного обучения
4) Начали рассматривать пример монетки 
5) Увидели правдоподобие, максимизировали его 






























