[[Adv_Ml (Николенко)]]


Лекция 2

Их прошлой лекции:

Данные - последовательность орлов и решек:
$$\large
D=htthhh
$$
Вероятность орла:
$$\large
\theta = P('орёл')
$$

Правдоподобие:
n - число орлов
m - число решек
$$\large
P(D|h) = \theta^{n}(1-\theta)^m 
$$
Гипотеза максимального правдоподобия:
$$\large
\theta_{ML}= \frac{n}{n+m}
$$
![[Pasted image 20220131181356.png]]

Написали вот такое вот апостериорное распределение:
$$\large
p(\theta|D) \propto p(D|\theta)p(\theta) = 
 \begin{cases}
 \theta^{n}(1-\theta)^{m}, \theta \in [0, 1] \\ 
 0, \theta \notin [0, 1]

 \end{cases}
$$
$$\large
\theta_{MAP} = \frac{n}{n+m}
$$

И нам после этого нужно найти вероятность следующего орла при условии D
$$\large
p(орёл |D) = \int p(орёл|\theta)p(\theta|D)d\theta =
$$
$$\large
p(орёл|D) = \frac{n+1}{n+m+2}
$$

Теперь переходим к ответу на вопрос

А с какой стати мы брали именно такую $\large P(\theta)$  (прямоугольную равномерную)
Для монетки априорное распределение должно выглядеть колоколообразно от нуля до 1. как желтый график на картинке выше.

Интуиции о конкретной форме этого колокола у нас нет.
Поэтому с точки зрения интуиции нам всё равно как выглядит этот горб.
Возникает понятие удобных априорных распределений
Как нам будет удобно формализовать эту функцию?
Нам бы хотелось что бы она была интегрируемая, и хотелось бы что бы апостериорное распределение выглядело потом попроще, и с ним было проще работать.
1. Функция пропорциональная гаусиану(нормальному распределению с обрезанными хвостами и вытянутая, что бы интеграл был единице), но это всё равно неудобно, формула гауссианы - сложная экспонента, и в апостериорном распределении получаем экспоненту умноженную на многочлен, что тоже не очень удобно обрабатывать.
2. Удобней было бы то же использовать многочлен.  $\large P(D|\theta) = \theta^{n}(1-\theta)^{m}$ удобнее всего умножать на семейство так называемых Бета-распределений:
$$\large 
P(\theta|\alpha , \beta) = \frac{1}{\beta(\alpha , \beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}, \quad
\theta \in [0,1] 
$$
И если априорное распределение выглядит так, то получается вот такой байесовский вывод:
$$\large
P(\theta|D) \propto P(D|\theta)P(\theta|\alpha, \beta) = \theta^{n}(1-\theta)^{m} \cdot \theta^{\alpha-1}(1-\theta)^{\alpha-1} = 
$$
Где дробь 1/бета-функцию - спряталась в пропорциональность.
И это вычисление в отличие от гаусиана, не представляет больших сложностей.
$$\large
= \theta^{n+\alpha-1}(1-\theta)^{n+\beta-1}
$$
Пометка: это всё на отрезке (0, 1), на остальном - 0

А значит:
$$\large
P(\theta |D)= Beta(\theta|\alpha +n, \beta + m)
$$
и получается супер-удобная ситуация
У нас есть априорное распределение  $p(\theta)$ которое является бета распределением с параметрами $\large \alpha, \beta$ которое мы умножаем на 
правдоподобие $\large P(D|\theta)$  и на выходе у нас получается апостериорное распределение $\large P(\theta|D)$ которое тоже beta распределение, но с изменившимися параметрами, причем мы знаем как.

![[Pasted image 20220207165758.png]]

Это удобно тем, что:
1. Это легко посчитать
2. теперь, после того как мы получили эти данные и учли их в апостериорном распределении, мы можем о них просто забыть.

Если к нам придёт новый датасет $\large D'$  с какими то новыми $\large n'$ орлами и $\large m'$ решками, то мы можем посчитать новый P от \theta при условии всех данных:
$$\large 
P(\theta| D, D') \propto 
$$
Мы могли бы начать всё сначала:
$$\large
\propto P(\theta)P(D,D'|\theta)
$$
Но так же мы могли бы предположить что:
$$\large
\propto P(\theta|D)P(D'|\theta)
$$
То есть нам не нужно пересчитывать всё сначала, мы можем расценивать  апостериорное распределение $\large  P(\theta|D)$ как априорное для поступаемых новых данных, мы можем воспользоваться этим результатом как априорным, и просто добавить в него новые данные.
(в случае монетки это не очень имеет смысл, так как там всего 2 числа, но с данными покруче это должно впечатлять) 

Когда складывается такая супер удобная ситуация, в нашей науке это называется сопряженными априорными распределениями 
Conjugate priors
Это если формально - семейство сопряженных априорных распределений

$\large P(\theta|\bar{a})$ - семейство сопряженных априорных распределений для семейства правдоподобий  $\large P(D|\theta)$ если $\large P(\theta|\bar{a}) \cdot \large P(D|\theta) \propto \large P(\theta|\bar{a}')$ из того же самого семейства , для любых данных 

$\large \bar{a}$ -какой то вектор параметров

**Тут шел показ ноутбука, с тем, как меняется апостериорное распределение в зависимости от данных и априорного распределения**

Как выглядит среднее у $\large \beta$ - распределения:
Если $\large \theta^{\alpha-1}(1-\theta)^{\beta-1}$ то максимус у всего этого дела в точке $\large \frac{\alpha-1}{\alpha+\beta - 2}$ 
Это значит что априорное распределение выглядит ровно так же как правдоподобие, и про него можно думать, как о правдоподобии какого то "виртуального" априорного датасета.
То есть
Если априорное распределение $\large p(\theta) = beta(\theta|11,11)$ то это ровно то же самое, что если бы мы начали с равномерного априорного распределения $\large p(\theta) = beta(1,1)$ и добавили к нему данных, в которых 10 орлов и 10 решек $\large P(D|\theta) = \theta^{10}(1-\theta)^{10}$ 
Этот эффект в машинном обучении называется Equivalent sample size (если у нас априорное распределение из сопряженного семейства, то скорее всего про него можно думать как о равномерное неинформативное фиксированное априорное распределение умножить на правдоподобие какого то фиксированного датасета, фиксированного размера)

Проведём странный эксперимент:
возьмем параметры n и m как 1/2:
$$\large
beta\left(\theta|\frac{1}{2},\frac{1}{2}\right)\propto
\theta^{\frac{1}{2}}(1-\theta)^{\frac{1}{2}}
$$
И естественно получается крайне странный график:
![[Pasted image 20220208233931.png]]

Интуиция этого процесса такая:
Мы купили монетку в магазине фокусов. И мы подозреваем что у неё есть только одна сторона, подбросив её 1 раз(до этого на неё не смотрели) понимаем какая это сторона, и график в той области начинает стремиться к бесконечности.

Для монетки трудно представить зачем это надо, но это оказывается нужным, на моменте когда у нас появляются кубики.

---
Теперь у нас игральная кость с несколькими гранями.
$\large K$ - количество граней
$\large (\theta_{1},\theta_{2},...\theta_{K}) = \bar{\theta}$  - вероятности граней

Так же из этого следует:
$$\large
\theta_{K} = 1 - \sum\limits_{k}^{K-1}\theta_{k}
$$

так же $\large 0 \leq \theta_{k} \leq 1$ и при этом $\large \sum\limits \theta_{k} =1$ 

На примере 3х гранного кубика 

![[Pasted image 20220209015739.png]]

А дальше аналогично монете

$$\large
p(D|\bar{\theta}) = \theta_{1}^{n_{1}}\theta_{2}^{n_{2}}...\theta_{K}^{n_{K}}
$$
Семейство априорных сопряженных распределений выглядит так же и называется теперь распределением Дерехле:
$$\large
p(\bar{\theta}|\alpha) =
\frac{1}{Dir(\bar{\alpha})}
\theta_{1}^{\alpha_{1}-1}\theta_{2}^{\alpha_{2}-1}...\theta_{K}^{\alpha_{K}-1}
$$
Байесовский вывод тут точно такой же: 
$$\large
P(\theta|D) \propto P(\theta)P(D|\bar{\theta}) \propto
\theta_{1}^{\alpha_{1}+n_{1}-1}...\theta_{K}^{\alpha_{K}+n_{K}-1}
$$

Точно так же как с монеткой, расмотрим априорные распределения с какими то дробными параметрами:
$$\large
P\left(\bar{\theta}|\left(\frac{1}{10},...,\frac{1}{10}\right)\right)= \frac{1}{Dir(\bar{\alpha})} \theta_1^{-\frac{9}{10}} \theta_2^{-\frac{9}{10}}...\theta_K^{-\frac{9}{10}}
$$
Как выглядит такое распределение:
![[Pasted image 20220209142421.png]]

Eсли альфа = 1 то равномерное распределение

Eсли альфа = 10 то мы ожидаем что это честный трёхгранный кубик и он имеет большой горб в серединке

Eсли альфа = 0.1 то то распределение выглядит коряво, и это значит что скорее всего вся масса вероятности этого кубика расположена на одной грани, а если не на одной, то на 2х из 3х.
Это говорит нам о том, что вектор $\large \theta$ разреженый (то есть такие априорные распределения, реально возникают как априорные распределения, обеспечивающие разреженость).
Например мы генерируем текст наивным байесом( мы кидаем огромный кубик со словами, и из него генерируем новое слово и тогда, если мы на вероятности граней этого кубика зададим априорное распределение с какими то маленькими дробными параметрами, близкими к 0, то это будет значить что на этом кубике реально выпадает не 20000 разных слов а 50, причем заранее не известно какие)
Это оказывается полезным для тематического моделирования, где каждый такой кубик это какая то тема, и тема определяется как распределение на словах.

Что бы слово не выпадало, $\large \alpha$ должна быть равна 0. Это может привести к проблемам, поэтому не надо так делать. лучше его просто исключить 

**Краткий итог**
$\theta$ - параметры модели ( вероятность орла)
D, x - данные

* Основная задача - как обучить параметры распределения и/или предсказать следующие его точки по имеющимся данным. 

* в байесовском выводе участвуют:
	* $\large p(x|\theta)$ - правдоподобие данных.
	* $\large p(\theta)$ - априорное распределение.
	* $\large p(x)= \int_{\Theta}p(x|\theta)p(\theta)d\theta$  - маргинальное правдоподобие.
	* $\large p(\theta|x)= \frac{p(x|\theta)p(\theta)}{p(x)}$ - апостериорное распределение. 
	* $\large p(x'|x)=\int_{\Theta}p(x'|\theta)p(\theta|x)d\theta$ - предсказание нового x'.


* Задача обычно в том, что бы найти $\large p(\theta|x)$ и/или $\large p(x'|x)$ 

* Когда мы проводим байесовский вывод, у нас, кроме правдоподобия, должно быть еще априорное распределение (prior distribution) по всем возможным значениям параметров.
* Задача байесовского вывода - как посчитать $\large p(\theta|x)$ и/или $\large p(x'|x)$ 
* Но что бы это сделать, надо сначала выбрать  $\large p(\theta)$. Как выбирать априорные распределения?

Нам бы хотелось что бы оно выражало нашу интуицию, но она как правило не даёт конкретной формы распределения, а только общую идею (типа колокол с центром в 1/2) а дальше мы можем выбрать то распределение, которое нам удобно. 

Разумная цель: давайте будем выбирать распределения так, что бы после умножения на правдоподобие(a posteriori) оставались такими же.   
* До начала вывода есть априорное распределение $\large p(\theta)$. 
* После него есть какое то новое апостериорное распределение $\large p(\theta|x)$ 
* Я хочу, что бы $\large p(\theta|x)$ имело тот же вид, что и $\large p(\theta)$, просто с другими параметрами.  


Не слишком формальное определение:
Семейство распределений $\large p(\theta|\alpha)$ называется семейством сопряженных априорных распределений для семейства правдоподобий $\large p(x|\theta)$, если после умножения на правдоподобие апостериорное распределение $\large p(\theta|x,\alpha)$ остаётся в том же семействе: $\large p(\theta|x,\alpha) = p(\theta|\alpha')$ 

* $\large \alpha$ - называются гиперпараметрами, это параметры распределения параметров.
* Тривиальный пример: семейство всех распределений будет сопряженным чему угодно, но это не очень интересно.

Каким будет сопряженное априорное распределение для бросания нечестной монетки (испытаний Бернулли)?
Ответ: это бета-распределение плотность распределения нечестности монетки $\large \theta$ 
$$\large
p(\theta|\alpha,\beta)= \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)}
$$

* Тогда, если мы посэмплируем монетку, получив s орлов и f решек, получится
$$\large
p(s,f|\theta)= 
\begin{pmatrix}
s+f  \\  s
\end{pmatrix}
\theta^{S}(1-\theta)^{f}
$$
$$\large
p(s,f|\theta)= ...простыня на слайдах...= \frac{θ^{s+α−1} (1 − θ)^{f+β−1}}{B(s + α, f + β)}
$$

Итого получается, что сопряжённое априорное распределение для параметра нечестной монетки $\large θ$ – это
$$\large
p(\theta|\alpha,\beta)\propto \theta^{\alpha-1}(1-\theta)^{\beta-1}
$$

После получения новых данных с s орлами и f решками гиперпараметры меняются на
$$\large
p(θ | s + α, f + β) ∝ θ^{s+α−1} (1 − θ)^{f+β−1}
$$

На этом этапе можно забыть про сложные формулы и выводы, получилось очень простое правило обучения (под обучением теперь понимается изменение гиперпараметров).

![[Pasted image 20220210145906.png]]

Простое обобщение: рассмотрим мультиномиальное распределение с $\large n$ испытаниями, $\large k$ категориями и по $\large x_i$ экспериментов дали категорию i.

Параметры $\large θ_i$ показывают вероятность попасть в категорию i:
$$\large
p(x|\theta)= 
\begin{pmatrix}
n  \\  x_{1}, ... , x_{n}
\end{pmatrix}
\theta_{1}^{X_{1}}\theta_{2}^{X_{2}}... \theta_{k}^{X_{k}}
$$
Сопряжённым априорным распределением будет распределение Дирихле:
$$\large
p(\theta|\alpha) \propto 
\theta_{1}^{\alpha_{1}-1}\theta_{2}^{\alpha_{2}-1}... \theta_{k}^{\alpha_{k}-1}
$$

---
## Линейная регрессия 

$$\large 
p(\theta|D) \propto p(D|\theta)p(\theta) 
$$

Есть правдоподобие $p(D|\theta)$ - оно задаётся самой моделью (что это значит пока не очень понятно)
Априорное распределение $p(\theta)$ мы выбираем сами.
На выходе получается апостериорное, которое уже мы хотим оптимизировать, а может даже интегрировать по нему (усреднять, получать предсказательное распределение).

Основной пример первой части курса, на котором мы увидим как всё это работает - **линейная регрессия**.

Разберёмся где в линейной регрессии, всё то о чем мы тут разглагольствуем. 

Есть набор пар из x и y это черные крестики:
$$\large 
D={(x_{n},y_{n})}_{n}
$$

Задача состоит в том, что бы провести прямую, которая лучше всего описывает данные
$$\large
y = w_{0}+w_{1}x
$$

![[Pasted image 20220211204747.png]]

Для многомерного случая:
$$\large 
D={(\bar{x}_{n},y_{n})}_{n}^{N}
$$
Наша гипотеза в том, что y похож на линейную комбинацию этих x:
$$\large
y \sim  w_{0} +w_{1}x_{1}+...+w_{d}x_{d} 
$$
В матричном виде
$$\large
y \sim  \bar{w}^{T}\bar{x} 
$$
где к вектору x добавили в начало единичку что бы размерности с w совпадали.


Как решается линейная регрессия: 
Метод наименьших квадратов.
$$\large
L = \sum\limits_{n=1}^{N} \left ( y_{n} -  \bar{w}^{T}\bar{x}_{n}\right )^{2}
$$
Классический метод наименьших квадратов, говорит о том что L надо минимизировать.

Запишем всё в матричном виде, что бы сразу начать привыкать:

Как записать сумму квадратов в матричном виде?
Это какой вектор, скалярно умноженный на самого себя

Тоесть:
$$\large
L=
\begin{pmatrix}
y_{1} - \bar{w}^{T}\bar{x}_{1} \\
... \\
y_{N} - \bar{w}^{T}\bar{x}_{N}

\end{pmatrix}^{T}
\cdot
\begin{pmatrix}
y_{1} - \bar{w}^{T}\bar{x}_{1} \\
... \\
y_{N} - \bar{w}^{T}\bar{x}_{N}

\end{pmatrix}
=
$$

$$\large
=(\bar{y} - X\bar{w})^{T} \cdot (\bar{y} - X\bar{w})=
$$
$$\large
 X\bar{w} = 
\begin{pmatrix}
--x_{1} --\\
... \\
--x_{N}--

\end{pmatrix}
\cdot
\begin{pmatrix}
w_{1}\\
... \\
w_{N}

\end{pmatrix}
$$
X - матрица размером N x d. 
w - размерность d x 1.

Раскроем скобки:
$$\large
=\bar{y}^{T}\bar{y} - (X\bar{w})^{T}\bar{y} - \bar{y}^{T}X\bar{w} + (X\bar{w})^{T}X\bar{w}=
$$

Воспользуемся волшебным соображением, что число можно транспонировать, и оно от этого не изменится.
	$\large (X\bar{w})^{T}\bar{y}$ - число
	$\large \bar{y}^{T}X\bar{w}$ - тоже число
Но одно из них это транспонированное другое, то есть это два одинаковых числа. Тогда раскрывая все транспонирования:
$$\large
=\bar{w}^{T}X^{T}X\bar{w} -  2\bar{y}^{T}X\bar{w} + \bar{y}^{T}\bar{y} \underset{\bar{w}}{\rightarrow} \min
$$
И так как минимизируем по w то можно забить на $\large \bar{y}^{T}\bar{y}$
Получается: 
$$\large
L=
\bar{w}^{T}X^{T}X\bar{w} -  2\bar{y}^{T}X\bar{w}  \underset{\bar{w}}{\rightarrow} \min
$$
Теперь рассмотрим как это оптимизировать:
$\large \bar{w}^{T}X^{T}X\bar{w}$ - тут какая то квадратичная функция, и мы хотим найти её экстремум.
Тут как в школе, найти производную, приравнять её к нулю в той точке и будет экстремум. 
Но мы работаем с векторами, поэтому надо брать не просто производную, а градиент.
$$\large
\nabla_{\bar{w}}L =0
$$
--- 
Упражнение: 
Градиент это
$$\large
\nabla_{\bar{w}}L =
\begin{pmatrix}
\frac{dL}{dw_{1}}\\
... \\
\frac{dL}{dw_{d}}

\end{pmatrix}
$$

Градиент вектора на вектор-константу.	
Где $\large \bar{w}^{T}\bar{a} = w_{1}a_{1}+...+w_{d}a_{d}$ :
$$\large
\nabla_{\bar{w}}(\bar{w}^{T}\bar{a}) =
\begin{pmatrix}
a_{1}\\
... \\
a_{d}
\end{pmatrix}
=
\bar{a}
$$
Градиент от скалярного произведения самого на себя
$$\large
\nabla_{\bar{w}}(\bar{w}^{T}\bar{w}) =

2\bar{w}
$$

Градиент от какой то квадратичной формы:
$$\large
\nabla_{\bar{w}}(\bar{w}^{T}A\bar{w}) = ???
$$
$$\large
\bar{w}^{T}A\bar{w} = \sum\limits_{i=1}^{d} \sum\limits_{j=1}^{d} a_{ij}w_{i}w_{j}
$$
и когда мы хотим взять частную производную 
$$\large
\frac{d(\sum\limits_{i=1}^{d} \sum\limits_{j=1}^{d} a_{ij}w_{i}w_{j})}{dw_{k}} 
=
\sum\limits_{j\neq k}a_{kj}w_{j}+
\sum\limits_{i\neq k}a_{ik}w_{i} +
2 a_{kk}w_{k}
=
$$
$\large w_{k}$ тут может встретится одним из двух способов, либо как $\large w_{i}$ либо как $\large w_{j}$
$\large 2 a_{kk}w_{k}$ вот это слагаемое можем вписать по одному в каждую сумму и будет:
$$\large
=
\sum\limits_{j=1}^{d}a_{kj}w_{j}+
\sum\limits_{i=1}^{d}a_{ik}w_{i}
=
\bar{a}_{k*}^{T}\bar{w} + \bar{a}_{*k}^{T}\bar{w}
$$
В итоге получается, что очередной k-тый элемент это k-тая строчка матрицы A  умноженная на $\large \bar{w}$  + k-тый столбец матрицы А умноженный на $\large \bar{w}$.

Тогда:
$$\large
\nabla_{\bar{w}}(\bar{w}^{T}A\bar{w}) = (A+A^{T})\bar{w}
$$

В жизни это чаще всего это возникает в ситуациях когда матрица А симметрична, и тогда это просто будет 2А

Помимо всего 2ая часть лосса - линейная функция $\large 2\bar{y}^{T}X\bar{w}$ 
Распишем её как 
$$\large
2\bar{y}^{T}X\bar{w} = 2 \bar{w}^{T}(X^{T}\bar{y})
$$
и вот этот вектор $\large (X^{T}\bar{y})$ скалярно умножается на $\large \bar{w}$ 


--- 

Применим новую мудрость к тому что у нас получилось:
(не забываем что производная суммы = сумме производных)
$$\large
\nabla_{\bar{w}}L = 2X^{T}X\bar{w} - 2X^{T}\bar{y} =0
$$

$\large X^{T}X$ - симметричная матрица поэтому в ответе просто $\large 2X^{T}X$

В итоге получилась система линейных уравнений:

$$\large
X^{T}X\bar{w} = X^{T}\bar{y} 
$$
$$\large
\bar{w}^{*} = (X^{T}X)^{-1}  X^{T}\bar{y} 
$$
Это результат из стандартных курсов по МЛ
Это выражение - псевдообратный муро-пенроуза (moore-penrose pseudoinverse) и это правильный способ обращать прямоугольную матрицу

Ответ конечно хороший, но вызывает массу интересных вопросов:

1. А с какой стати мы вообще так делали (при выборе функции ошибки)
Ведь таких функций очень много, не обязательно квадратичную, можно например экспоненту, или 4тую степень. 

На самом деле у суммы квадратов есть более глубокий смысл у квадратичной функции. 
Нам нужно не просто придумать абы какую функцию ошибки, а взять функцию ошибки которая бы соответствовала нашим вероятностным предположениям. 
![[Pasted image 20220212035326.png]]

Есть какая то идеальная $\large y= \bar{w}^T\bar{x}$ которая линейна, но настоящие y не лежат на одной прямой.
Говоря в терминах правдоподобия как (x включен по умолчанию ):
$$\large
p(y|\bar{w},\bar{x})
$$
Если x заданы ( нет распределения p(x) - дескриминативная модель) откуда берутся y?
Получается что: 
$$\large
y_{n}=\bar{w}^{T}\bar{x}_{n} + \xi_{n}
$$
Где $\large \xi_{n}$ - шум, в котором заключена вероятностная составляющая происходящего.
А из какого распределения берётся $\large \xi_{n}$ 
Логично что из гауссиана c центром в нуле и какой то дисперсией:
$$\large
\xi \sim N(0, \sigma^{2}) 
$$
Это естественная мысль из за центральной предельной теоремы, если наша ошибка складывается из большого числа других ошибок то скорее всего в пределе получится гауссиана.

Когда это может быть не так: когда есть причины полагать что все ошибки распределены по другому. Например ошибки односторонние (как взвешивают товар на рынке).

![[Pasted image 20220212041718.png]]

То есть предположение о нормальности может нарушится, но если мы не знаем что оно нарушается, следует  предполагать именно его.

Попробуем его использовать и найти гипотезу максимального правдоподобия:
N- нормальное распределение
$$\large
p(D|\bar{w},X)= \prod\limits_{n=1}^{N}p(y_{n}|\bar{w},\bar{x_{n}})=
\prod\limits_{n=1}^{N}N(y_{n}|\bar{w}^{T}\bar{x}_{n}, \sigma^{2})=
$$
$$\large
=
\prod\limits_{n=1}^{N}\frac{1}{\sqrt{2\pi\sigma^2}}
e^{-\frac{1}{\sqrt{2\sigma^2}}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2}}
\underset{\bar{w}}{\rightarrow} \max

$$
В машинном обучении практически всегда мы сначала хотим взять логарифм, потому что практически всегда правдоподобие - это большое произведение маленьких правдоподобий каждой точки данных.
$$\large
\ln p(D|\bar{w},X)= \sum\limits_{n=1}^{N}
(\frac{1}{2}\ln ((2\pi\sigma^{2})-\frac{1}{2\sigma^{2}}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2})=
$$
$$\large
= const -\frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2}
\underset{\bar{w}}{\rightarrow} \max
$$
константа(так как мы оптимизируем по w а в той части его нет)
константную дробь тоже вынесли за знак суммы.
Из за минуса перед суммой, максимизация этой функции эквивалентна минимизации суммы.

И получилось что метод наименьших квадратов это как раз и есть поиск w максимального правдоподобия, в предположении того что шум распределён нормально. 

Теперь мы поняли все вероятностные предположения метода наименьших квадратов, это нужно для того что бы мы могли заметить нарушения этих предположений.

Еще одно неявное предположение, что сигма постоянна.

Разные сигмы приводят нас к тому что мы не можем вынести их за сумму, эта дробь останется внутри, это приведёт к тому, что мы просто перевзвесим квадраты отклонений при оптимизации.
![[Pasted image 20220212133638.png]]

Дисперсию можно оценить, построив отдельные модели и уже в них оценивать дисперсии, и что получилось подставлять как веса к точкам. Веса обратно пропорциональны $\large \sigma^{2}$ .

Итак, предположим что есть точки, мы знаем вокруг чего они распределены, но это что то - не прямая. (например парабола)
Как обучить коэффициенты параболы?
$$\large
y \sim w_{0}+ w_{1}x+ w_{2}x^{2}=
\begin{pmatrix}
w_{0}\\
w_{1} \\
w_{2}
\end{pmatrix}^{T}
\cdot
\begin{pmatrix}
1\\
x\\
x^{2}
\end{pmatrix}
$$
видим что у нас опять линейная комбинация w.
Дальше абсолютно ничего не меняется.
Главное что бы мы знали какие это зависимости. 


$$\large
y \sim \bar{w}^{T} \bar{\varphi}(\bar{x})
$$
Предобработка х вектор (1 x x^2)^T в формуле выше.
$$\large
\bar{x} \rightarrow \bar{\varphi}(\bar{x})
$$
![[Pasted image 20220212150129.png]]

Линейная регрессия очень глобальная модель, небольшие изменения данных немного меняют наклон в прямой, а предсказания где то далеко на этой прямой, меняются значительно. 

Это можно вылечить при помощи правильного подбора функции признаков. Что бы это сделать надо просто выбрать локальные функции признаков.
Есть вектор признаков $\large \bar{\varphi}$ 
$$\large
\bar{\varphi} =
\begin{pmatrix}
...\\
\varphi_{i}(\bar{x})\\
...
\end{pmatrix}
$$
и нужно описать как выглядит $\large \varphi_{i}(\bar{x})$ 
$$\large
\varphi_{i}({x}) = e^{-\frac{1}{2S_{i}^{2}}(x-\mu_{i})^{2}} 
$$
s и $\mu$ это выбираемые параметры, и мы их не будем обучать
Обучение с такими признаками 

$$\large
y \sim \sum\limits_{i}w_{i}\varphi_{i}(x)
$$

![[Pasted image 20220212151703.png]]

Теперь на предсказание в какой то точке x оказывают существенное влияение только те w которые близки к этому x.
И наоборот раз значение w не оказывает влияния на этот у то и этот у в обучающей выборке не будет особо влиять на этот w

То есть мы берём интересующий на отрезок от самого маленького x до самого большего, делим его на равные части и в эти равные части ставим центры гаусианов, а дальше обучаем их линейную комбинацию.

Всё отлично, но эти кривые не имеют смысла для экстраполяции. 
То есть экстраполировать локальные регрессии - явно плохая идея.
Так же как и полиномиальную. 

Что бы то что мы делаем имело смысл в экстраполяции нужно иметь не только предположение того что описывает точки, но и предположение того что описывает физическую природу процесса.

Как заметить оверфитинг и как с ним справится рассмотрим на следующей лекции. 



















