[[Adv_Ml (Николенко)]]

# Статистическая теория принятия решений

Итак у нас есть данные, и куча моделей,

$$\large
D:  M_{1},M_{2},...M_{k}
$$
И мы бы хотели понять, какая из этих моделей наиболее правдоподобна при данном датасете D:
$$\large
p(M_{i}|D)
$$
Хотим оценить насколько вероятно что данные описывает модель M1, М2, Мk итд. 

И для такой оценки используется теорема Байеса 

$$\large
p(M_{i}|D) \propto p(M_{i})P(D|M_{i})
$$
Где
$\large p(M_{i})$ - априорная вероятность на модель Mi (на сколько нам еще до всяких датасетов, верится в модель Mi). Как правило мы ни из каких соображений её взять не можем и забиваем на неё(считаем равномерной, и она не учувствует в сравнении)

$\large P(D|M_{i})$ - это вероятность породить такой датасет, в модели Mi. 

Замечание: Мi рассматриваем без каких то параметров, то есть просто "линейная регрессия", "квадратичная регрессия",  "регрессия с 5 локальными признаками" или "кластеризация на 3 кластера" "кластеризация на 5 кластеров" итд.

У нас была формула байеса:
$$\large
p(\theta|D) = \frac{p(\theta)p(D|\theta)}{p(D)}
$$
И тут:
$\large p(\theta|D)$ - апостериорное распределение
$\large p(\theta)$ - априорное распределение
$\large p(D|\theta)$ - правдоподобие для конкретных параметров

А вот знаменатель 
$\large p(D)$ -  evidence - усредненная вероятность получить данные D из такой регрессии. 
Формально это можно дописать так:
$$\large
p(\theta|D, M_{i}) = \frac{p(\theta|M_{i})p(D|\theta, M_{i})}{p(D|M_{i})}
$$
Этот самый знаменатель, это и есть то что нам нужно. То есть мы хотим сравнивать разные модели по p(D) 
$$\large
P(D|M_{i}) = \int p(\theta|M_{i})p(D|\theta, M_{i}) d\theta
$$
Такой интеграл возьмется мягко скажем нечасто


Начнем с идиотской неприменимой на практике аппроксимации, но она даст понимание что происходит:

![[Pasted image 20220218212137.png]]

По оси x - $\large \theta$ , и на этой оси тета где то есть тета максимальная апостериорная $\large \theta_{MAP}$ И мы предполагаем супер упрощенные предположения:
Априорное распределение $\large p(\theta)$ равномерное (фиолетовая линия) У него есть ширина $\large \Delta \theta_{prior}$  и высота $\large \frac{1}{\Delta \theta_{prior}}$ 
Если априорное распределение равномерное, то это значит что 
$$\large \theta_{MAP} = \theta_{ML}$$
И еще более идиотское предположение что апостериорное распределение(синяя линия) тоже равномерное.
Его ширина  $\large \Delta \theta_{post}$ и высота $\large \frac{1}{\Delta \theta_{post}}$ 

В таких предположениях попробуем посчитать $\large P(D|M_{i})$ 

Априорное распределение вне своей ширины равно нулю, следовательно интеграл у нас по отрезку. А внутри отрезка оно постоянно и равно $\large \frac{1}{\Delta \theta_{prior}}$ 

$\large p(D|\theta)$  - правдоподобие, в этих дурацких определениях если апостериорное распределение равномерное и априорное было равномерное, то и  правдоподобие равномерное.  И значит что это константа на синем отрезке, других вариантов нет( на рисунке черная линия) высота её неизвестна
$$\large
p(D)= \int p(\theta)p(D|\theta)d\theta = 
\int\limits_{\Delta \theta_{prior}} \frac{1}{\Delta \theta_{prior}} \cdot p(D|\theta)d\theta =
$$
$$\large
=
\int\limits_{\Delta \theta_{post}} \frac{1}{\Delta \theta_{prior}} \cdot p(D|\theta_{ML})d\theta =
p(D|\theta_{ML}) \cdot \frac{\Delta \theta_{post}}{\Delta \theta_{prior}}
$$
Что на самом деле говорит эта формула: 
1. Хорошо когда датасет имеет высокое правдоподобие в данной модели.
2. Еще было бы неплохо что бы апостериорное  распределение было бы по шире, а априорное поуже. (что бы апостериорное распределение не слишком оверфитилось в сравнении с априорным)

Если это переписать по человечески то:
$$\large
\ln p(D) = \ln p(D|\theta_{ML}) - \ln 
\frac{\Delta \theta_{prior}}{\Delta \theta_{post}}
$$
То есть этот критерий говорит что когда мы сравниваем две модели мы можем сравнивать логарифм максимального правдоподобия, но со штрафом, который показывает насколько острым оказалось апостериорное распределение в сравнении с априорным в этой модели.
Т.е Если модель может описать всё что угодно(у неё широкое априорное распределение и там всё что угодно возможно) а потом она обучилась на этом датасете и внезапно её апостериорное распределение стало слишком узким, то это как то подозрительно.

Этот результат говорит уже всю суть, но пока не применим на практике. 

Если сделать чуть менее идиотское предположение: 

Точно такое же рассуждение, надо взять точно такой же интеграл, но в нем мы предпологаем, что приближаем априорное и апостериорное распределение какими то гауссианами:
$$\large 
p(\theta), p(\theta|D) \approx N(...)
$$
Дальше идут магические вычисления, которые мы делать не будет, и сразу приведём результат.
Результат называется 
BIC - bayesian information criterion
$$\large
\ln p(D) \approx \ln p(D|\theta_{ML})- \frac{1}{2} M \ln N
$$
где:
M - число параметров
N - число точек в датасете D

Тут уже обычно нет проблем с тем, что бы применить эту штуку на практике. 
Еще одна полезная картинка на этот счет

![[Pasted image 20220219123242.png]]

По оси X лежат датасеты

$P(D|M_{1})$ -(синяя) узкая избирательная модель, мало что описывает, но описывает это хорошо. 
$P(D|M_{2})$ - (красная) модель может описать побольше датасетов, но p(D) в них будет поменьше 
$P(D|M_{3})$ - (зеленая) модель которая может описать всё что угодно, но при этом и p(D) очень невысокая. 

Например  M1 - линейная регрессия, она описывает хорошо только датасеты, которые лежат близко к прямой, но  таких датасетов мало.
М2 - допустим это квадратичная регрессия, она описывает больше разных датасетов (больше точек ложатся рядом с параболой, нежели с прямой(прямая - частный случай параболы))  
M3 - многочлен 5той  

И когда мы смотрим на свой единственный датасет D (черный), то получается что:
М1 -  недостаточно выразительна и плохо описывает данные, а М2 и М3 обе могут хорошо описать данные. Но поскольку М2 проще, то р(D) её будет выше и мы её предпочтем.

Еще одно важное понятие: дивергенция кульбака лейблера
## Kullback-Leibler divergence
$$\large
KL(p||q) = \int p(\bar{x}) \ln \frac{p(\bar{x})}{q(\bar{x})}d\bar{x}
$$
Эта штука даёт хорошую меру похожести двух распределений 

* $\large KL(p||q) > 0$
* $\large KL(p||q) = 0 \Leftrightarrow p = q$ (почти всюду совпадают)

Она так же несимметрична 
$$\large
KL(q||p) = \int q(\bar{x}) \ln \frac{q(\bar{x})}{p(\bar{x})}d\bar{x}
$$
![[Pasted image 20220219145002.png]]

по оси x - x
по этому х у нас есть р(х) которая выглядит как смесь 2х гауссиан (серая линия) 
Задаёмся вопросом, мы хотим смесь этих 2х гауссиан приблизить одним гауссианом. Какая лучшая аппроксимация?
И тут возникает интересный эффект.

Если минимизируем $\large KL(p||q) \rightarrow \min$ . Видим что в формуле q в знаменателе и самой плохой ситуацией будет когда р(х) большая а q(x) маленькая.
То есть если мы будем рисовать q так что бы минимизировать p||q 
то рисуем гауссиан покрывающий оба эти горба(красная линия)

Если минимизируем $\large KL(q||p) \rightarrow \min$ . то ситуация ровно противоположная(фиолетовая линия). то функция q выберет один из пиков(который побольше по весу) и аппроксимирует его своим собственным гауссианом.

В машинном обучении чаще минимизируем $\large KL(p||q)$ хотя в разном контексте встречаются оба варианта. 
Предположим что q это то что мы ищем(то у чего есть параметры которые мы оптимизируем , а p - это то что нам дано) 
Предположим что у нас есть какие то точки данных (на графике черные полоски) которые были порождены из р(х) 
тогда распределение данных это распределение дискретное, которое есть в точках данных.
$$\large
\int P_{data}(x)f(x)dx = \sum\limits_{x\in D}f(x)
$$
Тогда поиск гипотезы максимального правдоподобия это когда мы максимизируем логарифм 
$$\large 
\ln p(D|\theta) \rightarrow \max 
$$
то есть максимизируем  сумму логарифмов правдоподобия датасета
$$\large
\sum\limits_{x\in D} \ln p(x|\theta) = \int p_{data}(x)\ln p(x|\theta)dx \rightarrow \max
$$
Максимизировать это, это то же самое что и это:
$$\large
KL(p_{data}||p) = \int p_{data}(x)\ln \frac{p_{data}(x)}{p(x|\theta)}dx \underset{\theta}{\rightarrow} \min 
$$
Эта штука появляется обычно где x это данные

А контекст в котором появляется $\large KL(q||p)$ такой:
Эта штука появляется когда х это параметры.
$$\large
p(\theta) = p(\theta|D) \propto p(\theta) p(D|\theta) 
$$
$\large p(\theta)$ - Сложное распределение которое мы хотим аппроксимировать
$\large p(\theta|D)$ - апостериорное распределение какой то очень сложной модели. 
![[Pasted image 20220219174857.png]]

Внимание: по оси x у нас тета. 
То есть эта штука возникает, когда мы хотим аппроксимировать вы пространстве параметров,  вот эту волнообразную хуергу, мы хотим аппроксимировать более простым распределением. 
И если мы будем минимизировать (p||q) ( накладывать длинную шапку) то оно будет абсолютно бесполезно, а вот если минимизировать (q||p) то мы накроем 1-2 пика и будет норм.

Смысл всего вышесказанного:
KL дивергенция - это хорошая мера похожести 2х распределений 

Можно попытаться сделать симметричную версию KL. Это называется дивергенция Йенсена-Шенона.
$$\large
JSD(p||q) =KL\left(p||\frac{p+q}{2}\right)+
KL\left(q||\frac{p+q}{2}\right)
$$

Давайте попробуем проверить что методика сравнения моделей методом  p(D) вообще корректна.

Sanity check(Санитарная проверка (просто проверяем что всё работает)):

Предположим что данные порождены действительно настоящей моделью М_true (запустили питон, нагенерировали себе точек и точно знаем что модель есть правильная)
Тогда было бы неплохо что бы это P(D) было бы больше чем у любой другой модели на этих данных:

$$\large
p(D|M_{true}) \Rightarrow
p(D|M_{true}) \geqslant p(D|M)
$$
или в другом виде 

$$\large
\ln p(D|M_{true}) \geqslant \ln p(D|M)
$$
Замечание: 
Для конкретного датасета это может быть не так, точки честно порождены вокруг параболы, а апроксимирует лучше прямая.
То есть датасет порождённый из сложной модели может оказаться похожим на датасет порожденный из  простой модели. 
![[Pasted image 20220219180920.png]]
Но было бы неплохо что бы это было в среднем верно.

Тогда давайте усредним:

$$\large
\int p(D|M_{true}) \ln p(D|M_{true})dD - 
\int p(D|M_{true}) \ln p(D|M)dD \geq 0
$$
$$\large
KL(P_{true}||p)
\int p(D|M_{true}) \ln \frac{p(D|M_{true})}{p(D|M)}dD - 
\geq 0
$$


Используется это в основном в обучении без учителя, по типу кластеризации (можем разбить на 3 кластера, а можем на 4)


Еще 2 замечания на примере линейной регрессии
превью ядерных методов в контексте SVM
## Equivalent kernel
Предсказания в линейной регрессии: 
$$\large
p(y|\bar{x},D) = N(y|\bar{w}_{MAP}^{T} \bar{x},\sigma^{2}) \quad , \ \sigma^{2} = \sigma_{0}^{2}+ \bar{x}^{T}\Sigma_{N}\bar{x}
$$

$$\large
\bar{w}_{MAP} = \frac{1}{\sigma_{0}^{2}}\Sigma_{N} \bar{X}^T\bar{y}
$$
$$\large
\bar{x}^{T}\bar{w}_{MAP} = \frac{1}{\sigma_{0}^{2}}\bar{x}^{T}\Sigma_{N} (\bar{X}^{T}\bar{y}) =
\sum\limits_{n=1}^{N}
\frac{1}{\sigma_{0}^{2}} \bar{x}^{T}\Sigma_{N} \bar{x}_{n} y_{n}
$$

![[Pasted image 20220219183531.png]]

$$\large
\bar{x}^{T}\bar{w}_{MAP} = 
\sum\limits_{n=1}^{N}
k(\bar{x}, \bar{x}_{n})
y_{n}
$$
к(x, x) - эквивалентное ядро 
Получается что предсказание в линейной регрессии представляет собой линейную комбинацию значений в точках и значений y_n

Внимание на картинку
![[Pasted image 20220219184612.png]]

У нас есть точки , мы по ним построили розовую прямую, предсказали на прямой точку $\bar{x}$ и если мы возьмем точечку справа и начнем двигать её вверх и вниз, то наша точка $\bar{x}$ так же будет двигаться линейно вверх и вниз. 
И это называется эквивалентное ядро

## Empirical Bayes (эмпирический байес)
Опять на примере линейной регрессии
удобней в одномерных гауссианах рассматривать не дисперсию а точность,( она просто выражается через дисперсию)
$$\large
\beta = \frac{1}{\sigma^{2}}
$$
В линейной регрессии есть 
$$\large
p(D|\bar{w}) = \prod\limits_{n}N\left(y_{n}| \bar{w}^{T} \bar{x}_{n}, \frac{1}{\beta}\right)
$$

И есть априорное распределение
$$\large
p(\bar{w}) = \prod\limits_{i}N\left(w_{i}| 0, \frac{1}{\alpha}\right)
$$
$\large \alpha$ и $\large \beta$ - гиперпараметры. их мы выбираем прежде чем обучать линейную регрессию. 
А откуда их брать?
Эмпирический байес - самый прямолинейный ответ на этот вопрос.
Мы хотим понять какие $\large \alpha$ и $\large \beta$  лучше всего подходят к нашему датасету.
$$\large
p(\alpha, \beta|D) \propto p(D|\alpha, \beta)p(\alpha, \beta)
$$
Предположим $\large p(\alpha, \beta)$ равномерным, тогда его не надо учитывать.
и в машинном обучении на это часто забивают, и это называется non-informative priors
Тогда наша задача максимизировать вот эту штуку:
$$\large
p(D|\alpha, \beta) \underset{\alpha, \beta}{\rightarrow} \max
$$
И это вполне можно сделать:
$$\large
p(D|\alpha, \beta) = \int p(D|\bar{w}, \beta)p(\bar{w}, \alpha)d\bar{w}
$$
и ответ получается такой:
$$\large
\ln p(D|\alpha, \beta) = \frac{d}{2} \ln \alpha +
\frac{N}{2}\ln \beta - 
\frac{1}{2}\ln\det A - 
\frac{N}{2}\ln 2\pi - 
$$
$$\large

 - \frac{\beta}{2} \sum\limits_{n}(y_{n}-\bar{\mu}_{n}^{T}\bar{x}_{n})^{2} -
 \frac{\alpha}{2}\bar{\mu}_{n}^{T}\bar{\mu}_{n}
$$
Где $\large \bar{\mu}_{n}$ - максимальная апостериорная гипотеза
$$\large
A = \beta X^{T}X + \alpha I
$$
Таким образом мы можем найти самые хорошие гиперпараметры.
эмпирический байес - метод поиска гиперпараметров который максимизирует p(D).


## Nearest neighbors (метод ближайших соседей) 
в плоскости,
![[Pasted image 20220219215202.png]]

То же но в регрессии (смотрим на ближайших соседей по x и выдаём среднее)
![[Pasted image 20220219215316.png]]

Гиперпараметр всего 1 - это число соседей k


**Curse of dimensionality - проклятие размерности**

В размерности 1-2-3 МЛ это очень простая наука, в этой размерности всё можно посчитать. 

Например на отрезке 0-1 есть какая-то функция, и мы хотим посчитать на ней интеграл.
![[Pasted image 20220219224025.png]]
И для этого просто достаточно поделить интервал на отрезки длины epsilon вычислить значения в этих точках а потом пользуясь методом приближенного численного расчета интеграла находим сам интеграл - изи-бризи.
Таких точен на таком интервале нужно 1/epsilon

А если перейдём к размерности 2: 
![[Pasted image 20220219224305.png]]
То сложность сразу возрастает до 1/epsilon^2

и это беда, потому что степени 1-2-3 мы еще можем выдержать а 100+ уже нет.

**2ое замечание - кожура апельсина. **

Есть шар-апельсин единичного радиуса и у него есть кожура толщиной $\large \varepsilon$ . 

В однмерном пространстве:
это отрезок от -1 до 1
![[Pasted image 20220219230105.png]]
В этом случае кожура размерности 1 занимает такую долю:
$\large \varepsilon$ (так как длина отрезка получилась 2 и с двух сторон по $\large \varepsilon$  получается 2$\large \varepsilon$ / 2)


В размерности 2:
![[Pasted image 20220219230450.png]]
В таком случае кожура занимает следующую долю:
$\large 1-(1-\varepsilon)^{2}$

В размерности 3 уже 
![[Pasted image 20220219231607.png]]
$\large 1-(1-\varepsilon)^{3}$

И дальше эта штука улетает в степенной зависимости к единице

И в размерности 10-20 покупать апельсины нет никакого смысла, там будет одна кожура.

Собственно в чем печаль: 
У нас есть масса методов, которые хороши для интерполяции но плохи для экстраполяции.
А другие методы, например knn опираются на значения функции в близких точках, и для них желательно что бы эти близкие точки были близко. 
![[Pasted image 20220219232201.png]]
А этот эффект говорит нам о том что в высокой размерности (например схематично шар) все точки лежат на границе этого шара в тоненьком колечке, и новая точка о которой нас спросят, тоже лежит на границе этого шара и это плохо, потому что получается что ближайшие соседи перестают быть ближайшими(расстояние между двумя случайно взятыми точками растёт)
А что еще противнее все точки оказываются на краю (как в примере с регрессией то же что и оранжевая точка)

Условно: если у нас есть 500 признаков и мы берём новую точку, то она с большой вероятностью по нескольким из этих 500 признаков будет на краю датасета. и если они важны, то тогда всё будет плохо.

И в результате это проклятие размерности не позволяет нам обобщить многие методы МЛ. в частности метод knn

А вот линейной регрессии например наплевать. 
Если размерность не настолько велика, а данных  гораздо больше чем размерность, то эти проблемы регрессию не касаются.

Этот эффект кожуры апельсина можно увидеть не только для равномерного распределения но и для какого угодно.

Например семплируем х из обычного гауссиана с центром в нуле и единичной матрицей ковариаций. 
И какое тогда будет распределение у радиусов данных семплов х(расстояние до нуля):
![[Pasted image 20220219233359.png]]

В размерности 1(серая линия) всё отлично, просто вдвое вытянутая половина гауссиана

В размерности 2(оранжевая линия):
расстояние от точки до нуля это сумма квадратов расстояний по каждой из размерностей,  и каждая координата накинута из нормального распределения. 
и получается что график начинает уползать.

В размерности 3 и выше (красная линия):
по центральной предельной теореме, получается мы складываем много случайных величин, мы будем стремиться ко все более и более узкому и хорошо определённому гауссиану.

Красивенький график 
![[Pasted image 20220219234222.png]]

## Statistical decision theory (статистическая теория принятия решений)

Сначала начнем с регрессии 
$\large x \in \mathbb{R}^{d}$   -есть какой то х размерности d
$\large y \in \mathbb{R}$  - просто число.

$\large D = \{(\bar{x}_{n}, y_{n})\}$ - датасет из х энных и у энных который порождён по какому то совместному распределению $\large p(\bar{x}, y)$ 
$\large p(\bar{x}, y)$  - это распределение из которого беруться и х и у. 

(Следующая строчка опциональна для пояснений)
на самом деле наша задача научится предсказывать:
$\large p(y|\bar{x}) \propto p(\bar{x}, y)$  

Что мы хотим найти:
$\large f(\bar{x})$ -  ф которая берет на вход х и предсказывает у $\large f: \bar{x} \mapsto y$
И эта функция должна лучше всего предсказывать у.
Что значит лучше всего:
повторим наши рассуждения о ЦПТ и возьмем квадратичную ошибку
$\large L(\bar{x}) = (f(\bar{x}) - y)^{2}$  - ошибка от одной точки

И тогда что нам нужно минимизировать 

Мы хотим минимизировать ожидаемую ошибку предсказания (expected prediction error) хотим минимизировать ожидание Е по  $\large p(\bar{x}, y)$ от квадратичной ошибки


$$\large
EPE[f]= \mathbb{E}_{p(\bar{x}, y)} \left [ (f(\bar{x}) - y)^{2} \right ] = \int p(\bar{x}, y)(f(\bar{x}) - y)^{2}d\bar{x}dy=
$$
Cлишком много стрелочек, проще картинкой:

![[Pasted image 20220220000822.png]]
Оказывается можно минимизировать поточечно внутренний интеграл. 
Перепишем в другой форме:
$$\large
\int (z-a)^{2}p(z)dz \underset{a}{\rightarrow} \min
$$
это минимизируется для среднего: $\large a = \mathbb{E}[z]$ 

Так же и тут, получилось что самая лучшая функция f 
$$\large
\hat{f}(\bar{x})= \mathbb{E}_{p(y|\bar{x})}[y]
$$
Это называется функция регрессии и это самое лучшее что мы можем сделать для задачи регрессии.

И теперь стали более понятны предпосылки knn:
$$\large
\hat{f}(\bar{x})= \mathbb{E}_{p(y|\bar{x})}[y] \approx \frac{1}{R}\sum\limits_{r=1}^{R}y_{r} \approx 
\frac{1}{R}\sum\limits_{r=1}^{R}y_{r}^{'} 
$$
$\large y_{r}^{'}$ - k-nn по $\large \bar{x}$ 

То есть y в точках которые являются соседями, похоже являются хорошей аппроксимацией для нужной нам точки.



**Ровно то же самое можно сделать для задачи классификации**

Функция потерь L теперь такая:
y - правильный ответ.
y' - предсказание.

![[Pasted image 20220220004925.png]]
И это оптимальный байесовский классификатор. 

И если мы всё знаем о наших распределениях, то так и нужно делать, но в жизни такое встретишь нечасто.

Вернемся снова к регрессии: 

![[Pasted image 20220220010623.png]]
Делаем такой трюк: вычитаем и прибавляем средний игрек по чему ни будь $\large \hat{f}$  и раскладываем эту разность квадратов.

Зачем мы это расписывали:
что бы что ни будь занулить. (зануляестя интеграл с вынесенной двойкой)

Получается такой результат:
$$\large
EPE[f]= \mathbb{E}_{p(\bar{x}, y)} \left [ (f(\bar{x}) - \hat{f}(\bar{x})^{2} \right ] +

\mathbb{E}_{p(\bar{x}, y)} \left [ (\hat{f}(\bar{x}) - y)^{2} \right ]
$$

![[Pasted image 20220220011322.png]]

Что у нас получилось.
1. Из этих 2х слагаемых $\large f(\bar{x})$ встречается только в первом, а во втором нет, то есть 2ое слагаемое это средняя дисперсия игрека вокруг своего же матожидания  - это ничто иное как **шум**

На примере регрессии, даже если мы точно знаем правильную прямую, то всё равно у предсказаной точки не будет нулевая ожидаемая ошибка, потому что у из данных будет с каким то гауссовским шумом. Мы - машинные облучатели эту ошибку не контролируем никак. 
А первая часть говорит нам что мы и так знаем, что надо взять ф похожим на ф с крышечкой. 

2.  дело в том что  ф от х это не просто ф от х , а ф от х обученное на каком то датасете D.
$$\large
f(\bar{x}) = f(\bar{x};D)
$$

пример: 
есть линейная регрессия. серая прямая - правильный ответ. 
и что может получится:
если есть только 3 красных х то с вероятностью 1/8 они все лежат ниже красной прямой. и тогда в ответ выйдет синяя линия
а с вероятностью другая 1/8 будет наоборот. 
![[Pasted image 20220220012326.png]]

$$\large
D \sim p(\bar{x},y)
$$
Следующее рассуждение такое: 
Проделываем тот же самый трюк (вычитаем и прибавляем ожидание ф в точке х по датасетам на которых она обучалась)

![[Pasted image 20220220012749.png]]

И вот получается такой результат:
![[Pasted image 20220220013005.png]]

Ожидаемая ошибка предсказания складывается из 

variance - дисперсия - то на сколько значение конкретной модели обученной на конкретном датасете будет отличатся от усредненной значения этих моделей обученных  на других датасетах.

bias - смещение то на сколько это ожидание по датасетам отличается от того что надо.

noise - шум

то есть разложение на смещение, дисперсию и шум.

