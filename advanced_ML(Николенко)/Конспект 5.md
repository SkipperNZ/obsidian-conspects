[[Adv_Ml (Николенко)]]



# Лекция 5

## Введение в классификацию

Надо различить два класса 
Разберём линейные методы
Задача: для каждой новой точечки, научится определять красная она или синяя.


![[Pasted image 20220304051543.png]]

Уравнение гиперплоскости:
$\large w_{0}$ - расстояние от начала координат до гиперплоскости.
$$\large
y = \bar{w}^{T}\bar{x}+w_{0} =0
$$

Расстояние от оранжевой точки до гиперплоскости: 
$$\large
d(\bar{x}, \bar{w}^{T}\bar{x}+w_{0}) = \frac{|y(\bar{x})|}{||\bar{w}||}= 
\frac{|\bar{w}^{T}\bar{x}+w_{0}|}{||\bar{w}||}
$$

Понять принадлежит ли новая точка к красным или синим, можно по знаку расстояния до плоскости.

что бы не складывать байес в векторе w отдельно можно добавить к x единичку. 
$$\large
\bar{x} \rightarrow 
\begin{pmatrix} 
 \bar{x}\\ 1
\end{pmatrix}
$$
и просто приравнять произведение к нулю
$$\large
\bar{w}^{T}\bar{x} = 0
$$
Допустим мы как то научились проводить эту гиперплоскость. 

Как провести (несколько гиперплоскостей) разделяющих несколько классов. 
Можем провести такие разноцветные линии, в целом понятно что делать в заштрихованных областях, а в треугольничке по середине - неочевидно.

Можно поставить ограничение что бы эти линии в точке - пересекались.

![[Pasted image 20220306204347.png]]

хорошо бы, что бы наша картинка выглядела как то так: 
![[Pasted image 20220306205227.png]]

Как искать эти прямые? что бы они пересекались в одной точке.

В линейной классификации мы ищем вот такое уравнение разделяющей поверхности:
$$\large
y = \bar{w}^{T}\bar{x}+w_{0} =0
$$
А в нелинейной удобно и правильно искать не уравнение разделяющей поверхности, а какие то функции (иногда их называют дискриминантами, иногда scorce) которые оценивают принадлежность к тому или иному классу (не вероятность):  

$$\large
y_{1}(\bar{x}) =
\bar{w}^{T}_{1}\bar{x}+w_{01}
$$
$$\large
y_{2}(\bar{x}) =
\bar{w}^{T}_{2}\bar{x}+w_{02}
$$
$$\large
y_{3}(\bar{x}) =
\bar{w}^{T}_{3}\bar{x}+w_{03}
$$
 и тогда, если мы ищем не уравнение поверхностей, а вот такие вот функции дискриминанты, и потом говорим что разделяющие поверхности получаются сами собой. и разделяющие прямые(черные) это те места где y1=y2, y1=y3, y2=y3

Это называется линейное тропическое многообразие. 
tropical geometry

То есть классификатор теперь работает как выбор максимума из y1 y2 y3 
$$\large
argmax\{ y_{1}, y_{2}, y_{3} \}
$$
а вот это черная "звезда" это место где $\large \max \{ y_{1},y_{2},y_{3}\}$ достигается более чем в 1 точке.

Как только классов становится больше двух сразу становится удобней искать функции дискриминантов


**То что мы не будем использовать на практике, но то что хорошо илюстрирует, то что происходи**

Линейный дискриминатор Фишера (fishers linear discriminant)


![[Pasted image 20220307062523.png]]

(левая картинка)
Есть синенькие и красненькие точки, как провести между ними гиперплоскость.

1. найдём центры масс синих и красных точек, и проведём между ними срединный перпендикуляр.

Но всё ломается если точки становятся как на 2ой картинке справа.

Фишер задаётся вопросом: что же плохого в том что бы восстанавливать серединный перпендикуляр. 
(как понять что направление соединяющее центры m1 и m2 плохое)

Суть задачи состоит в том, что бы найти направление w (линия под картинкой) и решить её в размерности 1.  
И линейный дискриминант фишера это попытка ответа на вопрос: 
а что это за направление, и чего мы от него хотим.

Первое свойство состоит в том что бы классы на нем были подальше, что бы проекция красных точек  были бы подальше от проекции синих точек. 

Вся идея линейного дискриминанта Фишера в следующем: 

На самом деле у нас есть 2 задачи (2 свойства которые мы бы хотели видеть в векторе w)

1. Что бы m1 было подальше от m2

$$\large
|\bar{w}^{T}\bar{m}_{2} - \bar{w}^{T}\bar{m}_{1}|
\underset{\bar{w}}{\rightarrow} 
\max
$$
Но это не единственное свойство которое мы хотим.

2. Мы бы хотели, что бы дисперсии в проекциях были бы поменьше. 

$$\large
S_{1} + S_{2} 
\underset{\bar{w}}{\rightarrow} 
\min
$$
Где:
C_1 - точки из первого класса.
m1, m2 - среднее
С_2 - точки из 2ого класса.
$$\large
S_{1} = \sum\limits_{\bar{x} \in C_{1}}
(\bar{w}^{T}\bar{x} - \bar{w}^{T}\bar{m}_{1})^{2}
$$
$$\large
S_{2} = \sum\limits_{\bar{x} \in C_{2}}
(\bar{w}^{T}\bar{x} - \bar{w}^{T}\bar{m}_{2})^{2}
$$

То есть надо в одно и то же время оптимизировать 2 функции.
Одну максимизировать, а другую минимизировать
В этом случае надо одно делить на другое:

Приведём эти условия к единому виду.

1.  Снимем модуль, взяв норму вектора в квадрате
$$\large
||\bar{w}^{T}(\bar{m}_{2} - \bar{m}_{1})||^{2} =
\bar{w}^{T}(\bar{m}_{2} - \bar{m}_{1}) \cdot
(\bar{m}_{2} - \bar{m}_{1})^{T} \bar{w} =
$$
	$\large (\bar{m}_{2} - \bar{m}_{1}) \cdot (\bar{m}_{2} - \bar{m}_{1})^{T}$ - это $\large S_{B}$ - межклассовая ковариация (between-class covariance) 
	тогда:
	$$\large
	\bar{w}^{T} \cdot S_{B} \cdot \bar{w} 
	\underset{\bar{w}}{\rightarrow} 
	\max
	$$

2.  так же расккроем и 2ое условие.
$$\large
\sum\limits_{\bar{x} \in C_{1}}
\bar{w}^{T}(\bar{x} - \bar{m}_{1}) \cdot
(\bar{x} - \bar{m}_{1})^{T} \bar{w} +

\sum\limits_{\bar{x} \in C_{2}}
\bar{w}^{T}(\bar{x} - \bar{m}_{2}) \cdot
(\bar{x} - \bar{m}_{2})^{T} \bar{w} =

$$
$$\large
=
\bar{w}^{T} \left(
\sum\limits_{C_{1}}
(\bar{x} - \bar{m}_{1}) \cdot
(\bar{x} - \bar{m}_{1})^{T} +

\sum\limits_{C_{2}}
(\bar{x} - \bar{m}_{2}) \cdot
(\bar{x} - \bar{m}_{2})^{T}

\right ) \bar{w} 
$$

Выражение в больших скобках - $\large S_{W}$ - внутриклассовая ковариация (within class covariance)

Получилась вот такая вот функция оптимизации, если одно поделить на другое:
$$\large
J(\bar{w})=
\frac{\bar{w}^{T} \cdot S_{B} \cdot \bar{w}}
{\bar{w}^{T} \cdot S_{W} \cdot \bar{w}}
	\underset{\bar{w}}{\rightarrow} 
	\max
$$
Это и есть дискриминант Фишера.

Как собственно его оптимизировать: 
будем брать градиент и приравнивать к нулю 

$$\large
\nabla_{\bar{w}}J = 
\frac{2 S_{B}\bar{w} \cdot (\bar{w}^{T} \cdot S_{W} \cdot \bar{w}) - 2 S_{W}\bar{w} \cdot (\bar{w}^{T} \cdot S_{B} \cdot \bar{w}}
{........} = 0 
$$
$$\large
S_{B}\bar{w} \cdot (\bar{w}^{T} \cdot S_{W} \cdot \bar{w}) =
S_{W}\bar{w} \cdot (\bar{w}^{T} \cdot S_{B} \cdot \bar{w})
$$

Теперь функции кубические((((
Но можно заметить пару интересных вещей 
Мы всё это ищем для того что бы найти направление вектора w на который у нас всё проецируется. а сам вектор нам не нужен

B тогда получается такая вот задача коллинеарности:

Ищем такой $\large \bar{w}$ такой что 
$$\large S_{B}\bar{w} \propto S_{W}\bar{w}$$
Уже проще но всё равно трудно. 

Заметим еще что матрица $\large S_{B}$ (если её раскрыть) пропорциональна $\large \bar{m}_{2} - \bar{m}_{1}$ 

И тогда

$$\large
\bar{w} \propto   S_{W}^{-1} (\bar{m}_{2} - \bar{m}_{1})

$$


## Порождающие модели для классификации 
Generative models for classification

![[Pasted image 20220308192048.png]]

Предположим, что у нас есть какая то модель, для того как устроены точки в тех или иных классах.
Красные точки - какой то гауссиан, и синенькие точки - другой гауссиан. 

Красные
$$\large
P_{1}(\bar{x}) = p(\bar{x}|C_{1})
$$
Синие
$$\large
P_{2}(\bar{x}) = p(\bar{x}|C_{2})
$$
Как теперь решить задачу классификации зная эти p(x)

Для новой точки определяем какой класс более вероятен
Используем теорему байеса:
$$\large
p(C_{1}| \bar{x}) = 
\frac{p(C_{1})p(\bar{x}|C_{1})}
{p(C_{1})p(\bar{x}|C_{1})+p(C_{2})p(\bar{x}|C_{2})}
$$
$\large p(C_{1})$ и $\large p(C_{2})$ - априорные вероятности классов (какой класс больше сколько точек в каком классе)

а остальное у нас дано по условию выше. 

Это и есть optimal bayesian classifier (в общем виде): 
$$\large
p(C_{k}| \bar{x}) = 
\frac{p(C_{k})p(\bar{x}|C_{k})}
{\sum\limits_{l}p(C_{l})p(\bar{x}|C_{l})}
$$
Разделяющая поверхность в этом случае - это где выполняются эти равенства:
$$\large
p(C_{1})p(\bar{x}|C_{1}) = p(C_{2})p(\bar{x}|C_{2})
$$

В качестве частного случая разберём:

Красные точечки - один гауссиан
Синие точечки - другой гауссиан
![[Pasted image 20220309052600.png]]

Давайте посмотрим какая тогда получается разделяющая поверхность.
$$\large
p(C_{1}|\bar{x}) = p(C_{2}|\bar{x}) 
$$
$$\large
p(C_{1})p(\bar{x}|C_{1}) = p(C_{2})p(\bar{x}|C_{2})
$$
Пролагорифмировав получаем: 
$$\large
\ln p(C_{1})+\ln p(\bar{x}|C_{1}) = \ln p(C_{2})+ \ln p(\bar{x}|C_{2})
$$
Дальше снова ебельдь:
![[Pasted image 20220309052938.png]]
![[Pasted image 20220309052950.png]]
Всё что в самой нижней строке - константа.

Выходит какая то квадратичная разделяющая поверхность 

QDA quadric discriminant analisys
![[Pasted image 20220309055713.png]]

Если сигмы совпадают то получается гиперплоскость 
Заметим также что априорные вероятности не могут изменить форму поверхности, а могут только сдвинуть её на константу.

LDA linear discriminant analisys
![[Pasted image 20220309055917.png]]

Ещё нехватает откуда взять мю и сигму
В случае QDA всё просто, это обучение 2х независимых гауссиан.

![[Pasted image 20220309060537.png]]

В случае LDA получается интересней

Там мы предполагаем что матрицы ковариаций одинаковы, тогда другая задача 
Нужно оптимизировать два гауссиана сразу с единой матрицей ковариаций. 
![[Pasted image 20220309060851.png]]

Если красненькие точечки находятся внутри синеньких, и мы это видим, то понятно что гиперплоскостью тут не обойдёшься. 
Но если есть основания предполагать, что мы можем нарисовать хороший эллипс(квадратичную поверхность) 

![[Pasted image 20220309061202.png]]

уравнение эллипса имеет вид 
$$\large
\bar{x}^{T}A\bar{x}+ \bar{B}^{T}\bar{x}+C = 0
$$
Тогда из 3х мерного пространства перейдём в пространство 6
![[Pasted image 20220309061516.png]]
![[Pasted image 20220309061557.png]]
Это как раз квадратичная поверхность в исходном пространстве 

Единственный минус - удорожание операции, так как размерность растёт 

в  SVM есть так называемый kernel trick. который позволяет обойти этот недочет, работая в большом пространстве, на самом деле не работая в нём.


 ## Логистическая регрессия
 (наверное главный метод линейной классификации)

Рассмотрим другой класс моделей
**Discriminative models**

 в порождающих моделях мы строим модель того как точки, получаются в том или ином классе
$$\large 
p(\bar{x}|C_{1}) - generate
$$

В дискриминирующей модели мы говорим, что это очень сложно, и мы с этим не справимся 
Но давайте напрямую моделировать  это распределение вроде как проще моделировать
$$\large
p(C_{1}|\bar{x}) \propto p(C_{1})p(\bar{x}|C_{1})
$$

Попробуем её как то реализовать. (хотим что бы там где то была линейная модель)

Воспользуемся теоремой Байеса 
в бинарном случае 
$$\large
p(C_{1}| \bar{x}) = 
\frac{p(C_{1})p(\bar{x}|C_{1})}
{p(C_{1})p(\bar{x}|C_{1})+p(C_{2})p(\bar{x}|C_{2})}
=
$$
поделим всё на числитель
$$\large
= 
\frac{1}
{1 + \frac{p(C_{2})p(\bar{x}|C_{2})}{p(C_{1})p(\bar{x}|C_{1})}}
=
$$
вот эта дробь внизу называется odds
Снова прологарифмируем 

$$\large
= 
\frac{1}
{1 + e^{-\ln \frac{p(C_{2})p(\bar{x}|C_{2})}{p(C_{1})p(\bar{x}|C_{1})}}}

$$
эта штука в степени уже называется log-odds  
и это уже число от - бесконечности до + бесконечности.

И когда мы уже это число моделируем линейной функцией $\large \bar{w}^{T}\bar{x}$  это называется логистической регрессией
Получается моделируем так:
$$\large
p(C_{1}|\bar{x}) = 
\frac{1}{1+e^{-\bar{w}^{T}\bar{x}}}
$$

![[Pasted image 20220309064745.png]]

Выглядит вот так:
![[Pasted image 20220309064905.png]]

Несколько удобных свойств: 
1. он симметричный $\large \sigma(a)= 1 - \sigma(-a)$
2.  производная 
$$\large
\sigma'(a)= \frac{- e^{-a}}
{-(1+ e^{-a})^{2}} = 
\frac{1}{1+e^{-a}} \cdot 
\frac{e^{-a}}{1+e^{-a}} = 1 - \frac{1}{1+e^{-a}}
$$
или 
$$\large
\sigma'(a)= \sigma(a)(1 - \sigma(a))
$$

для нескольких классов:
$$\large
p(C_{k}|\bar{x}) = \frac{p(\bar{x}|C_k)p(C_{k})}
{\sum\limits_{l}p(\bar{x}|C_{l})p(C_{l})}
= 
\frac{e^{\ln p(\bar{x}|C_{k})}}
{\sum\limits_{l}e^{p(\bar{x}|C_{l})p(C_{l})}}
=
\frac{e^{\bar{w}_{k}^{T}\bar{x}}}
{\sum e^{\bar{w}_{l}^{T}\bar{x}}}

$$

это уже softmax.

$$\large
softmax(a_{1},..., a_{K}) = \left (
\frac{e^{a_{1}}}{\sum e^{a_{l}}}
,...,
\frac{e^{a_{K}}}{\sum e^{a_{l}}}

\right )
$$

Снова вернемся к бинарной классификации, что бы не писать лишнего.

У нас есть датасет, у него есть какие то точки $\large \bar{x}_n$ и к точке прилагается правильный ответ $\large t_{n} \in \{0,1\}$ 
$$\large
D= \{(\bar{x}_{n},t_{n})\}_{n=1}^{N}
$$

Формализуем теперь что такое правдоподобие:
$$\large
p(\bar{w}|D) \propto p(\bar{w})p(D|\bar{w})
$$
![[Pasted image 20220310062834.png]]


Как в логистической регрессии у нас была функция интикатор - мы домнажали вероятности на y и (1-у) что бы нужная часть была равна нулю.
В нашем случае неудобно будет это раскрывать поэтому лучше всё пробросить в степени.
![[Pasted image 20220310063505.png]]


опять возьмем логарифм 
![[Pasted image 20220310063651.png]]

что бы это решить первым делом возьмем градиент
![[Pasted image 20220310071820.png]]

Градиент в итоге такой 
![[Pasted image 20220310073322.png]]

как делается градиентный спуск:

![[Pasted image 20220310074309.png]]

Красивый геометрический смысл.
(левая картинка - верный ответ)
была какая то разделяющая поверхность(зелёная) у неё была какая то нормаль $\large \bar{w}$  b и пришла новая точка (красная ) $\large \bar{x}_{n}$  
чир говорит нам розовая формула. 
тогда мы берём x_n прибавляем его с коэфициентом к w и у нас будет новая (синяя w) 

(правая картинка - неверный ответ)
![[Pasted image 20220310074321.png]]

Можно построить метод второго порядка

![[Pasted image 20220310080041.png]]

гессиан теперь выглядит так 
![[Pasted image 20220310080223.png]]

В общем получается какой то метод второго порядка,
Это то же что то типа метода наименьших квадратов. 
Называется он IRLS iterative reweighted leest squares (итеративные перевзвешенные наименьшие квадраты)

**Лапласовская аппроксимация (laplace approximation)**
Есть какое то сложное распределение вероятностей p(x) (красное)
и мы хотим его приблизить простым распределением q(x) (синее)
в некоторой точке максимума
![[Pasted image 20220310085615.png]]

Как его найти? 
Есть трюк. 
А давайте воспользуемся формулой тейлора, но не для p(x) а для логарифма p(x)


![[Pasted image 20220310085804.png]]



Лапласовские апроксимации нам нужны, для того что бы построить предсказательное распределение логистической регрессии. 


Что мы умеем делать 
![[Pasted image 20220310090619.png]]

А как построить предсказательное распределение 
![[Pasted image 20220310090717.png]]

Аапроксимируем лапласом вот эту штуку в скобках гауссианом.
![[Pasted image 20220310091408.png]]

Дальше тоже трюки:

![[Pasted image 20220310092249.png]]

![[Pasted image 20220310092321.png]]


![[Pasted image 20220310092342.png]]






