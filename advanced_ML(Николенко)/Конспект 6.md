[[Adv_Ml (Николенко)]]



# Лекция 6

## Кластеризация и EM-алгоритм


**Байесовский вывод для гауссиана**

Для простоты будем рассматривать одномерный
$$\large
p(x|\mu, \sigma) =
\frac{1}{\sqrt{2 \pi \sigma^{2}}}e^
{-\frac{1}{2\sigma^{2}}(x - \mu)^{2}}
$$
На самом деле гораздо удобней использовать вместо дисперсии - тау.  Это precision(точность)
$$\large
\tau = \frac{1}{\sigma^{2}}
$$
Тогда гауссиана выглядит так:
$$\large
p(x|\mu, \tau) =
\sqrt{\frac{\tau}{2\pi}}e^
{-\frac{\tau}{2}(x - \mu)^{2}}
$$

Что мы будем делать? Будем обучать гауссиан. 

Есть датасет: $\large D=\{ x_{1},x_{2},...,x_{n}\}$ 

И хочется найти апостериорное распределение:
$$\large
p(\mu,\tau|D)
$$
Может показаться что это линейная регрессия, но на самом деле не совсем. 
Раньше в первом дз дисперсия практически не была нужна, и выбиралась как  гиперпараметр и учувствовала только в регуляризации.
Теперь научим полноценный гауссиан. 
Рассмотрим 3 случая:
1. $\large \tau = const$ и будем делать байесовский вывод только в отношении к среднему: $\large p(\mu|D)$ . 
Это в точности  что мы делали в линейной регрессии  
$$\large
p(\mu|D) \propto p(\mu)\times p(D|\mu) = \prod\limits_{i=1}^{n} p(x_{i}|\mu) = 
\prod\limits_{i=1}^{n}\sqrt{\frac{\tau}{2\pi}}e^
{-\frac{\tau}{2}(x_{i} - \mu)^{2}}
$$
![[Pasted image 20220311034721.png]]

2. 2ой случай $\large \mu = const$ (чисто промежуточный случай к полному обучению гауссиана, на практике мы его не используем)
Учим при константном среднем только дисперсию
$$\large
p(\tau|D) \propto p(\tau) \times p(D|\tau)
$$
![[Pasted image 20220311035721.png]]

3. Когда и и мю и тау неизвестны и мы бы хотели построить априорные распределения на два параметра.
$$\large p(\mu, \tau|D) \ = \ ? $$
$$\large
p(\mu, \tau|D) \propto p(\mu, \tau) \times p(D|\mu, \tau)

$$
$$\large
\ln p(x|\mu, \tau) = const + \frac{1}{2} \ln \tau -
\frac{\tau}{2}(x- \mu)^{2}
$$

Первый позыв рзять мю и тао независимым от мю и тао (оранжевые формулы) 
но такое семейство распределений нельзя выбрать, оно не будет сопряженным (там что то нифига не раскладывается)
как на картинке, если появляются точки то мю и тау сразу становятся зависимыми
![[Pasted image 20220311051133.png]]

Раз вот так делать нельзя, мы не можем это предпологать
получается по формуле ниже либо выражение через мю либо черех тау (выбираем ту что через тау)
В качестве $\large p(\tau)$ - какое то гамма распределение
В качестве $\large p(\mu|\tau)$ - гауссиан  
![[Pasted image 20220311051804.png]]

![[Pasted image 20220311052145.png]]

![[Pasted image 20220311052223.png]]
какое будет предсказательное распределение для новой точки, если даны предыдущие 
![[Pasted image 20220311052245.png]]

Видим что тут уже не гауссиан, а распределение стьюдента.


**Экспоненциальное семейство** Exponential family

Говорят что распределение лежит в экспоненциальном семействе если его можно представить следующим образом. 

$$\large
p(\bar{x}|\bar{\eta}) = h(\bar{x})\cdot g(\bar{\eta}) \cdot e^{\bar{\eta}^{T}\bar{u}(x)}
$$

$$\large
\ln p(\bar{x}|\bar{\eta}) = \ln h(\bar{x})+ \ln g(\bar{\eta}) + \bar{\eta}^{T}\bar{u}(x)
$$

$\large \bar{\eta}^{T}$  - natural parameter (естественный параметр) это что то что имеет смысл(величина полезная для понимания того, что происходит в этом распределении.)
Например:
1. распределение бернулли. $\large \theta$ - вероятность события(орла или решки)

$$\large
p(x|\theta)= \theta^{x}(1-\theta)^{1-x}= 
e^
{x \ln \theta + (1-x) \ln (1- \theta)}
$$
в виде экспоненциального семейства:
$$\large
p(x|\theta) = (1- \theta) \cdot e^
{x(\ln \theta - \ln(1-\theta))}
$$
где:
$\large h(x) =1$,  $\large g(\eta) =1-\theta$,  $\large u(x) = x$,
$\large \eta =\ln \frac{\theta}{1-\theta}$ - это как раз log-odds

![[Pasted image 20220313044049.png]]

2. гауссиан
![[Pasted image 20220313044114.png]]

Плучается естественными параметрами для гауссиана получается не мю и сигма, а тау и тау * мю


Для любого распределения из экспоненциального семейства можно записать сопряженное априорное распределение прямо сразу 
![[Pasted image 20220313063727.png]]

## EM алгоритм

**Начнем с задачи кластеризации**

Это классическая задача обучения без учителя.
clastering     ------->  unsupervised learning

надо как то найти $\large p(\bar{x})$ 

![[Pasted image 20220313065448.png]]

Как искать кластеры, как разбивать на группы.

Самый простой алгоритм: иерархическая кластеризация
hierarchical clastering


Рисуем все свои точки (внизу на линии)
выбираем из них 2 самые близкие и объединяем их в один кластер
повторяем.
Нарисовав такое дерево, оказывается что мы решили задачу кластеризации на сколько угодно кластеров, и мы можем отсекать вершуну до тех пор пока не получим требуемое количество кластеров
![[Pasted image 20220313070803.png]]

Как определить расстояние между кластерами? 
есть много разных ответов.
в целом они делятся на 2 категории. 

single link кластеризация 
когда расстояние между двумя кластерами, это минимум, расстояний между двумя точками из них
$$\large
d(C_{1},C_{2}) = \underset{x \in C_{1}, y \in C_{2}}{\min} d(x,y)
$$
complete_link кластеризация
когда расстояние между двумя кластерами, это максимум, расстояний между двумя точками из них
$$\large
d(C_{1},C_{2}) = \underset{x \in C_{1}, y \in C_{2}}{\max} d(x,y)
$$

Они по разному себя ведут
есть 4 кластера, а между двумя их них есть перемычка между ними.

single link в начале получит 4 островка и дальше довольно быстро начнет добавлять к нижнему кластеру перемычку в виде отростка. 
и потом срастит в один верхний и нижний левые. 

complete_link наоборот  в начале получит 4 островка, а перемычка будет как то отдельно существовать и раньше объединятся 2 кластера справа прежде чем кластеры слева. 

где минимумы кластеры разлапистые, где максимумы, кластеры максимально локализованы 
![[Pasted image 20220313072257.png]]

Это 2 крайних случая, можно еще кучу других, промежуточных придумать.


Другой алгоритм 

**DBSCAN**

Тут есть 2 параметра
1. размер окрестности $\large \varepsilon$ 
2.  минимальное число точек, которое может образовывать плотный кластер m

Выбираем случайную точку (где то справа)
описываем вокруг неё $\large \varepsilon$ окрестность 
и если в этой окрестности нашлось m точек, мы начинаем новый кластер (считаем что все точки которые лежат в этой окрестнсти приписываем этому кластеру и рекурсивно повторяем процедуру)

А если взяли точку и вокруг неё нету достаточного количества m точек(зелёная снизу) то считаем что это просто выброс не образующий кластер (outlier)

![[Pasted image 20220313073302.png]]

получаем такие свойства:
1. выбросы не причисляются к каким то классам
2. количество кластеров образуется само по себе
3. нужно что бы была эффективная процедура поиска точек в окрестности (в евклидовых пространствах невысокой размерности это можно сделать, в пространствах высокой размерности уже такое)

**K-means**

есть точки
разбиваем из на (допустим) 3 кластера
каким то случайным образом выбираем центроиды этих кластеров
(3 случайные точки данных не слишком близко расположенных друг к другу)
дальше для всех остальных точек пытаемся определить к какому классу она принадлежит по ближайшему соседу.
дальше передвинем центроиды в их центр масс.
повторяем расчет, переназначаем точки пока процесс продолжается, пока точки не перестанут меняться. 

![[Pasted image 20220313074357.png]]


в k-means кластеры получаются строго геометрическими, в DBSCAN могут расползаться как угодно


**Посмотрим на то, как кластеризация выглядит с вероятностной точки зрения**

Есть какие то точки(розовые) тогда гипотеза состоит в том что каждый из кластеров представляет какое то распределение вероятности (нарисованы гаусианчики, но это не обязательно должны быть они)
![[Pasted image 20220313080528.png]]

В чем заключается наше предположение, откуда берутся $\large p(\bar{x})$  откуда берутся эти точки.
Они берутся из одного из кластеров но надо выбрать из какого. 
получается 2-х шаговый процесс:

Сначала типа кидаем кубик из которого получается номер кластера $\large \bar{z} = (0,...1,...0)$ которая строит one hot представление номера кластера.   
а потом x набрасываем из распределения pk(x):
$$\large
\bar{x} \sim p_k(\bar{x})
$$
![[Pasted image 20220314033215.png]]

То есть:
$\large p(\bar{x})$ имеет внутри себя параметры кубика, и параметры каждого распределения кластеров
И если хотим максимизировать p(D) при условии всех этих параметров, то это будет большое произведение по всем иксам от таких вот сумм, которые мы хотим максимизировать.
![[Pasted image 20220314034159.png]]

И это сложная задача. 
Что бы с этим как то работать, надо сделать такое вот наблюдение:

Заметим что в этой конструкции есть скрытые переменные, такие что если бы мы их знали всё бы было очень легко. 
Это как раз эти самые z.
Если бы мы знали z:
![[Pasted image 20220314035026.png]]

И если мы хотим это максимизировать по $\large \pi , \theta$ то

![[Pasted image 20220314035435.png]]

И задача уже разделилась. Просто обучаем кубик $\pi$ на котором выпали Znk последовательно + еще правая сумма
То есть если бы мы знали из какого кластера какая точка, никакой бы проблемы не было бы.

В такой ситуации, когда само правдоподобие - сложная штука, но есть какие то скрытые переменные, которые делают её простой штукой и придуман EM алгоритм. 

Выглядит он так

### EM-algorithm
expectation maximization 

Простой пример: одномерная кластеризация (монетка). 

![[Pasted image 20220314040124.png]]

В таком условии 
![[Pasted image 20220314040210.png]]

Если бы знали какая точка из какого гауссиана, никаких проблем бы не возникло. 

И тогда EM алгоритм - это итеративный алгоритм, у которого есть 2 шага: 

E-шаг:
Считаем ожидание Zn по распределению текущих параметров.
$$\large
\mathbb{E}_
{z_{n}|\pi, \mu_{1}, \sigma_{1}, \mu_{2}, \sigma_{2}}
[z_{n}] = \ ?
$$

M-шаг:
Тут берём эти ожидания z_n подставляем их в правдоподобие, и максимизируем
$$\large
\prod p(x_{n}, \mathbb{E}_{z_{n}} | \pi ...)
\underset{\theta}{\rightarrow}
\max
$$

Итак более подробно: 
E-шаг:
Zn - верно ли что Xn взялось из первого кластера
![[Pasted image 20220314041437.png]]
Что такое ожидание Zn?
Это означает  вероятность того что Zn взялось из первого кластера

![[Pasted image 20220314041336.png]]

![[Pasted image 20220314041347.png]]

По сути это и есть тот самый оптимальный байесовский классификатор. 
есть 2 гауссиана, точка, и надо понять какой гауссиана он принадлежит
![[Pasted image 20220314042512.png]]

M-шаг:
Пишем логорифм правдоподобия вместе с Z и вместо них подставляем туда ожидания Z с предыдущих шагов.
![[Pasted image 20220314042705.png]]

Получается что ЕМ алгоритм - это итеративное повторение этих шагов.
![[Pasted image 20220314074706.png]]

Так выглядит ЕМ алгоритм для кластеризации. 

С вероятностной точки зрения: кластеризация - обучение смеси распределений. 
![[Pasted image 20220314075011.png]]

В общем виде:
![[Pasted image 20220314075032.png]]

Что бы он работал, нужно уметь обучать распределения по точкам с весами. 

Когда нужно кластеризовать точки на 2d поверхности, EM прям не очень работает, зато он очень общий


**Кластеризация на строках**

часто возникает в биоинформатике, где есть много строк с геномом. 

![[Pasted image 20220314081351.png]]

тут можно как то определить расстояние, но на самом деле больше смысла если определить  порождающую модель строки.
Выдвигаем гипотезу - откуда взялись все эти строчки.


Зафиксируем длину строки (для простоты), и тогда распределение кластера задаётся кубиком в каждой позиции этой строки. и разные кластеры отличаются друг от друга, разными вероятностями граней этой пирамидки.

Какие параметры у этой модели:
откуда идёт новая строчка x
это смесь (сумма по кластерам) 



![[Pasted image 20220314082034.png]]

![[Pasted image 20220314082412.png]]


Как делать что бы не было произведений очень маленьких чисел.

![[Pasted image 20220314082825.png]]








