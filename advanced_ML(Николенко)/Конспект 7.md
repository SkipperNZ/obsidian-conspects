[[Adv_Ml (Николенко)]]


# EM -алгоритм в общем виде


В прошлый раз мы обсуждали EM алгоритм только в задаче кластеризации. 

Мы обсудили саму задачу кластеризации, поняли её с точки зрения теории вероятностей как задачу разделения смеси распределений. 
Увидели что есть какие то точечки (серые) и считаем что одни точечки приходят из одного распределения, другие из другого итд.
![[Pasted image 20220407031413.png]]

Задача состоит в том, что бы научится обучать параметры этой смеси распределений. 

В прошлый раз говорили следующее: 
Есть у нас какое то очень сложное правдоподобие, которое представляет собой сумму по кластерам: 
$$\large
p(\bar{x}|\bar{\theta}) = 
\sum\limits_{k} \pi_{k}p_{k}(\bar{x}|\bar{\theta_{k}})
$$
Таким образом, по скольку это правдоподобие  одной точки представляет сумму. То когда мы переходим к правдоподобию всего датасета, у нас получается произведение сумм.

$$\large
p(D|\bar{\theta}) = 
\prod_{n} \left (
\sum\limits_{k} \pi_{k}p_{k}(\bar{x})
\right)
\underset{\bar{\theta}}{\rightarrow} \max
$$
Оптимизировать её весьма затруднительно, так как она весьма трудно разворачивается. 

Что бы её оптимизировать мы тогда сделали вот какое замечание:

Мы предположили, что есть вспомогательные переменные z

Эти переменные показывают из какого именно кластера, взята какая точка. z_nk равна единице, тогда и только тогда, когда x_n взята из C_k кластера.
z: $\large z_{nk} = 1 \Leftrightarrow x_{n} \in C_{k}$ 

То есть мы сначала, кидаем кубик и выбираем кластер, потом добываем из этого кластера точку.

Чем же замечательны эти вспомогательные переменные? 
Теперь если записать правдоподобие совместное данных иксов и скрытых переменных z, то всё оказывается просто. Получится не произведение сумм, а произведение произведений. 
$$\large
p(D,Z|\bar{\theta}) = \prod_{n}\prod_{k}
(\pi_{k}p_{k}(\bar{x}))^{znk}
\underset{\bar{\theta}}{\rightarrow} \max
$$
А вот эту штуку, если знать D и z  максимизировать по тетта одно удовольствие.
Это обучение каждого кластера по отдельности и еще одного кубика 

Итак мы придумали итеративную процедуру: 
Cначала посчитать ожидания зетов по какому то текущему тета М-тому при фиксированном значении параметров, потом подставить их в это совместное правдоподобие  и найти новый тета(м +1) как 
аргмакс р(D и ожидание z_nk| тета)
 $$\large
\mathbb{E}_{\theta^{(m)}} [z_{nk}] 
\rightarrow 
\theta^{(m+1)} = 
\arg \max \ p(D, \mathbb{E}[z_{nk}]|\theta)
$$
Попробуем понять что тут происходит

### EM-algorithm
В общем виде

Что нужно чтобы эту схему запустить и использовать. 
Есть l(teta) который является лагорифмом правдоподобия  и  мы хотим его максимизировать по тета.
$$\large
l(\theta) = \log(p(x|\theta))
\underset{\theta}{\rightarrow} \max
$$
И EM алгоритм нужен в общем случае, когда это трудно сделать напрямую (градиентом итд)
Как мы это делаем?
Наша гипотеза состоит в том, что есть еще какие то скрытые переменные z, такие что p(x,z|teta)  такое совместное правдоподобие, решается проще. 

А дальше мы хотим максимизировать l(teta)
Будем максимизировать его каким то итеративным процессом 
будет тета0 которую выберем более менее случайно 
![[Pasted image 20220408040838.png]]

И хотим утроить этот процесс так, чтобы всякий раз, когда мы переходим от тетаM переходим к тета(М+1) у нас бы увеличивалось правдоподобие(серая приписка снизу)

И так что нам от такого процесса нужно 
Рассматриваем l(teta) и хотим его увеличить от предыдущего l(tetaM)
![[Pasted image 20220408041128.png]]
теперь попробуем суда вставить z
есть только 1 способ это сделать, надо заменить p(x) на интеграл
с права ничего не меняем так как это вроде как константа.
![[Pasted image 20220408041218.png]]
И еще 1 трюк.  Было бы клево, если бы мы могли расценить этот интеграл как ожидание по какому нибудь распределению.

Умножим и поделим на вот такое вот условное распределение: 
теперь вот этот интеграл это ожидание обведённой в синенький кружочек функции, по распределению зелёного кружечка

![[Pasted image 20220408041554.png]]
![[Pasted image 20220408041831.png]]

Собственно тут написан логарифм матожидания 
Применяется неравенство енцена 
Выпуклая функция -  которая больше либо равна своей хорде.
А неравенство енцена это как бы продолжение этого определения на  бесконечную сумму.
![[Pasted image 20220408042042.png]]

Воспользуемся им, заметив что логарифм это такая же функция.
![[Pasted image 20220408042512.png]]
В этой подчёркнутой константе z никакого нет, и мы просто вносим её под матожидание  и она просто пойдёт в знаменатель под логарифм 
![[Pasted image 20220408042538.png]]

Теперь  если обозвать то что получилось L красивое от тета(красным написано снизу)  то у нас получилось вот что:
![[Pasted image 20220408043234.png]]

Теперь что нам говорит EM алгоритм: 
Получилась вот токая нижняя  оценка, давайте возьмем  такое L красивое и будем максимизировать его по тета
Тогада в этой новой тете, которая достигает максимума L(teta) тоже увеличивается  в сравнении с предыдущей 
![[Pasted image 20220408043354.png]]

И остаётся один вопрос, а почему она увеличивается (почему в аргмаксе не может быть отрицательного значения). 
![[Pasted image 20220408043809.png]]
Так как 
![[Pasted image 20220408043947.png]]
То максимум явно не меньше нуля

Фиолетовая линия - то что хотим максимизировать. 
ось внизу - тета
Что происходит в EM алгоритме на самом деле. 
Есть какой то тетаM и мы строим функцию Lкрасивое+ l(tetaM) и она в точке тетаМ касается этого L(teta) и всегда остаётся меньше либо равна этой фиолетовой линии. 
А если перейти в максимум этой штуки, то L(тета) в ней тоже увеличивается итд.
![[Pasted image 20220408044105.png]]

Это всё теоретическое обоснование EM алгоритма, которое нам нужно
Еще замечание, в Lкрасивом в знаменателе тета не содержится и в оптимизации не учувствует, и вместо  Lкрасивое в EM алгоритме обычно оптимизируют функцию Q 
![[Pasted image 20220408045432.png]]

Теперь вопрос, а какое это имеет отношение к тому что было на прошлой лекции. 

Снова перепишем кластеризацию исходя из этого формализма:
X - точки
z - те самые z_nk тые.  z_n - one hot вектора.
teta - состоит из вертора пи(вероятности кластеров) и какие то параметры каждого кластера
p(x|teta) - вот это произведение сумм
логарифм этого p - и есть l(teta) - сложная функция которую мы хотим оптимизировать. 
![[Pasted image 20220408171407.png]]

А простая функция - 
![[Pasted image 20220408172136.png]]

Что говорит EM алгоритм - давайте запишем функцию Q и будем её максимизировать. 
![[Pasted image 20220408172320.png]]

Видим что это мат ожидание по z от большой линейной комбинации этих z. 
Спускаем матожидание по z и получается последнее уравнение, где часть обведена зелёненьким кружочком. 

Теперь вот эти ожидания(в зелёном кружечке) - те что мы считали на E шаге. Потом мы их как бы должны подставить  на месть z_nk в то самое правдоподобие. 

Так получилось потому что логарифм p(x,z| teta) - линейный по z

по скольку в случае кластеризации(и в подавляющем большинстве случаев) скытая переменная линейна (не в квадрате) в этом логарифме. то можно пользоваться этой схемой. 

Парочка очевилных соображений:
![[Pasted image 20220408173905.png]]

А зачем максимизировать?  Вместо argmax можно искать новый тета который просто больше(или равен)
этот вариант называется обобщенный EM (generalized EM) в жизни он встречается реже, чем может показаться. 
![[Pasted image 20220408173922.png]]

А вот чаще встречается, что вот это ожидание трудно посчитать. 
Но можно научится семплировать из этого распределения, и функцию q приблежать как формулу снизу, где z_r взяты из распределения(справа)
Это - монте-карло ем
в варианте когда 1 семпл - стахостический ем.
![[Pasted image 20220408173941.png]]


Пример: 






























































































































































































































