[[Adv_Ml (Николенко)]]


# Линейная регрессия

Одно из главных предположений линейной регрессии состоит в том, что шум распределён нормально вокруг предполагаемой линии

Из прошлой лекции:
$$\large
\ln p(D|\bar{w},X)= \sum\limits_{n=1}^{N}
(\frac{1}{2}\ln ((2\pi\sigma^{2})-\frac{1}{2\sigma^{2}}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2})=
$$
$$\large
= const -\frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2}
\underset{\bar{w}}{\rightarrow} \max
$$

Одна из первых плодотворных дебютных идей, что бы избежать переобучения - добавить данных.
Оверфитинг - не свойство модели, а свойство пары модель/датасет.

Вторая дебютная идея: 
Подметим, что у явно переобученных моделей, коэффициенты при x начинают резко расти, увеличении степени многочлена, и начало этого резкого роста подозрительно совпадает с  началом переобучения. 
То есть надо сказать модели что у неё не должно быть слишком больших по модулю весов.

$$\large
L(\bar{w}) = \sum\limits_{n}(y_{n}-\bar{w}^{T\bar{x}_{n})^{2}+}\lambda\cdot||\bar{w}||_{2}^{2} \rightarrow \min
$$
$$\large
(y -X\bar{w})^{T}(y -X\bar{w}) +


\lambda\bar{w}^{T}\bar{w} \rightarrow \min
$$

$$\large
\nabla_{\bar{w}}L =
2X^{T}X\bar{w}^{*}-2X^{T}\bar{y}+2\lambda\bar{w}^{*} =0
$$

$$\large
\bar{w}^{*} = (X^{T}X+\lambda I)^{-1}X^{T}\bar{y}
$$


$\large \lambda\bar{w}^{T}\bar{w}$ - ridge regression - 2ая норма ругуляризатора L2

А теперь разберём что такое эта регуляризация, её физический и вероятностный смысл. И из каких соображений его выбирать. 

Это слагаемое призвано выражать нашу интуицию о том, что наверное веса не должны быть большими. Это предположение мы делаем когда ещё не видели никаких данных, отсюда делаем вывод, что это  **априорное распределение**

Теперь мы хотим строить апостериорное распределение, пропорционально произведению априорного распределения и правдоподобия:
$$\large
p(\bar{w}|D) \propto
p(\bar{w})p(D|\bar{w}) =
p(\bar{w})
\prod\limits_{n=1}^{N}p(y_{n}|\bar{w},\bar{x_{n}})
$$
Гауссиан под произведением мы никак не контролируем, только $\large p(\bar{w})$ - его мы хотим видеть нормальным, с центром в нуле.

$$\large
p(\bar{w}) = N(\bar{w}|\bar{0}, \alpha I)
$$
альфа тут - коэффициент который можно двигать 
I - единичная матрица

$$\large
\ln p(\bar{w}|D) =
const - \frac{1}{2\sigma^{2}}
\sum\limits_{n}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2} - 
\frac{d}{2}\ln 2\pi - \frac{d}{2}\ln 2\alpha -
$$
$$\large

-\frac{1}{2\alpha}\bar{w}^{T}\bar{w}= const - \frac{1}{2\sigma^{2}}\sum\limits_{n}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2}- \frac{1}{2\alpha}\bar{w}^{T}\bar{w}
$$

Пояснения к формуле выше. 
Как выглядит плотность многомерного гауссиана:
![[Pasted image 20220212171456.png]]

Константы уходят (на картинке зачеркнуты)
![[Pasted image 20220212171824.png]]

И получилась в точности гребневая регрессия
$$\large
\sum\limits_{n}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2}+
\frac{\sigma^{2}}{\alpha}\bar{w}^T\bar{w}
\rightarrow \min
$$
Коэффициент регуляризации это отношение дисперсии шума к дисперсии того распределения которого мы хотим придумать.

То есть гребневая регрессия (добавление регуляризатора $\large \lambda \cdot ||\bar{w}||^{2}_{2}$ ) это в точности априорное распределение с центром в нуле и какой то пропорциональной единичной матрицы ковариации. 

Давайте сделаем пересчет из априорного распределения в апостериорное:
$$\large
\ln p(\bar{w}|D)= const + \ln p(D|\bar{w})+\ln p(\bar{w})=
$$
$\large \ln p(\bar{w}) = N(\bar{w}|\bar{\mu}_{0}, \Sigma_{0})$ 
$$\large
= const - \frac{1}{2\sigma^{2}}\sum\limits_{n}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2}- \frac{d}{2}\ln 2\pi - \frac{1}{2}\ln \det \Sigma_{0} -
$$
$$\large
- \frac{1}{2}(\bar{w}-\bar{\mu}_{0})^{T}\Sigma_{0}^{-1}(\bar{w}-\bar{\mu}_{0})=
$$
Заебался переписывать:
![[Pasted image 20220212173934.png]]

![[Pasted image 20220213012521.png]]

![[Pasted image 20220213012629.png]]

Хотя это и удобно, никто не запрещает в качестве регуляризатора брать не только сопряженное распределение.

Самый популярный альтернативный регуляризатор это первая норма ветора L1

$$\large
L(\bar{w})=\sum\limits_{n}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2} + \lambda \sum\limits_{i=1}^{d}|w_{i}|
\rightarrow \min
$$
В случае регрессии это называется Lasso regression.
А какому распределению соответствует этот  регуляризатор.
$\large \lambda \sum\limits_{i=1}^{d}|w_{i}|$ это на самом деле $\large const - \ln p(\bar{w})$

У lasso есть интересный эффект: если увеличиваем регуляризацию, то многие коэффициенты будут становится строго равными 0.
Это позволяет понять какие переменные важны, а какие нет.

Объясним этот эффект. Он берётся из интересного наблюдения:
Мы в машинном обучении с регуляризаторами фактически делаем задачу оптимизации с дополнительным членом. и она эквивалентна просто оптимизации вот этой функции 
$$\large
\sum\limits_{n}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2}
\rightarrow \min 
$$
Но при условии ограничивающем вот это слагаемое 
$$\large
\sum\limits_{i=1}^{d}|w_{i}|
$$
То есть фактически когда мы добавляем регуляризатор мы говорим так:
Ты ищешь оптимальные веса, но не во всём пространстве весов, а в каком то ограниченном подмножестве.
То же самое верное и для гребневой L2.

Нарисуем эти условия (в размерности 2, так как в больших размерностях рисовать сложно)
Оси - веса. 
Модель - обычная 2d линейная регрессия 
![[Pasted image 20220213015311.png]]

Как устроено апостериорное распределение(или правдоподобие) в этом пространстве это гауссиана(как я понял размерности пространства) с нулём в $\large \bar{w}_{ML}$ 

В двумерном случае:
L1: $\large \sum\limits_{i=1}^{d}|w_{i}| = |w_{1}|+|w_{2}| \leq \lambda$ 
L2: $\large \sum\limits_{i=1}^{d}(w_{i})^{2} = (w_{1})^{2}+(w_{2})^{2} \leq \lambda$ 
Отсюда и выходят области круга и квадрата.
Получается если у нас априорная точка находится внутри этой зоны, то всё ок, а если (и как часто бывает) снаружи то ищется точка ближайшего пересечения этой гауссианы с центром в $\bar{w}_{ML}$ с краем данных зон, и там и будет максимальная апостериорная гипотеза. $\large \bar{w}_{MAP}$ 

![[Pasted image 20220213025935.png]]

В чем разница между левой и правой картинкой?
для желтеньких гауссиан особо никакой, но у правой зоны есть углы и если рисовать такие эллипсы из разных точек пространства, то немалая часть таких точек (бирюзовые эллипсы справа) будет давать максимальную апостериорную гипотезу котоая приходит в этот угол. 
Особенность: в любом из углов такого квадрата, один из коэффициентов равен нулю.
Этот эффект значительно усиливается при переходе к большему числу размерностей. Например в размерности 100 уже будет дофига нулей.


![[1__e8BLNA749W_7yxi7hz-DA.gif]]

Можно ещё сильнее увеличить этот эффект Lq:
$$\large
L(\bar{w})= \sum\limits_{n}(y_{n}-\bar{w}^{T}\bar{x}_{n})^{2}+ \lambda \sum\limits_{i}|w_{i}|^{q}
$$
Чем больше q Тем сильнее круг начинает раздуваться в квадрат, на левом рисунке помечен как $L\infty$  Это вроде как не даёт особых преимуществ. 
А если наоборот уменьшать q, и делать его меньше единицы, то будет розовая картинка на правом рисунке.

Итак мы решили половину задач байесовского вывода, мы нашли апостериорное распределение и тут же научились его максимизировать, так как оно оказалось гауссианом. 

## Теперь попробуем делать предсказания.

Мы хотим найти распределение y в каком то новом $\bar{x}$ при условии имеющихся данных D. (predictive distribution).
Надо добвить w и проинтегрировать по нему
и внутри это превратиться в произведение правдоподобия одной точки и апостериорного распределения $\bar{w}|D$ 
Соответственно первая часть это буквально правдоподобие одной точки, а вторая часть это наше посчитаное апостериорное распределение
$$\large
p(y|\bar{x}, D)= \int p(y,\bar{w}|\bar{x},D)d\bar{w}=
\int p(y|\bar{w},\bar{x})p(\bar{w}|D)d\bar{w}=
$$
$$\large
=\int N(y|\bar{w}^{T}\bar{x}, \sigma^2)N(\bar{w}|\bar{\mu}_{N}\Sigma_{N})d\bar{w}
$$
Как можно брать такой интеграл?
Надо сделать из этих двух гауссиан один (желтая формула) интеграл гаусиана равен единице, а вот то что вынесли за интеграл(то что не зависит от w) как раз и останется.

И надо прологарифмировать:
![[Pasted image 20220213153140.png]]

Итого получилось:
$$\large
\Sigma^{'-1} = \Sigma^{-1}_{N}+\frac{1}{\sigma^{2}}\bar{x}\bar{x}^{T}
$$
$$\large
\mu^{'} = \Sigma^{1}(\Sigma^{-1}_{N}\bar{\mu}_{N}+\frac{y}{\sigma^{2}}\bar{x})
$$
![[Pasted image 20220213155750.png]]

И где то тут опять получился гауссиан. 

![[Pasted image 20220213160303.png]]

Итого из всей этой байды следует что:
$$\large
\sigma_{pred}^{2}=\sigma^{2}+\bar{x}^{T}\Sigma_{N}\bar{x}
$$
$$\large
\bar{\mu}_{pred}=\bar{\mu}_{N}
$$
сигма - дисперсия (увеличится, причем на какую то функцию от х)
мю - среднее

Наши предсказания где то более точные, а где то менее точные, и это зависит от того, в какой точке мы предсказываем.
Интуитивно это так:
если у нас много точек, и эти точки хорошо описывают линейную регрессию то в настоящем байесовском предсказании так: 
если предсказываем x в окрестности других иксов то дисперсия маленькая но она будет быстро расти , когда мы выходим туда, где нет данных.
![[Pasted image 20220213161149.png]]

![[Pasted image 20220214014503.png]]




























