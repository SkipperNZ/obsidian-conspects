
### BCE
$$\large
J = -(y\log (p)+(1-y)\log (1-p))
$$

### Focal loss
![[Pasted image 20240623170732.png]]
$$
FL(pt) = −αt(1 − pt) γ log(pt).
$$

### Beyond BCE
* Применение кросс-энтропии может сломаться о дисбаланс классов (и не только в сегментации)
* Вспомним, что в детекторах объектов говорили про понятие Intersection-over-Union (IoU) (синоним - Jaccard Index):
![[Pasted image 20220503021828.png]]
Вообще IoU это некая функция, которая на вход получает 2 множества а на выходе возвращает некую оценку близости этих множеств. 


**Jaccard Index для сегментации**

Если продолжить цепочку рассуждений и обобщить эту функцию на случай сегментации и  работать с сегментационными масками.

* По аналогии определим Jaccard Index для пары сегментационных масок:
![[Pasted image 20220503025731.png]]
Хотелось бы эту штуку использовать для оптимизации наших моделей почему: если наша модель еще не начала хорошо работать на пикселях переднего плана, и сильно ошибается, то если мы будем брать BCE по каждому пикселю а потом усреднять, то в случае если наши объекты занимают мало места мы будем получать мало значение Loss и соответственно наша сеть будет долго учится на таких примерах и все градиенты будут заполнены сигналом от фона. 
Но в случае если наша предсказанная маска очень плохо пересекается с маской ground true в этом смысле индекс жокара будет очень низким. И такой лосс будет более эффективно обучать нейросеть.

* Напрямую оптимизировать Jaccard Index нельзя 

Можно взять аппроксимацию, которую легко посчитать числено. 
(оригинальную нельзя посчитать числено, потому что мы не очень понимаем как посчитать мощность множества и их пересечения)
но если мы в качестве пересечения возьмем произведение этих двух масок, а в знаменателе возьмем не сумму пересечений, а просто сложем 2 маски и вычтем их них результат перемножения этих 2х масок.
А еще сложность в том, что у нас одно множество ground true это бинарные метки, а предсказанная маска - небинарная (от нуля до единици) и поэтому тоже нельзя напрямую индекс посчитать.
* Но можно аппроксимировать его, например:
![[Pasted image 20220503025835.png]]

Давайте сделаем из этого Loss
Индекс жокара чем больше, тем лучше, а в лосах наоборот, чем он больше, тем дело хуже.
Получается Jaccard loss который очень часто используется в задачках сегментации.
* Получить из этого лосс можно, например, так:
$$\large
Loss_{J} = 1 -\log(J_{seg})
$$
* Часто комбинируют с BCE:
$$\large
Loss = \alpha \cdot Loss_{BCE}+(1-\alpha)\cdot Loss_{J}
$$

Есть еще **Dice Loss** почти тоже самое, но немного отличаются

Jaccard Loss vs Dice Loss - постоянная [путаница](https://stats.stackexchange.com/questions/381789/what-is-the-difference-between-dice-loss-vs-jaccard-loss-in-semantic-segmentatio)

[A survey of loss functions for semantic segmentation (2020)](https://arxiv.org/pdf/2006.14822.pdf)