
Batch Normalization
Значительное ускорение сходимости при обучении

* Проблема: internal covariate shift
	*  Обновление параметров слоя ведет к изменению распределения его выходных значений - эффект домино
* Некоторые функции активации“насыщаются” (sigmoid, tanh) и “хорошо” пропускают градиент только в окрестности 0


* Решение: делаем пере-нормировку активаций по ходу сети
	* Только при обучении по мини-батчам!

![Image](https://lh6.googleusercontent.com/N7v5xIL4aaPUk8yz13QnlkG22-H2C1aFk-iidgn3WKb1DH4JBuggBxkWWhbk0Yfsn2Zh59ZVVriubwVmCuR1ZH0ilWXv1JuJnjo3Idf7RJrM2AqEVCO6njPBwRZqwrjw1Z9WBv8lNbhm)


![[Pasted image 20220330182450.png]]

![](https://lh3.googleusercontent.com/tCMy65Rq1hAHqppgyb0ewwk_8WvcLT1vaBVtcP-XaDfFdW6oDHrxavFtV5EcnAfRfHuHd4hbM55KzD-2cDGBmkXSSIljYk4SyPR9nkHzQikcXnwCcdq46ekVGBjCa3RC36mAft_b823c)

![[Pasted image 20220330182702.png]]
