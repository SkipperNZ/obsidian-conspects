[[base_dl]]

# Лекция 1

## Логистическая функция активации
$$\large 
\large Y(X,w) = F(w_{0}+ \sum\limits_{i=1}^{N}X_{i}w_{i})
$$
Если F сигмоида, то похоже на логистическую регрессию.
$$\large
F(S)= \frac{1}{1+e^{-S}}
$$

$$\large
P(L=1|X)=Y(X,w) = \sigma(r)=\frac{1}{1+e^{-r}}
$$
$$\large
r =w_{0}+ \sum\limits_{i=1}^{N}X_{i}w_{i}
$$
$\large w$ - веса.
$\large X$ - матрица входных данных
r - линейная комбинация, которая идёт на вход симииды.
$\large Y(X,w)$ - выход нейрона, после сигмоиды.


Что такое r?
$$\large
-r = \log(\frac{1}{P}-1)=\log \frac{1-P}{p}
$$
$$\large
r= \log \frac{P}{1-P}
$$
r - логарифм отношения правдоподобий двух гипотез: L = 1 и L = 0.

$$\large 
Y(X,w) = F(w_{0}+ \sum\limits_{i=1}^{N}X_{i}w_{i})
$$
Для задачи бинарной классификации:
$$\large
P(X, w) = 
\begin{cases}
1, Y(X,w) \geq threshold \\
0, Y(X,w) < threshold
\end{cases}
$$
Если F монотонна, то разделяющая поверхность - гиперплоскость.

если функция не монотонна(например синус), то пространство будет разделено не на 2 части а на полосы.

**Функция потерь**

$$\large
Error(w) = \frac{1}{|X|}\sum\limits_{X}Error(X,w)
$$
Эта функция ошибки дискретна и это не очень хорошо, так как не дифференцируемо.

Вместо этого можнно брать расстояние от верного ответа до нашего 
* L2
$$\large 
Error(X,w)= (Y(X,w)-L(X))^{2}
$$
* Перекрёстная энтропия
$$\large
Error(X,w) = 
\begin{cases}
-\log Y(X,w), \ \ \ \ \ \ \ \ \ \ \ \ \ L(X) = 1  \\
\\
-\log(1- Y(X,w)), \ \ \ \ L(X) = 0
\end{cases}
$$

**Усреднение перекрёстной энтропии**

* На каждом шаге считается перекрёстная энтропия H(p,q)
* Значения усредняются для нескольких семплов

Итоговый лосс:
$$\large
\begin{multline*}

Loss = -\sum\limits_{i}P(X_{i})\sum\limits_{j}P(L=j|X_{i})\log Q(L=j|X_{i}) \\
\geq - \sum\limits_{i}P(X_{i})\sum\limits_{j}P(L=j|X_{i})\log Q(L=j|X_{i}) \\
= - \sum\limits_{i} \sum\limits_{j} P(L=j,X_{i})\log \frac{P(L=j, X_{i})}{P(X_{i})} \\
- \sum\limits_{i} \sum\limits_{j} P(L=j,X_{i})\log P(L=j, X_{i}) + \sum\limits_{i}P(X_{i})\log P(X_{i}) \\
= H(X,L)-H(X)=H(L|X)=H(L)-I(X,L) \\
.
\end{multline*}
$$

![[Pasted image 20220202160849.png]]

**Градиентный спуск**

Одномерный случай:
$$\large 
f: \mathbb{R \rightarrow R}
$$
$$\large 
w = argmin f(w)
$$
$$\large 
w_{i+1} = w_{i}-\lambda f'(w_{i}), \ \ \ \forall w_{0}
$$
Многомерный случай:

$$\large 
f: \mathbb{R^n \rightarrow R}
$$
$$\large 
w = argmin f(W)
$$
$$\large 
W_{i+1} = W_{i}-\lambda \nabla f(W_{i}), \ \ \ \forall W_{0}
$$


![[Pasted image 20220202163729.png]]

## Семинар 














