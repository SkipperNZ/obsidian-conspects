[[base_dl]]

# Лекция 2


**План лекции**
* Повторение: логистическая регрессия через матричные перемножения.
* Полносвязные сети
* Обучение полносвязных сетей через метод обратного распространения  ошибки (backpropagation)


Сгенерировали два облачка 
![[Pasted image 20220205135444.png]]

Размерность у выборки (каждой точечки) - 2. Это координаты x и y
Размерность у параметров модели, которую будем учить (логистическая регрессия) - 3. это параметры выборки + bias. 
![[Pasted image 20220205135822.png]]

Что бы сделать их этого алгоритма классификацию, нужно добавить активационную функцию (сигмоиду итд). В нашем случае это пороговая функция sign.

Бинарная крос-энтропия (BSE):
Функция максимального правдоподобия
у - метка класса
p - предсказанная вероятность
$$\large
J = -(y\log (p)+(1-y)\log (1-p))
$$
Что-бы это хорошо работало, метки класса должны быть 0 или 1

После этого получается какое то число J

После этого обновляем веса таким образом:
$$\large
W^{t+1}=W^{t}-\eta \times \nabla Loss_{W}
$$



* как быть если классов больше чем 2?
* как быть если разделяющая поверхность нелинейная?  


Например если у нас 3 класса, размерность точки так же осталась 2, это x и y.
Матрица весов же теперь имеет размерность  3х2
$$\large
\begin{bmatrix}
w_{00} \ w_{01} \\
w_{10} \ w_{11} \\
w_{20} \ w_{21} \\
\end{bmatrix}
\times
\begin{bmatrix}
x \\
y \\
\end{bmatrix}
+
\begin{bmatrix}
b_{0} \\
b_{1} \\
b_{2}
\end{bmatrix}
=
\begin{bmatrix}
z_{0} \\
z_{1} \\
z_{2}
\end{bmatrix}
$$
Как слать так, что бы в финальном векторе содержались вероятности принадлежности к 3м классам. 

Для  этого нужно использовать softmax

$$\large
softmax\left (
\begin{bmatrix}
w_{00} \ w_{01} \\
w_{10} \ w_{11} \\
w_{20} \ w_{21} \\
\end{bmatrix}
\times
\begin{bmatrix}
x \\
y \\
\end{bmatrix}
+
\begin{bmatrix}
b_{0} \\
b_{1} \\
b_{2}
\end{bmatrix}
\right )
=
\begin{bmatrix}
z_{0} \\
z_{1} \\
z_{2}
\end{bmatrix}
$$

softmax - функция, которая берёт на входе вектор длинны n, и применяет к каждому её элементу z следующую функцию:
$$\large
\sigma(\vec{z})_{i} = 
\frac{e^{z_i}}{\sum\limits_{j=1}^{K} e^{z_{j}}}
$$
где:
$\large \sigma$ - softmax
$\large \vec{z}$ - input vector
$\large e^{z_i}$ - standard exponential function for input vector
$\large K$ - number of classes in the multi-class classifier
$\large e^{z_j}$ - standard exponential function for input vector

То есть
















































































































