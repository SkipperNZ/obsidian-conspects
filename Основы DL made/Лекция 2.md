[[base_dl]]

# Лекция 2


**План лекции**
* Повторение: логистическая регрессия через матричные перемножения.
* Полносвязные сети
* Обучение полносвязных сетей через метод обратного распространения  ошибки (backpropagation)


Сгенерировали два облачка 
![[Pasted image 20220205135444.png]]

Размерность у выборки (каждой точечки) - 2. Это координаты x и y
Размерность у параметров модели, которую будем учить (логистическая регрессия) - 3. это параметры выборки + bias. 
![[Pasted image 20220205135822.png]]

Что бы сделать их этого алгоритма классификацию, нужно добавить активационную функцию (сигмоиду итд). В нашем случае это пороговая функция sign.

Бинарная крос-энтропия (BSE):
Функция максимального правдоподобия
у - метка класса
p - предсказанная вероятность
$$\large
J = -(y\log (p)+(1-y)\log (1-p))
$$
Что-бы это хорошо работало, метки класса должны быть 0 или 1

После этого получается какое то число J

После этого обновляем веса таким образом:
$$\large
W^{t+1}=W^{t}-\eta \times \nabla Loss_{W}
$$



* как быть если классов больше чем 2?
* как быть если разделяющая поверхность нелинейная?  


Например если у нас 3 класса, размерность точки так же осталась 2, это x и y.
Матрица весов же теперь имеет размерность  3х2
$$\large
\begin{bmatrix}
w_{00} \ w_{01} \\
w_{10} \ w_{11} \\
w_{20} \ w_{21} \\
\end{bmatrix}
\times
\begin{bmatrix}
x \\
y \\
\end{bmatrix}
+
\begin{bmatrix}
b_{0} \\
b_{1} \\
b_{2}
\end{bmatrix}
=
\begin{bmatrix}
z_{0} \\
z_{1} \\
z_{2}
\end{bmatrix}
$$
Как слать так, что бы в финальном векторе содержались вероятности принадлежности к 3м классам. 

Для  этого нужно использовать softmax

$$\large
softmax\left (
\begin{bmatrix}
w_{00} \ w_{01} \\
w_{10} \ w_{11} \\
w_{20} \ w_{21} \\
\end{bmatrix}
\times
\begin{bmatrix}
x \\
y \\
\end{bmatrix}
+
\begin{bmatrix}
b_{0} \\
b_{1} \\
b_{2}
\end{bmatrix}
\right )
=
\begin{bmatrix}
z_{0} \\
z_{1} \\
z_{2}
\end{bmatrix}
$$

softmax - функция, которая берёт на входе вектор длинны n, и применяет к каждому её элементу z следующую функцию:
$$\large
\sigma(\vec{z})_{i} = 
\frac{e^{z_i}}{\sum\limits_{j=1}^{K} e^{z_{j}}}
$$
где:
$\large \sigma$ - softmax
$\large \vec{z}$ - input vector
$\large e^{z_i}$ - standard exponential function for input vector
$\large K$ - number of classes in the multi-class classifier
$\large e^{z_j}$ - standard exponential function for input vector

То есть конкретный i-тый элемент этого вектора $\large e^{z_i}$ мы возводим в экспоненту, и делим на сумму всех экспонент, то есть экспоненты всех векторов.
Таким образом гарантируется что на выходе у softmax в каждой ячейке будет положительное число, оно будет в диапазоне 0<число<1, и при этом за счет деления на общий знаменатель, все эти числа суммируются к единице.

```python
import numpy as np
def softmax(xs):
	return np.exp(xs)/ sum(np.exp(xs))

xs = np.array([-1, 0, 3, 5])
print(softmax(xs))
>>> [0.0021657, 0.00588697, 0,11824302, 0.87370431]
```

p.s даже с нампаевской суммой такая функция будет работать плохо, она будет очень нестабильно работать с определённым пулом значений. Внутри pytorch всё организовано сильно лучше.


**Если проблема нелинейная** 

Надо использовать многослойную сеть. 
![[Pasted image 20220205172705.png]]

**Рассмотрим подробно метод обратного распространения ошибки** 
$\large x = input$
$\large z = Wx+b_1$
$\large h= ReLU(z)$
$\large \theta = Uh+b_{2}$ 
$\large \hat{y} =softmax(\theta)$ 
$\large J = CE(y, \hat{y})$ 


Градиенты, которые нужно просчитать: 

$$\large 
 \frac{dJ}{dU} \quad \frac{dJ}{db_{2}} \quad \frac{dJ}{dW} \quad \frac{dJ}{db_{1}}
$$
Считаем за счет chain rule:
$$\large 
 \frac{dJ}{dU} =  \frac{dJ}{d\hat{y}} \frac{d\hat{y}}{d\theta} \frac{d\theta}{dU}
$$
$$\large 
 \frac{dJ}{db_{2}} =  \frac{dJ}{d\hat{y}} \frac{d\hat{y}}{d\theta} \frac{d\theta}{db_{2}}
$$

Все эти 3 частные производные, мы уже знаем, когда сделали forward pass через нашу сеть. 
Каждый раз когда мы считали $\large \theta = Uh+b_{2}$ , мы уже знали $\large \frac{d\theta}{dU}$ итд

Что такое chain rule:
$$\large
\frac{d}{dx} \left [ f \left ( g(x) \right ) \right ] =
f'(g(x))g'(x)
$$
Ну и на простом примере:
![[Pasted image 20220206113037.png]]

Как же посчитать bacpropagation во время forward pass.

Разберём простую функцию:
$$\large
f(x,y,z) = (x+y)z
$$
И возьмем входные данные: 
x = -2
y = 5
z = -4
В виде блок-схемы это выглядит так:
![[Pasted image 20220206133805.png]]

В итоге хотим знать результаты:
$$\large 
 \frac{df}{dx} \quad \frac{df}{dy} \quad \frac{df}{dz}
$$
Для этого надо знать все градиенты по дороге, так как мы будем умножать по очереди все частные производные.

Уже на первом этапе forward pass мы можем знать  результаты 
$$\large 
 \frac{dq}{dx} = 1 \quad \frac{dq}{dy} = 1 
$$
так как $\large q= x+y$ 

Функция f в данном случае равна:
$$\large
f =qz
$$
$$\large 
 \frac{df}{dq} = z \quad \frac{df}{dz} = q 
$$
Теперь считаем:
$$\large
\frac{df}{df} = 1
$$
И теперь берём и расставляем красные циферки которые считали на предыдущих шагах:
$$\large 
 \frac{df}{dq} = z = -4
$$
$$\large 
\frac{df}{dz} = q = 3
$$

$$\large 
\frac{df}{dy} =\frac{df}{dq} \frac{dq}{dy} = -4 \cdot 1 = - 4
$$
и аналогично:
$$\large 
 \frac{df}{dx} = \frac{df}{dq} \frac{dq}{dx} = -4 \cdot 1 = - 4
$$
![[Pasted image 20220206145538.png]]

Теперь честно, руками посчитаем backprop на частичке нашего примера для многомерного случая: 

$\large \theta = Uh+b_{2}$ 
$\large \hat{y} =softmax(\theta)$ 
$\large J = CE(y, \hat{y})$ 

уже на первом шаге, когда мы прогоняем данные через веса, мы можем просчитать:
$$\large
\frac{d\theta}{dU}
$$
$$\large
\frac{d\theta}{db_{2}}
$$
Следующий шаг, применяем softmax
$$\large
\frac{d\hat{y}}{d\theta}
$$
Теперь кросс-энтропия:
$$\large
\frac{dJ}{d\hat{y}}
$$
И теперь, то единственное что осталось сделать, что бы просчитать итоговые градиенты:
$$\large
\frac{dJ}{dU} =\frac{dJ}{d\hat{y}} \frac{d\hat{y}}{d\theta}  \frac{d\theta}{dU}
$$
$$\large
\frac{dJ}{db_{2}} =\frac{dJ}{d\hat{y}} \frac{d\hat{y}}{d\theta} \frac{d\theta}{db_{2}}
$$


В пайторче вообще существует 2 режима работы сетки, это: 
train - сохраняет градиены при прямом прогоне, для дальнейшего обучения.
eval - не сохраняет градиенты (with torch no grad)

**С одномерным случаем разобрались, теперь попробуем 2мерный** 

Рассмотрим функцию $f$ которая берёт на вход вектор длинны n и возвращает вектор длины m
$$\large
f: \mathbb{R}^{n}\rightarrow \mathbb{R}^m
$$
в формальной записи:
$$\large
f(x) = [f_{1}(x_{1}, ... ,x_{n}),f_{2}(x_{1}, ... ,x_{n}),..., f_{m}(x_{1}, ... ,x_{n})]
$$
просто имеем в функции f(x) m функций через запятую.
Вот пример
$$\large
f(x) = 
\begin{bmatrix}
w_{01} \ w_{02} \ w_{03} \\
w_{11} \ w_{12} \ w_{13} \\
w_{21} \ w_{22} \ w_{23} \\
w_{31} \ w_{32} \ w_{33} \\
\end{bmatrix}
\times
\begin{bmatrix}
x_{0}\\
x_{1} \\
x_{2} \\
\end{bmatrix}
$$

Производная такой функции - матрица m x n где в каждой ячейке записана частная производная.
$$\large
\frac{df}{dx} = 
\begin{bmatrix}
\frac{df_{1}}{dx_{1}} \ ... \ \frac{df_{1}}{dx_{n}} \\
... \ ... \ ... \\
\frac{df_{m}}{dx_{1}} \ ... \ \frac{df_{m}}{dx_{n}} \\
\end{bmatrix}
$$

Такая матрица частных производных называется Якобианом.

Рассмотрим 2мерный случай
$$\large
f(x) = [f_{1}(x), f_{2}(x)]
$$
$$\large
g(y) = [g_1(y_{1}, y_{2}), g_2(y_{1},y_{2})]
$$
$$\large
g(x) = [g_{1}(f_{1}(x), f_{2}(x)), \quad  g_2(f_{1}(x), f_{2}(x))]
$$
Это ровно то, что было в примере:
В данном случае f(x) это внутренняя функция а g(x) - внешняя

Теперь давайте посчитаем:
$$\large
\frac{dg}{dx} =
\begin{bmatrix}
\frac{d}{dx}[g_{1}(f_{1}(x), f_{2}(x)) \\
\frac{d}{dx}[g_{2}(f_{1}(x), f_{2}(x)) \\
\end{bmatrix}
=
\begin{bmatrix}
\frac{dg_{1}}{df_{1}} \frac{df_{1}}{dx} + \frac{dg_{1}}{df_{2}} \frac{df_{2}}{dx}\\

\frac{dg_{2}}{df_{1}} \frac{df_{1}}{dx} + \frac{dg_{2}}{df_{2}} \frac{df_{2}}{dx}\\
\end{bmatrix}
$$
Откуда взялись плюсы. (обещали скинуть ссылку в чате)

$$\large
\frac{dg}{dx}= \frac{dg}{df} \frac{df}{dx} =
\begin{bmatrix}
\frac{dg_{1}}{df_{1}} \ \frac{dg_{1}}{df_{2}} \\
\frac{dg_{2}}{df_{1}} \ \frac{dg_{2}}{df_{2}} \\
\end{bmatrix}

\begin{bmatrix}
\frac{df_{1}}{dx} \\
\frac{df_{2}}{dx} \\
\end{bmatrix}
$$

В качестве упражнения рассчитаем градиент умножения матрицы на вектор:
$$\large
z = Wx
$$
$$\large
\frac{dz}{dx} -?
$$
При этом каждый i-тый элемент матрицы z :
каждый элемент этого вектора это сумма элементов строчки k из матрицы W помноженных на k-тый элемент из x
$$\large
z_{i} = \sum\limits_{k=1}^{m}W_{ik}x_{k}
$$
Честно запишем как i-тый элемент вектора z зависит от j-того элемента вектора x:
$$\large
(\frac{dz}{dx})_{ij}=
\frac{dz_{i}}{dx_{j}}=
\frac{d}{dx_{j}}\sum\limits_{k=1}^{m}W_{ik}x_{k}=
\sum\limits_{k=1}^{m}W_{ik}\frac{d}{dx_{j}} x_{k} = 
W_{ij}
$$
В сумме единственное слагаемое , которое не будет нулевым это то в котором k = j, а, производная в таком случае равна единице
$$\large
\frac{dz}{dx} =W
$$

**Резюме**
*  Полносвязные сети (Fully-connected neural networks) - сеть из N блоков, где каждый блок состоит из линейной операции (умножение на матрицу) и нелинейной функции активации
*  Примеры нелинейных функций: tanh, sigmoid, softmax, relu
*  Backpropagation - рекурсивный алгоритм подсчета градиентов в сети, использующий chain rule
*  forward pass - подсчет выходов из каждого слоя сети
*  backward pass - подсчет градиентов и обновление весов сети



































































