[[base_dl]]

# Лекция 3

План лекции: 
1. Постановка задачи оптимизации в машинном обучении
2. Сложности оптимизации глубоких нейронных сетей
3. Методы оптимизации
4. Learning rate
5. Регуляризация
6. Адаптация нейронных сетей с помощью оптимизации


### Постановка задачи оптимизации в машинном обучении 

Пайплайн алгоритма машинного обучения.

* Сборка данных для обучения
* Понять и выбрать метрики для оценки нашего алгоритма
* Выбор модели
* подбор весов, что бы метрики росли.

Накладываются ограничения по вычислительным ресурсам, времени.
![[Pasted image 20220220132354.png]]

Как подбирать параметры?
Для того что бы это правильно делать, нужно правильно подобрать функцию потерь.
Их много, самые известные:
![[Pasted image 20220220132637.png]]
Бинарная, простая либо 1 либо 0 за ответы
Хинж лосс более сложная, применяется в svm

У нас есть данные и давайте рассмотрим понятие:
**Минимизация эмпирического риска** - минимизация нашей функции потерь по тем данным которые у нас есть.
f - некий набор семейства алгоритмов, которые могут например задаваться одной структурой (все полносвязные нейронные сети с 2мя скрытыми слоями) но отличающиеся между собой параметрами(весами)
L(f) - функция  как среднее значение по всем данным xi и yi 
![[Pasted image 20220220133237.png]]

Метод максимального правдоподобия 
![[Pasted image 20220220134209.png]]

Любой алгоритм классификации имеет в среднем по всем возможным распределениям, порождающим данные, одинаковую вероятность ошибки при классификации ранее не наблюдаемой точки данных.

К счастью, нам не нужно строить универсальный алгоритм машинного обучения.

Нам нужно понять, какое распределение данных лучше всего подходит к решаемой нами задаче, и какой алгоритм машинного обучения лучше всего подойдёт для таких данных.

![[Pasted image 20220220141750.png]]


**Универсальная теорема аппроксимации**

Нейронная сеть прямого распространения сигнала с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью.

Но для этого может понадобится огромное количество нейронов в этом скрытом слое.

Поэтому используют несколько слоёв, что влечёт ряд сложностей.
![[Pasted image 20220220142609.png]]

Размытие градиента 
Когда в качестве активации используется сигмоида. 
![[Pasted image 20220220144658.png]]
красная линия - производная сигмоиды
![[Pasted image 20220220144602.png]]
Видим что только когда значения х близки к нулю, значения производной синмоиды значимо отлично от нуля.
Это порождает проблему размытия градиента, что когда мы по chane rule по backpropagation идём к началу сетки, там мы используем значения градиентов с предыдущих весов. И если в какой то момент получится что производные сигмоиды окажутся близко к нулю, то все дальнейшие значения мы будем умножать на почти нулевые значения, и ошибка почти не будет распространятся на первые слои 
Варианты борьбы - другие функции активации ReLU
Обратная ситуация - взрыв градиента. 

### Gradient descent градиентный спуск
![[Pasted image 20220220150314.png]]

Его модификации реально применяемые на практике: 

**Stochastic gradient descent (SGD) стохастический градиентный спуск**

![[Pasted image 20220220150603.png]]


**Mini-batch gradient descent** 
![[Pasted image 20220220151602.png]]

**Momentum**
![[Pasted image 20220220152302.png]]
Тут вводится ускорение v (метод тяжелого шарика)


**Nesterov Momentum**
![[Pasted image 20220220152555.png]]

Значения lerning rate
![[Pasted image 20220220153019.png]]

![[Pasted image 20220220153227.png]]



**Адаптивный подбор learning rate**

Следующие методы адаптивно подбирают его, обратно пропорционально квадратному корню величины градиента:

* Adagrad (Duchi et al., 2011)

• Adadelta (Zeiler, 2012)

• RMSprop (Hinton, 2016)

• AdaBelief (Zhuang, 2020)

Самый влиятельный метод - это **ADAM Adaptive Moment Estimation** 

Он использует первый и второй моменты градиента.

Значение градиента:
$$\large
g_{t}=\nabla_{\theta}J(\theta) 
$$

Первый(m) и второй(v) моменты:
$$\large
m_{t}= \beta_{1}m_{t-1}+(1-\beta_{1})g_{t}
$$
$$\large
v_{t}= \beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}
$$
$\large \beta_{1}$ и $\large \beta_{2}$ -параметры алгоритма


Поправки моментов на смещение вычисляются вот так: 

$$\large
\hat{m}_{t}= \frac{m_{t}}{1-\beta_{1}^{t}}
$$
$$\large
\hat{v}_{t}= \frac{v_{t}}{1-\beta_{2}^{t}}
$$
у беты в t не в степени, это t-ый номер. Чаще всего это константа и она одна на все итерации. 

Тогда значения параметров на следующем шаге:
$$\large
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}\hat{m}_{t}
$$
$\large \eta$ - как я понял изначальный лёрнинг рейт
$\large \epsilon$ - параметр на всякий случай что бы не было деления на 0

* $\large \beta_{1}=0.9$ , $\large \beta_{2}= 0.999$ , $\large \epsilon = 10^{-8}$  
* наиболее успешный (в среднем) на практике метод

### Без использования градиента (прямые методы)

![[Pasted image 20220220195740.png]]

![[Pasted image 20220220200355.png]]

### Регуляризация
![[Pasted image 20220220205653.png]]
![[Pasted image 20220220205734.png]]

![[Pasted image 20220220205830.png]]

![[Pasted image 20220220210144.png]]

## Семинар pyTorch

импорты
```python 
import torchvision # штуки для работы с картинками в торче (датасеты и прочее)

from torch import nn # имеет внутри себя все модули нейронных сетей

import torchvision.transforms as transforms #стандартные преобразования трансформс для аугментации входных данных.

```

Базовые операции в торче.
```python 
x = torch.arange(25).reshape(5, 5).float() # Создаём матричку 5на5
x.shape # печать формы матрицы
x * x # поэлементно возвести в квадрат
torch.matmul(x, x.T) # матричное умножение x на x транспонированое
torch.mean(x, axis=0)  # Cреднее по столбцам
torch.cumsum(x, axis=1) # Сумма по строкам
```


Pytroch сам считает backpropagation для нас с помощью модуля autograd
```python 
# создаем тензор нулей
preds = torch.zeros(5, requires_grad=True)

# вектор предсказаний единиц
labels = torch.ones(5, requires_grad=True)

# loss: MAE
loss = torch.mean(torch.abs(labels - preds))

# запускаем backprop
loss.backward()

# градиенты доступны в поле .grad: как я понял это 
# d loss / d preds
preds.grad

# градиенты можно занулить
preds.grad.zero_()

```

 Градиенты накапливаются при каждом вызове backward()
```python 
# создаем тензор
preds = torch.zeros(5, requires_grad=True)

  
# вектор предсказаний
labels = torch.ones(5, requires_grad=True)

  
# loss: MAE
loss = torch.mean(torch.abs(labels - preds))

  
# запускаем backprop

for i in range(5):
	loss.backward(retain_graph=True)
	print (i, preds.grad)
```


линейный logreg на торче
```python 
model = torch.nn.Linear(2, 1) # Линейный слой принимает 2 значения, а выдаёт 1
criterion = torch.nn.BCELoss() # функция выдающая бинарную кросс-энтропию

optim = torch.optim.SGD(model.parameters(), lr=0.1) # Для оптимизации в торче есть отдельный пакет optim и в нем есть sgd
# параметры есть по умолчанию, и идут из коробки из модуля nn

  
model.train() # этим мы переводим нашу модель в состояние train

model
#выдаст: Linear(in_features=2, out_features=1, bias=True)

model.weight
#выдаст: Parameter containing:
#tensor([[ 0.3869, -0.1364]], requires_grad=True)

model.bias
#выдаст: Parameter containing:
# tensor([-0.2409], requires_grad=True)

model.weight.data.dtype
#выдаст: torch.float32

```

Обучение теперь выглядит так:
```python 
#цикл на тысячу шагов обучения
for i in range(1000):

	# считаем предсказание
	y_pred = torch.sigmoid(model(X.float()))

	# считаем лосс
	# flatten() что бы точно привести значения в вектор
	loss = criterion(y_pred.flatten(), y.float())

	# прокидываем градиенты
	# когда вызываем loss.backward(), считаются градиенты лосса по всем тензорам в памяти, у которых requires_grad=True. Т.е. по model.parameters(), которые мы в optim передали посчитается градиент при loss.backward(), а при optim.step() параметры обновятся с учетом новых полученных градиентов.
	loss.backward()

	# делаем шаг оптимизатором его мы тоже заводили в ячейке выше
	# optim знает о модели так как мы передали её в него когда её создавали
	optim.step()

	# зануляем градиенты
	optim.zero_grad()

	#дальше код отрисовки

```




Далее в пытаемся линией разделить нелинейные данные:
все то же самое но добавили моментум просто напрямую его туда закинув:
```python 
optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```



Что бы сделать сложную нейронку нужно:
```python 
from torch import nn # импортировать модуль nn

model = nn.Sequential() # создаём контейнер

# добавляем слои
model.add_module('first', nn.Linear(2, 2))
model.add_module('first_activation', nn.Sigmoid())
model.add_module('second', nn.Linear(2, 1))

```



Дальше потихоньку усложняли модель и вот самая сложная для 2хклассовой:
```python 

criterion = torch.nn.BCELoss()
optim = torch.optim.SGD(model.parameters(), lr=2, momentum=0.9)
model.train()

# если хотим включить куда:
model.to('cuda:2')

model = nn.Sequential()

model.add_module('first', nn.Linear(2, 5))
model.add_module('first_activation', nn.Sigmoid())
model.add_module('second', nn.Linear(5, 5))
model.add_module('second_activation', nn.Sigmoid())
model.add_module('third', nn.Linear(5, 1))
model.add_module('third_activation', nn.Sigmoid())
```
И так как мы добавили в конце сигмоиду, предикты можно получать без неё:
```python 
 y_pred = model(X.float())

 loss = criterion(y_pred.flatten(), y.float())
 loss.backward()
 optim.step() 
 optim.zero_grad()
```

Теперь делаем больше 2х классов:
в конце добавляем на выходе 3 класса
```python 

model = nn.Sequential()
model.add_module('first', nn.Linear(2, 5))
model.add_module('first_activation', nn.Sigmoid())
model.add_module('second', nn.Linear(5, 5))
model.add_module('second_activation', nn.Sigmoid())
model.add_module('third', nn.Linear(5, 3))

# и используем кросс энтропийный лосс 
criterion = torch.nn.CrossEntropyLoss()
optim = torch.optim.SGD(model.parameters(), lr=0.1)
model.train()
```
Получается так:
![[Pasted image 20220221172551.png]]

если хотим добавить регуляризацию, то она добавляется в optim через строчку weight_decay=0.1   в данном случае это L2
```python 
optim = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.1)
```


Так же можно добавить в модель дропаут между слоёв
(вроде как работает для предыдущего слоя):
```python 
model.add_module('drop', nn.Dropout())
```


и у него же можно добавлять вероятносный параметр p
```python 
model.add_module('drop', nn.Dropout(p=0.2))
```

а что бы заморозить слои для трансфер лёрнинга
надо заморозить слои через 
`requires_grad=False`

Перевод модели в состояние работы а не обучения 
автомавтически становятся все `requires_grad=False`
```python 
model.eval()
```

или
```python 
with torch.no_grad():
	model(x)
```



















































































































































































