[[base_dl]]



# Лекция 4
План лекции:

* Как устроены изображения
* ML на изображениях
* Свертка
* Сверточный слой
* Пулинг
* CNN
* Итоги

**Как засунуть изображение в модель ML**

* Признаки = исходные значения яркости пикселей 
хреновый вариант (на примере распознавания рукописной циферки)  так как если распознаваемая циферка на картинке сместится в сторону то всё сломается 

* Признаки извлекаются вручную (hand-crafted) 
Тоже не очень, так делали раньше, извлекаются всякие статистики и уже с ними работают.

* Признаки извлекаются автоматически
Наш вариант который и будем рассматривать 

## Свёртка
Начнём с одномерного случая:
Имеется некоторый сигнал. Например, показания физического датчика за промежуток времени.

Сигнал явно шумный - как его "почистить"?
![[Pasted image 20220222041648.png]]

Применим к "сглаживание" с помощью скользящего среднего
* Выберем размер окна; например, 5
* Каждое значение сигнала заменим средним по окрестности размера 5; можно повторить несколько раз.

**Появились краевые эффекты - почему?**
Для первых и последних точек нет полного числа соседей, поэтому пришлось добавить нули по краям сигнала


Эта штука называется **паддинг**
![[Pasted image 20220222045048.png]]
![[Pasted image 20220222045137.png]]

По сути, что именно мы сделали? 
Свертку с ядром \[⅕, ⅕, ⅕, ⅕, ⅕\] 

Свертка - операция, при которой
* Ядро свертки "скользит" по входному сигналу
* В каждой точке приложения ядра вычисляется скалярное произведение
* Результат записывается в выходной сигнал

Дискретная формула свертки (1D) сигнала y с ядром w размера K:
$$\large
y_{i}=(x\cdot w)_{i} = \sum\limits_{k=0}^{K-1} x_{i+k} \cdot w_{k}
$$
Сглаживание сигнала в прошлом примере = одномерная свертка.
Свертка может быть и более высокой размерности
Размерность свертки = количество направлений, вдоль которых движется ядро свертки, проходя по сигналу
![[Pasted image 20220222045955.png]]

В двумерном случае: 
Воходной сигнал 4х4
Так как нет падингов, то сигнал на выходе меньше чем на входе.

![[Pasted image 20220222142017.png]]

![[Pasted image 20220222143220.png]]

Если не делать падинг с таким ядром то картинка похудеет на 2 пикселя по горизонтали и на 2 пикселя по вертикали.

**Свойства свёрток:**

* Ассоциативность: $\large a \cdot(b \cdot c) =(a \cdot b) \cdot c$ 
* Коммутативность:  $\large a \cdot b = b \cdot a$  
* Линейность:
	* $\large (a+b)\cdot c = a\cdot c +b \cdot c$
	*  $\large (ka \cdot b) =k(a \cdot b)$ 

В области обработки изображений используются свертки с явно заданными ядрами.

Что получится, если сделать веса ядер свертки обучаемыми?
Ответ: 	**Сверточный слой**
	
 ## Сверточный слой

* Сверточный слой состоит из нескольких (K >= 1) ядер свертки
	К каждому результату свертки можно добавить bias
* На вход подается тензор X размера HxWxD
* Каждое из K ядер сворачивается с входным тензором
* В итоге получается новый тензор Y размера H'xW'xK
* Применив padding, можно сделать H = H', W = W'

В данном случае едро суммирует все значения по всем слоям и выдаёт 1 число. Следовательно на выходе после ядра будет просто двумерная "картинка". Это называется "Карта активации"

bias просто добавляется к скалярному произведению.
![[Pasted image 20220222154249.png]]

![[Pasted image 20220222154303.png]]


Сколько всего обучаемых весов у сверточного слоя, если:
* размер входного тензора HxWxD
* количество ядер K
* размер ядра 3x3
* используется bias?

Ответ $\large 3 \cdot 3 \cdot D$ -количество весов + 1 bias и всё это умножить на количество ядер K. Тогда 
Всего весов: $\large K \cdot (3 \cdot 3 \cdot D + 1)$

Сверточный слой обрабатывает входной сигнал локально "Взаимодействуют" только те ячейки, которые находятся рядом (попадают в одно окно свертки)

![[Pasted image 20220222160908.png]]

**Параметры свёрточного слоя:**
* Число ядер (оно же: "число фильтров", "ширина слоя")
* Размер ядра (3x3, 5x5, 3x5, …)
* Padding (добавление значений по краям входного сигнала)
![[Pasted image 20220222182243.png]]
* Stride (величина шага скользящего окна, бывает и >1)
![[Pasted image 20220222182328.png]]
* Dilation (добавление "разреженности" в матрицу ядра свертки)
![[Pasted image 20220222182445.png]]

Размер выходного тензора : $\large D_{out} \times H_{out} \times W_{out}$ 
$$\large
H_{out}=
\frac{(H_{in}- size + 2 \times padding)}{stride} +1
$$
$$\large
W_{out}=
\frac{(W_{in}- size + 2 \times padding)}{stride} +1
$$

**Рецептивное поле (receptive field)**

Рецептивное поле (receptive field) нейрона - размер области исходного сигнала (изображения), которая может вносить вклад в активацию данного нейрона

![[Pasted image 20220222183551.png]]

Как меняется РП выхода слоя при:
*  Увеличении ядер сверток? - Увеличивается, так как одна активация смотрит на большее "пятно"
* Увеличении dilation? - Растет
* Добавлении padding? -не меняется, просто на выходе становится больше активаций в штуках


Сверточный слой в торче:
```python
import torch

conv_layer = torch.nn.Conv2d()
```
![[Pasted image 20220222190815.png]]


## Разберем теперь backward pass
![[Pasted image 20220222191110.png]]

Итак: 
Есть двумерное ядро свёртки размера 3

Свернули этим ярдом x получили y меньшей длинны.

Нужно сделать обновление для веса $\large w_{j}$ на этом слое.
Для этого надо вычислить:
$$\large
\frac{dL}{dw_{j}} = 
\sum\limits_{i}\frac{dL}{dy_{i}} \times 
\frac{dy_{i}}{dw_{j}}
$$
Распишем выходные $\large y_{i}$
$$\large
y_{i}=\sum\limits_{k=0}^{K-1}x_{i+k}w_{k}
$$
Распишем для наглядности все y
$$\large
y_{0}= x_{0}\cdot w_{0} +x_{1}\cdot w_{1} +x_{2}\cdot w_{2}
$$
$$\large
y_{1}= x_{1}\cdot w_{0} +x_{2}\cdot w_{1} +x_{3}\cdot w_{2}
$$
$$\large
y_{2}= x_{2}\cdot w_{0} +x_{3}\cdot w_{1} +x_{4}\cdot w_{2}
$$
 И тут хорошо видно что производная i-ого у по j-тому w зависит только от х в позиции i+j
 $$\large
\frac{dy_{i}}{dw_{j}} = x_{i+j}
$$
тогда получаем:
$$\large
\frac{dL}{dw_{j}} = 
\sum\limits_{i}\frac{dL}{dy_{i}} \times 
x_{i+j}
$$
$$\large
\frac{dL}{dw_{j}} = 
(x \cdot \frac{dL}{dy})_{j}
$$


Чтобы получить сверточную нейросеть, помимо сверточного слоя понадобятся:
* Слои активации (Tanh, ReLU, …)
* Слои пулинга
* (опционально) Полносвязные выходные слои

## Пулинг

Проблема памяти
* Обычно ширина сверточных слоев растет по мере увеличения глубины сети . Например, в сети ResNet18 количество сверток в одном слое растет так: 64 - 128 - 256 - 512
* Чем больше размер тензоров, тем больше потребление памяти
* Слой Pooling позволяет уменьшать H/W тензоров

То есть мы как бы ужимаем тензоры по ширине и высоте, что бы дальше их ростить по глубине. 

Как делается:
* Разобьем тензор на части по ширине и высоте
* В каждой части независимо от других посчитаем 1 статистику (среднее - AveragePooling, максимум - MaxPooling)
* Склеим полученные статистики обратно в тензор
* Получится тензор прежней глубины, но меньшего размера по ширине и высоте
![[Pasted image 20220222202224.png]]
 В общем в привывчных терминах - зашакалим тензор.
 
**Global Pooling**
* Можно посчитать статистики сразу по всему каналу, а не по регионам
* В результате получится тензор размера 1х1хD, который можно отправить в полносвязный слой
* Это будет Global Pooling

- глобальный шакалинг.


## CNN

CNN = convolutional neural network

Типичная структура CNN:

((сверточный слой ⇒ активация) * k ⇒ пулинг) * m ⇒ линейный слой

В современных архитектурах как правило используются свертки 3х3

![[Pasted image 20220222203530.png]]

* Первый сверточный слой видит "сырые" данные. Его веса худо-бедно "понятны"
* Чем глубже в сеть, тем более абстрактным становится представление исходного объекта

**Итоги**

**Сверточный слой**
* Основан на операции свертки
* Сам "извлекает признаки"
* Хорошо работает для данных с локальной связностью
* Эффективнее полносвязного слоя по числу весов

**Сверточная нейросеть**

* Базовые слои - сверточный, активация, пулинг, полносвязный(е) в конце
* Чем глубже слой в сети, тем с более абстрактными признаками он работает


# Практика


перевод изображения в тензор
```python
to_tensor = torchvision.transforms.ToTensor()
img_tensor = to_tensor(img)
```


перевод из тензора в картинку:
```python
import torchvision.transforms as transforms

# функция, переводящее тензор в PIL-изображение
to_pil_image = transforms.ToPILImage()
output_img = to_pil_image(output_tensor.squeeze(0))
```


в pytorch есть команда, комбинирующая другие команды в 1
на задротском - макрос. как я понял.
тут у нас:
- перевод в тензор
- нормализация 
Normalize((среднее по 3м каналам которое вычитается), (стандартное отклонение на которое делится))
```python
transform = transforms.Compose(
	[transforms.ToTensor(),
	transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
```


работаем с датасетом cifar10:
в торчвижоне есть модуль datasets в котором уже есть cifar10
`root='./data'` - куда писать
`train=True` - тренировочная часть
`download=True` - если торч не найдет в руте датасет то будет качать
`transform=transform`  передаём трансформ ктотрый сделали выше
```python
# dataset задаёт данные
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
	download=True, transform=transform)
```

что бы это всё работало нужно еще обернуть в даталоадер:
он отвечает за то как будет механически происходить доставание данных.
содержится в `torch.utils.data.DataLoader`
передаём в него наш датасет
`batch_size=batch_size` -сколько картинок будет в батче
`shuffle=True` - нужно ли перемешать 
`num_workers=2` - сколько воркеров достающих данные
```python
# dataloader подгружает их
trainloader = torch.utils.data.DataLoader(trainset,
	batch_size=batch_size,shuffle=True, num_workers=2)
```

то же для тестового 
```python
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
	download=True, transform=transform)
	testloader = torch.utils.data.DataLoader(testset,batch_size=batch_size,shuffle=False, num_workers=2)
```


**Отрисуем**

Функция для отрисовки:
```python
def imshow(img):
	# убрать нормализацию
	img = img / 2 + 0.5
	npimg = img.numpy()
	plt.imshow(np.transpose(npimg, (1, 2, 0)))
	plt.show()
```


`iter` - одна итерация(вытаскивает наш батч)
`torchvision.utils.make_grid` - удобная утилита для отрисовки
```python
# взять случайный батч изображений

dataiter = iter(trainloader)
images, labels = dataiter.next()

imshow(torchvision.utils.make_grid(images))

print('  '.join('{}'.format(classes[labels[j]]) for j in range(4)))
```

**Начнем обучатся**
Сделаем рутину обучения отдельным методом.
`net` - сама модель
`criterion` - лосс функция 
`optimizer` - метод оптимизации знающий параметры модели и умеющий их оптимизировать.
`trainloader` - что бы получать данные
`num_epochs` - количество эпох
```python
def train_model(net, criterion, optimizer, trainloader, num_epochs=5):
```


не ясная конструкция 
```python
for i, data in enumerate(trainloader, 0):
```

**Точность модели на тестовом сете**

```python
def all_accuracy(net, testloader):

 _, predicted = torch.max(outputs.data, 1)

```

**Полносвязная сеть для классификации**

`import torch.nn.functional as F` - разные полезные слои, в том числе и активации.
Сеть пишется в виде класса
наследуется от `nn.Module`
в конструкторе задаётся 3 полносвязных линейный слоя(не свёрточных)
`x = x.view(-1, 3 * 32 * 32)`  -  входную картинку преобразует в вектор такого размера.
```python
import torch.nn as nn
import torch.nn.functional as F

class FeedForwardNet(nn.Module):
	def __init__(self):
		super(FeedForwardNet, self).__init__()
		self.fc1 = nn.Linear(3 * 32 * 32, 128)
		self.fc2 = nn.Linear(128, 32)
		self.fc3 = nn.Linear(32, 10)


	def forward(self, x):
		# tip: используйте функцию view для преобразования входа
		x = x.view(-1, 3 * 32 * 32)
		x = self.fc1(x)
		x = F.relu(x)
		x = self.fc2(x)
		x = F.relu(x)
		x = self.fc3(x)

		return x


net = FeedForwardNet()

```

**Зададим функцию потерь**

```python
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
```

### Свёрточная сеть для классификации

Тоже класс только с небольшими изменениями:
`self.conv1 = nn.Conv2d(3, 6, 5)` - свертка 
3 - rgb 
6 - на выходе глубина тензора 
5  - kernel_size
`self.pool = nn.MaxPool2d(2, 2)`  - пулинг
`x = self.pool(F.relu(self.conv1(x)))` - подаём картинку в свёрточный слой , применяем к нему reLu и делаем пулинг.
и потом херачим линейные слои как и раньше
```python
def __init__(self):
	super(Net, self).__init__()
	self.conv1 = nn.Conv2d(3, 6, 5)
	self.pool = nn.MaxPool2d(2, 2)
	self.conv2 = nn.Conv2d(6, 16, 5)
	self.fc1 = nn.Linear(16 * 5 * 5, 120)
	self.fc2 = nn.Linear(120, 84)
	self.fc3 = nn.Linear(84, 10)


def forward(self, x):
	x = self.pool(F.relu(self.conv1(x)))
	x = self.pool(F.relu(self.conv2(x)))
	x = x.view(-1, 16 * 5 * 5)
	x = F.relu(self.fc1(x))
	x = F.relu(self.fc2(x))
	x = self.fc3(x)
	return x

```


**Сохранение и загрузка сети**
```python
torch.save(net.state_dict(), PATH)

# загрузим сеть

net = Net()
net.load_state_dict(torch.load(PATH))
```


**Cuda**
```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

net.to(device)

inputs, labels = data[0].to(device), data[1].to(device)
```
Для работы на нескольких видеокартах есть специальные модули типа `nn.DataParallel(net)`


## Transfer learning


**Предобратотка данных**
`RandomResizedCrop` - обрезает до заданного размера случайным образом (квадратным), для прямоугольника надо 2 числа
`RandomHorizontalFlip` - случайно делает горизонтальный переворот(с вероятностью 0.5 по умолчанию)
`transforms.Resize(256)` - 
`transforms.CenterCrop(224)`
такие параметры нормалайза используются в imageNet
```python
data_transforms = {
	'train': transforms.Compose([
		transforms.RandomResizedCrop(224),
		transforms.RandomHorizontalFlip(),
		transforms.ToTensor(),
		transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
	]),

	'val': transforms.Compose([
		transforms.Resize(256),
		transforms.CenterCrop(224),
		transforms.ToTensor(),
		transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

 ]),

}
```


**Рутина обучения**
`scheduler` - планировщик. вроде меняет лёрнинг рейт
```python
def train_model(model, criterion, optimizer, scheduler, num_epochs=25):

 best_model_wts = copy.deepcopy(model.state_dict()) # копируем параметры лучшей модели что бы если что откатить до неё

```

Собственно сам трансфер лёрнинг:
 Предобученный на ImageNet ResNet-18

`device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")` - напоминалка что такое девайс
```python
model_ft = models.resnet18(pretrained=True) # предобученая модель
num_ftrs = model_ft.fc.in_features # последний слой модели (количество переменных которые в него приходят)

# заменяем последний слой на свой слой
model_ft.fc = nn.Linear(num_ftrs, 2) # в него входят фичи из предыдущих слоёв а выходят только 2

model_ft = model_ft.to(device) # переводим на тот девайс который нам нужен

criterion = nn.CrossEntropyLoss()

optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

# в качестве лернинг рейта используем степ лёрнинг рейт
# через каждые step_size=2 эпохи умножает лернинг рейн на 0.1 
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1)

```

**В начале дообучиваем все слои. Это долго**

```python
model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)

visualize_model(model_ft)
```

 **Теперь до обучаем только последний слой**

```python
model_conv = torchvision.models.resnet18(pretrained=True)

for param in model_conv.parameters():
	# градиенты теперь не будут здесь протекать
	param.requires_grad = False

	# так же создаём новый слой
	num_ftrs = model_conv.fc.in_features
	# по умолчанию requires_grad = True
	model_conv.fc = nn.Linear(num_ftrs, 2)

	# далее вроде так же
	criterion = nn.CrossEntropyLoss()

	optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)

	exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=2, gamma=0.1)

```


учим и смотрим что на выходе:
```python
model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=5)

visualize_model(model_conv)
```



Всё!