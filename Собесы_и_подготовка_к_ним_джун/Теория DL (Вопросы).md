Тут Будут разобраны теоретические вопросы по DL которые могут попадаться на собеседовании.

```toc
```

В лекции 4 base DL backprop по свертке и то как собрать модель самому

### Кэширование градиентов 
![[Pasted image 20220609032426.png]]

### Логистическая регрессия

Функция сигмоиды:
$$\large
f(z) = \frac{1}{1+e^{-z}}
$$
Производная сигмоиды
$$\large
\sigma(x)` = \sigma(x)(1-\sigma(x))
$$
Где 
$$\large
z = \theta^{T}x = \theta_{0} + \theta_{1}x_{1}+...+
\theta_{n}x_{n}
$$
![[Pasted image 20220601030137.png]]


### Softmax
![[Pasted image 20220609030554.png]]
```python
import numpy as np
def softmax(xs):
	return np.exp(xs)/ sum(np.exp(xs))

xs = np.array([-1, 0, 3, 5])
print(softmax(xs))
>>> [0.0021657, 0.00588697, 0,11824302, 0.87370431]
```


### Активационные функции
Основная суть функций активации - внести в преобразования нелинейность

![[Pasted image 20220601030611.png]]

**Сигмоида** 
-  мапит линейное пространство R в (0,1)
Минусы 
* на концах слева и справа, почти нулевой градиент.
* сдвигает среднее значение - центр не в 0 а в 0.5 (если до сигмоиды среднее значение было 0, то после сигмоиды среднее будет явно больше или равно 0)
* Экспаненту вычислительно трудно считать

**Гиперболический тангенс** 
Решает проблему со сдвигом среднего а в остальном как сигмоида

**Relu**
$\large f(a)=\max(0,a)$
* Очень легко считать.
* не затухают градиенты справа от нуля
Минусы
* нулевой градиент при x < 0
* Сдванутый относительно нуля выход 

**leaky Relu**
$\large f(a)=\max(0.01a,a)$
* Очень легко считать.
* не затухают градиенты справа от нуля и не ноль слева
Минусы:
* Все так же сдвигает но не так сильно среднее

**ELU**
редко используют
![[Pasted image 20220602035629.png]]


### Backpropagation and chain rule
![[Pasted image 20220601031810.png]]
![[Pasted image 20220602021652.png]]

То же на примере логистической регрессии
![[Pasted image 20220602023427.png]]

В матричном виде выглядит так:
![[Pasted image 20220602023849.png]]
![[Pasted image 20220602023952.png]]

### Optimizers

![[Pasted image 20220602024122.png]]

Градиентный спуск можно делать по всей выборке, но это очень дорого можно это делать супер долго.

Существует куда больше оптимизаций:
* Momontum
* Adagrad
* Adadelta
* RMSprop
* Adam
* Даже другие нейронки

![[Pasted image 20220607215307.png]]
  
### SGD
Посчитали лосс на N объектах,  усреднили градиенты и пошли вперёд.
На маленьких батчах довольно шумно. И если наш батч маленький может быть сильно шумнее.
![[Pasted image 20220607215702.png]]

### Momentum
Так как у нас все таки хоть и градиентный, но спуск. то возможна вот такая идея:
Метод тяжелого шарика. 
Это когда у нас есть накопленное значение усредненного градиента за последние k шагов. 
![[Pasted image 20220607220312.png]]
Плюсы:
* Можем избегать локальных оптимумов. 


### Nesterov momentum
(заглядывание в будующее)
Предлагается посчитать градиент не в текущей точке, а в той точке, куда мы попадём проскользив по вектору ускорения и уже в той точке его посчитаем, вычтем и сделаем шаг.
![[Pasted image 20220608012708.png]]

В седловых точках моментум и sgd чувствует себя неочень. 
![[Pasted image 20220608024915.png]]


### Adagrad: SGD with cache
**Идея**
разные измерения - разные

Давайте нормировать градиент на то как сильно меняется градиент от шага к шагу.
Если значение функции сильно влияет на значение функции потерь значит нужно её настраивать аккуратно что бы оптимум не проскочить, если значение слабо влияет то особо нет разницы на сколько мы ходим. 
(Кэш - вектор размера градиента, а каждая координата этого вектора - отдельная нормировочная константа)
![[Pasted image 20220608025652.png]]

Проблема: потихоньку угасает лёрнинг рейт.

### RMSProp SGD with cache with exp. Smoothing
Позволяет более точно оценивать градиент в зависимости от того большой ли у нас уклон или нет.
Идея: 
А давайте не только наращивать кеш, а его сглаживать. 

![[Pasted image 20220608030221.png]]


### Adam (скрестить Rmsprop и momentum)
Лучшее решение по умолчанию.
![[Pasted image 20220608030913.png]]


Отсылка к карпатому 
![[Pasted image 20220608031412.png]]

Все оптимайзеры кроме SGD дополнительно отжирают память.





### Data Normalization
Есть коричневый элипс данных, мы можем сделать:
1) Нулевое среднее для каждой из осей. 
2) Дисперсию по осям отнормировать.
![[Pasted image 20220608042401.png]]
Для чего это нужно? 
* Градиент для веса 2ого признака будет сильно больше чем для первого.
* В задачах классификации если элипс сильно вытянут то может просто нехватить машинной точности что бы разделять классы. Там изменение даже одного бита приводит к сильному изменению прямой. 
* Градиенты могут быть сильно большие. 

Нормировать надо с умом, если признаки данных зависимы то их надо нормировать одинаково. (например 2 признака в одной и той же шкале(микрофарады, вольты итд), допустим на одном датчике значение менялось от 7 до 790 а во 2ом от 10 до 12, после нормировки оба меняются от -1 до 1 в каких то попугаях. физический смысл этих значений поменялся и было важно что они значили) 

### Weight initialization
**Если инициализировать все веса нулями.**
Тогда для всех нейронов обновления будут одинаковыми, потому что у них у всех нулевой вклад. и для всех весов одинаково поменяется градиент. и наша модель из размера слоя 50 будет равна одному нейрону.
Но на практике все будет работать из за накопленной ошибки, но лучше так не делать.

**Если инициализировать рандомными маленькими числами**
Есть тоже проблема,  дисперсия у весов будет расти и устойчивость модели будет снижаться.
![[Pasted image 20220608045748.png]]


**Надо случайные числа которые не просто из случайных значений а из распределения у  которого дисперсия 1/n**

В торче из коробки всё работает неплохо.


### Batch normalization
Он позволил тренировать сети гораздо быстрее чем без него
Батчнорм это просто стандартизация данных, но не на входе, а в промежуточных представлениях. 
(средние считается по батчу)
![[Pasted image 20220608050439.png]]

По умолчанию используется conv -> batchNorm -> reLu 
но работает и после релу неплохо.
В оригинале добавляется еще гамма  растяжение и бета сдвиг(последняя строчка и эти значения выучивались.)
Это позволяло уйти из линейности в нелинейность
![[Pasted image 20220608052648.png]]
![[Pasted image 20220608053224.png]]

Такое ускорениек обучение получается из за того, что стало возможно безболезнено повысить лернинг рейт.

**На инференсе**
Батч получается размером 1 (так как надо обработать 1 конкретный объект)
Поэтому мы храним значения среднего и дисперсии с трейна и используем их.
![[Pasted image 20220608053833.png]]


### Переобучение
Растет качество на трейне и при этом сильно быстрее чем на валидации.
![[Pasted image 20220608033541.png]]


![[Pasted image 20220608054257.png]]


### Регуляризация 
И называется она уже не просто L1 и L2 а weight decay
![[Pasted image 20220608054407.png]]


### Dropout - главная техника из регуляризации
Частный случай более общего подхода к регуляризации. 
Позволяет модели не переобучатся на какие то конкретные значения признаков.
(некоторые веса зануляются)
![[Pasted image 20220608055128.png]]

Дропаут это просто умножение на булеву маску. 
Так как мы предполагаем что данные центрированы, то среднее не должно меняться, а вот дисперсия пострадает.
![[Pasted image 20220608155955.png]]

По факту происходит что то типо как у решающего леса - ансамблирование. 

Дропаут позволяет понизить степень переобучения, за счет того что мы используем на каждом шаге более простую модель, а потом ансамблируем что бы получить более устойчивую оценку результата.
Обобщающая способность у модели использующей дропаут ниже чем у такой же модели без дропаута, но зато склонность к переобучению ниже

При отключении дропаута на тесте станет больше ненулевых значений, поэтому дисперсия скаканёт. И вместо дропаута нужно включать нормировку (поделить на вероятность дропаута) и тогда всё будет нормально. 






